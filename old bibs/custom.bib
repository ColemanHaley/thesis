
% LTeX: enabled=false
@article{cotterell-et-al-2019-complexity,
  title     = {On the {{Complexity}} and {{Typology}} of {{Inflectional Morphological Systems}}},
  author    = {Cotterell, Ryan and Kirov, Christo and Hulden, Mans and Eisner, Jason},
  editor    = {Lee, Lillian and Johnson, Mark and Roark, Brian and Nenkova, Ani},
  year      = {2019},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {7},
  pages     = {327--342},
  publisher = {MIT Press},
  address   = {Cambridge, MA},
  doi       = {10.1162/tacl_a_00271},
  urldate   = {2025-05-14},
  abstract  = {We quantify the linguistic complexity of different languages' morphological systems. We verify that there is a statistically significant empirical trade-off between paradigm size and irregularity: A language`s inflectional paradigms may be either large in size or highly irregular, but never both. We define a new measure of paradigm irregularity based on the conditional entropy of the surface realization of a paradigm--- how hard it is to jointly predict all the word forms in a paradigm from the lemma. We estimate irregularity by training a predictive model. Our measurements are taken on large morphological paradigms from 36 typologically diverse languages.},
  file      = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Cotterell et al_2019_On the Complexity and Typology of Inflectional Morphological Systems.pdf}
}
@book{stassen1997,
  title     = {Intransitive {Predication}},
  isbn      = {978-0-19-823693-1},
  url       = {https://doi.org/10.1093/oso/9780198236931.001.0001},
  abstract  = {Intransitive Predication constitutes a major contribution to the study of typological linguistics and theoretical linguistics in general. Basing his analysis on a sample of 410 languages, Leon Stassen investigates cross-linguistic variation in one of the core domains of all natural languages. The author views this domain as a `cognitive space', the topography of which is the same for all languages. It is assumed to consist of four subdomains, which correspond to a four-way distinction between the semantic classes of event predicates, property predicates, class predicates, and locational predicates. Leon Stassen offers a typology of the structural manifestations of this domain, in terms of the nature and number of the formal strategies used in its encoding. He discusses a number of abstract principles which can be employed in explaining the cross-linguistic variation embodied by the typology. In the final chapter, he brings together the research results in a universally applicable model, which can be read as a `flow chart' for the encoding of intransitive predications in different language types.},
  publisher = {Oxford University Press},
  author    = {Stassen, Leon},
  month     = sep,
  year      = {1997},
  doi       = {10.1093/oso/9780198236931.001.0001}
}

@article{demarneffe-et-al-2021-universal,
  title     = {Universal {{Dependencies}}},
  author    = {{de Marneffe}, Marie-Catherine and Manning, Christopher D. and Nivre, Joakim and Zeman, Daniel},
  year      = {2021},
  month     = jun,
  journal   = {Computational Linguistics},
  volume    = {47},
  number    = {2},
  pages     = {255--308},
  publisher = {MIT Press},
  address   = {Cambridge, MA},
  doi       = {10.1162/coli_a_00402},
  urldate   = {2025-05-14},
  abstract  = {Universal dependencies (UD) is a framework for morphosyntactic annotation of human language, which to date has been used to create treebanks for more than 100 languages. In this article, we outline the linguistic theory of the UD framework, which draws on a long tradition of typologically oriented grammatical theories. Grammatical relations between words are centrally used to explain how predicate--argument structures are encoded morphosyntactically in different languages while morphological features and part-of-speech classes give the properties of words. We argue that this theory is a good basis for crosslinguistically consistent annotation of typologically diverse languages in a way that supports computational natural language understanding as well as broader linguistic studies.},
  file      = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/de Marneffe et al_2021_Universal Dependencies2.pdf}
}

@article{futrell-2015-largescale,
  title     = {Large-Scale Evidence of Dependency Length Minimization in 37 Languages},
  author    = {Futrell, Richard and Mahowald, Kyle and Gibson, Edward},
  year      = {2015},
  month     = aug,
  journal   = {Proceedings of the National Academy of Sciences},
  volume    = {112},
  number    = {33},
  pages     = {10336--10341},
  publisher = {Proceedings of the National Academy of Sciences},
  doi       = {10.1073/pnas.1502134112},
  urldate   = {2025-05-14},
  abstract  = {Explaining the variation between human languages and the constraints on that variation is a core goal of linguistics. In the last 20 y, it has been claimed that many striking universals of cross-linguistic variation follow from a hypothetical principle that dependency length---the distance between syntactically related words in a sentence---is minimized. Various models of human sentence production and comprehension predict that long dependencies are difficult or inefficient to process; minimizing dependency length thus enables effective communication without incurring processing difficulty. However, despite widespread application of this idea in theoretical, empirical, and practical work, there is not yet large-scale evidence that dependency length is actually minimized in real utterances across many languages; previous work has focused either on a small number of languages or on limited kinds of data about each language. Here, using parsed corpora of 37 diverse languages, we show that overall dependency lengths for all languages are shorter than conservative random baselines. The results strongly suggest that dependency length minimization is a universal quantitative property of human languages and support explanations of linguistic variation in terms of general properties of human information processing.},
  file      = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Futrell et al_2015_Large-scale evidence of dependency length minimization in 37 languages.pdf}
}

@inproceedings{futrell-quantifying-2015,
  title     = {Quantifying {{Word Order Freedom}} in {{Dependency Corpora}}},
  booktitle = {Proceedings of the {{Third International Conference}} on {{Dependency Linguistics}} ({{Depling}} 2015)},
  author    = {Futrell, Richard and Mahowald, Kyle and Gibson, Edward},
  editor    = {Nivre, Joakim and Haji{\v c}ov{\'a}, Eva},
  year      = {2015},
  month     = aug,
  pages     = {91--100},
  publisher = {Uppsala University, Uppsala, Sweden},
  address   = {Uppsala, Sweden},
  urldate   = {2025-05-14},
  file      = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Futrell et al_2015_Quantifying Word Order Freedom in Dependency Corpora.pdf}
}

@article{gerdes-et-al-2021-typometrics,
  title      = {Typometrics: {{From Implicational}} to {{Quantitative Universals}} in {{Word Order Typology}}},
  shorttitle = {Typometrics},
  author     = {Gerdes, Kim and Kahane, Sylvain and Chen, Xinying},
  year       = {2021},
  month      = feb,
  journal    = {Glossa: a journal of general linguistics},
  volume     = {6},
  number     = {1},
  publisher  = {Open Library of Humanities},
  issn       = {2397-1835},
  doi        = {10.5334/gjgl.764},
  urldate    = {2025-05-14},
  abstract   = {This paper develops the concept of word order universals based on a data analysis of the~Universal Dependencies project, which proposes treebanks of more than 90 languages~encoded with the same annotation scheme. The nature of the data we work on allows~us to extract rich details for testing well-known typological implicational universals~and, further, explore new kinds of universals that we call quantitative universals. We~show how such quantitative universals are in essence different from implicational~universals, including statistical universals, by the fact that they no longer lay down~any claims on categorical statements, but rather on continuous parameters, opening~a new field of research we propose to call typometrics.},
  copyright  = {Copyright: {\copyright} 2021 The Author(s).                     This is an open-access article distributed under the terms of the                        Creative Commons Attribution 4.0 International License (CC-BY 4.0), which                        permits unrestricted use, distribution, and reproduction in any medium,                        provided the original author and source are credited. See http://creativecommons.org/licenses/by/4.0/.},
  langid     = {english},
  file       = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Gerdes et al_2021_Typometrics.pdf}
}

@inproceedings{levshina-2020-how,
  title      = {How Tight Is Your Language? {{A}} Semantic Typology Based on {{Mutual Information}}},
  shorttitle = {How Tight Is Your Language?},
  booktitle  = {Proceedings of the 19th {{International Workshop}} on {{Treebanks}} and {{Linguistic Theories}}},
  author     = {Levshina, Natalia},
  editor     = {Evang, Kilian and Kallmeyer, Laura and Ehren, Rafael and Petitjean, Simon and Seyffarth, Esther and Seddah, Djam{\'e}},
  year       = {2020},
  month      = oct,
  pages      = {70--78},
  publisher  = {Association for Computational Linguistics},
  address    = {D{\"u}sseldorf, Germany},
  doi        = {10.18653/v1/2020.tlt-1.7},
  urldate    = {2025-05-14},
  file       = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Levshina_2020_How tight is your language.pdf}
}

@inproceedings{ostling-2015-word,
  title     = {Word {{Order Typology}} through {{Multilingual Word Alignment}}},
  booktitle = {Proceedings of the 53rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 7th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 2: {{Short Papers}})},
  author    = {{\"O}stling, Robert},
  editor    = {Zong, Chengqing and Strube, Michael},
  year      = {2015},
  month     = jul,
  pages     = {205--211},
  publisher = {Association for Computational Linguistics},
  address   = {Beijing, China},
  doi       = {10.3115/v1/P15-2034},
  urldate   = {2025-05-14},
  file      = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Östling_2015_Word Order Typology through Multilingual Word Alignment.pdf}
}

@misc{beyer2024paligemmaversatile3bvlm,
  title  = {{PaliGemma}: A versatile 3B {VLM} for transfer},
  author = {Lucas Beyer and Andreas Steiner and André Susano Pinto and Alexander Kolesnikov and Xiao Wang and Daniel Salz and Maxim Neumann and Ibrahim Alabdulmohsin and Michael Tschannen and Emanuele Bugliarello and Thomas Unterthiner and Daniel Keysers and Skanda Koppula and Fangyu Liu and Adam Grycner and Alexey Gritsenko and Neil Houlsby and Manoj Kumar and Keran Rong and Julian Eisenschlos and Rishabh Kabra and Matthias Bauer and Matko Bošnjak and Xi Chen and Matthias Minderer and Paul Voigtlaender and Ioana Bica and Ivana Balazevic and Joan Puigcerver and Pinelopi Papalampidi and Olivier Henaff and Xi Xiong and Radu Soricut and Jeremiah Harmsen and Xiaohua Zhai},
  year   = {2024}
}
@article{staub-predictability-2024a,
  title      = {Predictability in {{Language Comprehension}}: {{Prospects}} and {{Problems}} for {{Surprisal}}},
  shorttitle = {Predictability in {{Language Comprehension}}},
  author     = {Staub, Adrian},
  year       = {Forthcoming},
  journal    = {Annual Review of Linguistics},
  publisher  = {Annual Reviews},
  doi        = {10.1146/annurev-linguistics-011724-121517},
  urldate    = {2024-10-14},
  abstract   = {Surprisal theory proposes that a word\&apos;s predictability influences processing difficulty because each word requires the comprehender to update a probability distribution over possible sentences. This article first considers the theory\&apos;s detailed predictions regarding the effects of predictability on reading time and N400 amplitude. Two rather unintuitive predictions appear to be correct based on the current evidence: There is no specific cost when an unpredictable word is encountered in a context where another word is predictable, and the function relating predictability to processing difficulty is logarithmic, not linear. Next, the article addresses the viability of the claim, also associated with Surprisal, that conditional probability is the ``causal bottleneck'' mediating all effects on incremental processing difficulty. This claim fares less well as conditional probability does not account for the difficulty associated with encountering a low-frequency word or the difficulty associated with garden path disambiguation. Surprisal provides a compelling account of predictability effects but does not provide a complete account of incremental processing difficulty.},
  langid     = {english},
  file       = {/Users/coleman/Zotero/storage/7LE5JCVZ/annurev-linguistics-011724-121517.html}
}

@inproceedings{zhai-sigmoid-2023,
  title     = {Sigmoid Loss for Language Image Pre-Training},
  booktitle = {2023 {{IEEE}}/{{CVF}} International Conference on Computer Vision ({{ICCV}})},
  author    = {Zhai, X. and Mustafa, B. and Kolesnikov, A. and Beyer, L.},
  year      = {2023},
  month     = oct,
  pages     = {11941--11952},
  publisher = {IEEE Computer Society},
  address   = {Los Alamitos, CA, USA},
  doi       = {10.1109/ICCV51070.2023.01100},
  abstract  = {We propose a simple pairwise sigmoid loss for imagetext pre-training. Unlike standard contrastive learning with softmax normalization, the sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. With only four TPUv4 chips, we can train a Base CLIP model at 4k batch size and a Large LiT model at 20k batch size, the latter achieves 84.5},
  keywords  = {computer vision,memory management,robustness,self-supervised learning,standards}
}
@incollection{booij-inflection-2007,
  title      = {Inflection},
  shorttitle = {The {{Grammar}} of {{Words}}},
  booktitle  = {The {{Grammar}} of {{Words}}: {{An Introduction}} to {{Linguistic Morphology}}},
  author     = {Booij, Geert},
  editor     = {Booij, Geert},
  year       = {2007},
  month      = jul,
  pages      = {99--124},
  publisher  = {Oxford University Press},
  doi        = {10.1093/acprof:oso/9780199226245.003.0005},
  urldate    = {2024-10-15},
  abstract   = {Inflection is the expression of morphosyntactic properties on words. Examples are case and number marking on nouns, and number and person marking on verbs. These properties play a role in computing the correct form of word in a sentence. Unlike derivation, inflectional processes do not create new words but forms of a word. There are different theoretical models for inflection: Word-and-Paradigm, Item-and-Arrangement, and Item-and-Process models.},
  isbn       = {978-0-19-922624-5},
  file       = {/Users/coleman/Zotero/storage/9GZ3YNIL/337167067.html}
}
@inproceedings{chen-pali-2023,
  title     = {{{PaLI}}: {{A}} Jointly-Scaled Multilingual Language-Image Model},
  booktitle = {The Eleventh International Conference on Learning Representations, {{ICLR}} 2023, Kigali, Rwanda, May 1-5, 2023},
  author    = {Chen, Xi and Wang, Xiao and Changpinyo, Soravit and Piergiovanni, A. J. and Padlewski, Piotr and Salz, Daniel and Goodman, Sebastian and Grycner, Adam and Mustafa, Basil and Beyer, Lucas and Kolesnikov, Alexander and Puigcerver, Joan and Ding, Nan and Rong, Keran and Akbari, Hassan and Mishra, Gaurav and Xue, Linting and Thapliyal, Ashish V. and Bradbury, James and Kuo, Weicheng},
  year      = {2023},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  timestamp = {Wed, 24 Jul 2024 16:50:33 +0200}
}

@article{haley-corpusbased-2023,
  title     = {{Corpus-based measures discriminate inflection and derivation cross-linguistically}},
  author    = {Haley, Coleman and Ponti, Edoardo M. and Goldwater, Sharon},
  year      = {2023},
  month     = jun,
  journal   = {Society for Computation in Linguistics},
  volume    = {6},
  number    = {1},
  publisher = {University of Massachusetts Amherst Libraries},
  issn      = {2834-1007},
  pages     = {403--407},
  doi       = {10.7275/z5z0-xx64},
  urldate   = {2024-10-15},
  abstract  = {Japanese passives are traditionally considered to have two types: direct and indirect passives. However, more recent studies, such as Ishizuka (2012), suggest the two types can be unified un- der the same syntactic movement analysis. Uti- lizing the Balanced Corpus of Contemporary Written Japanese (BCCWJ; Maekawa, 2008; Maekawa et al., 2014), this study aims to in- vestigate how likely different types of passives appear in the naturally occurring texts, espe- cially in relation to markedness-based hierar- chy called Noun Phrase Accessibility Hierar- chy (NPAH; Keenan and Comrie, 1977), and to investigate if true indirect passives occur in contemporary written Japanese.},
  langid    = {None},
  file      = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Haley et al_2023_Corpus-based measures discriminate inflection and derivation.pdf}
}

@article{lynott-lancaster-2020,
  title      = {The {{Lancaster Sensorimotor Norms}}: Multidimensional Measures of Perceptual and Action Strength for 40,000 {{English}} Words},
  shorttitle = {The {{Lancaster Sensorimotor Norms}}},
  author     = {Lynott, Dermot and Connell, Louise and Brysbaert, Marc and Brand, James and Carney, James},
  year       = {2020},
  month      = jun,
  journal    = {Behavior Research Methods},
  volume     = {52},
  number     = {3},
  pages      = {1271--1291},
  issn       = {1554-3528},
  doi        = {10.3758/s13428-019-01316-z},
  urldate    = {2024-10-15},
  abstract   = {Sensorimotor information plays a fundamental role in cognition. However, the existing materials that measure the sensorimotor basis of word meanings and concepts have been restricted in terms of their sample size and breadth of sensorimotor experience. Here we present norms of sensorimotor strength for 39,707 concepts across six perceptual modalities (touch, hearing, smell, taste, vision, and interoception) and five action effectors (mouth/throat, hand/arm, foot/leg, head excluding mouth/throat, and torso), gathered from a total of 3,500 individual participants using Amazon's Mechanical Turk platform. The Lancaster Sensorimotor Norms are unique and innovative in a number of respects: They represent the largest-ever set of semantic norms for English, at 40,000 words {\texttimes} 11 dimensions (plus several informative cross-dimensional variables), they extend perceptual strength norming to the new modality of interoception, and they include the first norming of action strength across separate bodily effectors. In the first study, we describe the data collection procedures, provide summary descriptives of the dataset, and interpret the relations observed between sensorimotor dimensions. We then report two further studies, in which we (1) extracted an optimal single-variable composite of the 11-dimension sensorimotor profile (Minkowski 3 strength) and (2) demonstrated the utility of both perceptual and action strength in facilitating lexical decision times and accuracy in two separate datasets. These norms provide a valuable resource to researchers in diverse areas, including psycholinguistics, grounded cognition, cognitive semantics, knowledge representation, machine learning, and big-data approaches to the analysis of language and conceptual representations. The data are accessible via the Open Science Framework (http://osf.io/7emr6/) and an interactive web application (https://www.lancaster.ac.uk/psychology/lsnorms/).},
  langid     = {english},
  file       = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Lynott et al_2020_The Lancaster Sensorimotor Norms.pdf}
}
@article{brysbaert-concreteness-2014,
  title    = {Concreteness Ratings for 40 Thousand Generally Known {{English}} Word Lemmas},
  author   = {Brysbaert, Marc and Warriner, Amy Beth and Kuperman, Victor},
  year     = {2014},
  month    = sep,
  journal  = {Behavior Research Methods},
  volume   = {46},
  number   = {3},
  pages    = {904--911},
  issn     = {1554-3528},
  doi      = {10.3758/s13428-013-0403-5},
  urldate  = {2024-10-15},
  abstract = {Concreteness ratings are presented for 37,058 English words and 2,896 two-word expressions (such as zebra crossing and zoom in), obtained from over 4,000 participants by means of a norming study using Internet crowdsourcing for data collection. Although the instructions stressed that the assessment of word concreteness would be based on experiences involving all senses and motor responses, a comparison with the existing concreteness norms indicates that participants, as before, largely focused on visual and haptic experiences. The reported data set is a subset of a comprehensive list of English lemmas and contains all lemmas known by at least 85~\% of the raters. It can be used in future research as a reference list of generally known English lemmas.},
  langid   = {english},
  keywords = {Concreteness,Crowdsourcing,Ratings,Word recognition},
  file     = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Brysbaert et al_2014_Concreteness ratings for 40 thousand generally known English word lemmas.pdf}
}

@misc{gemmateam2024gemmaopenmodelsbased,
  title         = {Gemma: {O}pen Models Based on {G}emini Research and Technology},
  author        = {Team Gemma},
  year          = {2024},
  eprint        = {2403.08295},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2403.08295}
}
@misc{pimentel2024computeprobabilityword,
  title         = {How to Compute the Probability of a Word},
  author        = {Tiago Pimentel and Clara Meister},
  year          = {2024},
  eprint        = {2406.14561},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2406.14561}
}
@article{gates2019element,
  title     = {Element-centric clustering comparison unifies overlaps and hierarchy},
  author    = {Gates, Alexander J and Wood, Ian B and Hetrick, William P and Ahn, Yong-Yeol},
  journal   = {Scientific reports},
  volume    = {9},
  number    = {1},
  pages     = {8574},
  year      = {2019},
  publisher = {Nature Publishing Group UK London}
}
@inproceedings{mscoco,
  author    = {Lin, Tsung-Yi
               and Maire, Michael
               and Belongie, Serge
               and Hays, James
               and Perona, Pietro
               and Ramanan, Deva
               and Doll{\'a}r, Piotr
               and Zitnick, C. Lawrence},
  editor    = {Fleet, David
               and Pajdla, Tomas
               and Schiele, Bernt
               and Tuytelaars, Tinne},
  title     = {Microsoft COCO: Common Objects in Context},
  booktitle = {Computer Vision -- ECCV 2014},
  year      = {2014},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {740--755},
  abstract  = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  isbn      = {978-3-319-10602-1}
}

@book{vogel2011approaches,
  title     = {Approaches to the typology of word classes},
  author    = {Vogel, Petra M and Comrie, Bernard},
  volume    = {23},
  year      = {2011},
  publisher = {Walter de Gruyter}
}

@incollection{oxford,
  author    = {Bisang, Walter},
  isbn      = {9780199281251},
  title     = {{Word Classes}},
  booktitle = {{The Oxford Handbook of Linguistic Typology}},
  publisher = {Oxford University Press},
  year      = {2010},
  month     = {11},
  abstract  = {{This article introduces the four prerequisites for distinguishing word classes: semantic criteria; pragmatic criteria/criteria of discourse function; formal criteria; and distinction between lexical and syntactic levels of analysis. The most important approaches to word classes based on the first three prerequisites are addressed. The article also deals with the distinction between content words and function words. It then takes up the discussion of the universal status of the noun/verb distinction by integrating the fourth prerequisite. The languages discussed are Classical Nahuatl, Late Archaic Chinese, and Tongan. The distinction between content words and function words is not identical to the distinction between open and closed word classes. The article reviews Dixon's seminal approach to adjectives. The sub-classes of adverbs are considered. The definition of word classes integrates all the central elements that make language structure, and it integrates a whole paradigm of constructions.}},
  doi       = {10.1093/oxfordhb/9780199281251.013.0015},
  url       = {https://doi.org/10.1093/oxfordhb/9780199281251.013.0015},
  eprint    = {https://academic.oup.com/book/0/chapter/335277661/chapter-ag-pdf/44444033/book\_38630\_section\_335277661.ag.pdf}
}


% LTeX: enabled=false
% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@inbook{ackemaneeleman2019,
  title     = {Default person versus default number in agreement},
  abstract  = {In this paper, we compare the behaviour of the default in the person system (third person) withthe default in the number system (singular). We argue, following Nevins (2007; 2011), thatthird person pronouns have person features, while singular DPs lack number features. Theevidence for these claims comes from situations in which a single head agrees with multiple DPs that have contrasting person and number specifications. In case the number of morphological slots in which agreement can be realized is lower than the number of agreement relations established in syntax, such contrasting specification may prove problematic. As it turns out, conflicts between singular and plural do not result in ungrammaticality, but conflicts between third person and first or second person do. Such person clashes can be avoided if the morphological realization of the relevant person features is syncretic. Alternatively, languages may make use of a person hierarchy that regulates the morphological realization of conflicting specifications for person. The argument we present is rooted in, and supports, the theory of person developed in Ackema & Neeleman (2013; to appear).},
  keywords  = {person, number, default, agreement, person hierarchy},
  author    = {Peter Ackema and Ad Neeleman},
  year      = {2019},
  doi       = {10.5281/zenodo.3458062},
  language  = {English},
  isbn      = {9783961102013},
  series    = {Open Generative Syntax},
  publisher = {Language Science Press},
  pages     = {21--54},
  booktitle = {Agreement, Case and Locality in the Nominal and Verbal Domains}
}
@inproceedings{adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{anderson-1982-wheres,
  author  = {Anderson, Stephen R.},
  journal = {Linguistic Inquiry},
  pages   = {571–612},
  title   = {Where’s morphology?},
  volume  = {13},
  year    = {1982}
}
@misc{arppe2019finite,
  author  = {Arppe, Antti and Harrigan, Atticus and Schmirler, Katherine and Antonsen, Lene and Trosterud, Trond and N{\o}rsteb{\o} Moshagen, Sjur and Silfverberg, Miikka and Wolvengrey, Arok and Snoek, Conor and Lachler, Jordan and Santos, Eddie Antonio and Okim{\=a}sis, Jean and Thunder, Dorothy},
  url     = {https://giellalt.uit.no/lang/crk/PlainsCreeDocumentation.html},
  title   = {Finite-State Transducer-Based Computational Model of {Plains Cree} Morphology},
  year    = {2014--2019},
  urldate = {2020-11-02}
}
@inproceedings{babazhanova,
  title     = {Geometric Probing of Word Vectors},
  author    = {Babazhanova, Madina and Tezekbayev, Maxat and Assylbekov, Zhenisbek},
  booktitle = {ESANN 2021 Proceedings - 29th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
  month     = oct # {{--}} # jun,
  year      = {2021},
  address   = {Virtual, Online, Belgium},
  publisher = {i6doc.com publication},
  doi       = {10.14428/esann/2021.ES2021-105},
  pages     = {587--592}
}

@incollection{backhouse-inflected-2004,
  title     = {Inflected and {{Uninflected Adjectives}} in {{Japanese}}},
  booktitle = {Adjective Classes: {A} Cross-Linguistic Typology},
  author    = {Backhouse, Anthony E},
  editor    = {Dixon, R M W and Aikhenvald, Alexandra Y},
  year      = {2004},
  month     = sep,
  pages     = {50--73},
  publisher = {Oxford University PressOxford},
  doi       = {10.1093/oso/9780199270934.003.0002},
  urldate   = {2024-10-07},
  abstract  = {Abstract             This chapter deals with adjectives in Japanese. Typologically, Japanese is a dependent-marking language; typical constituent order in the clause is predicate-final, and modifiers precede heads. Nouns function as the head of NPs commonly followed by case markers such as ga (NOM) and o (Ace), as modifier of nouns in NPs followed by the adnominal marker no, as complement of the copula da, and as complement of other copular verbs (such as naru `become') followed by the marker ni. Verbs function as the head of intransitive and transitive predicates, and directly precede NPs in modifying structures. Unlike nouns, verbs and the copula da arc inflected, largely on an agglutinating pattern. Lexically, Japanese has dearly delineated strata. The Sino and foreign strata arc the result of borrowing from classical Chinese and (chiefly) European languages respectively; Sino words are, at least diachronically, typically bimorphemic. In addition, mimetic items form a distinct stratum within the native vocabulary.},
  isbn      = {978-0-19-927093-4 978-1-383-04146-0},
  langid    = {english}
}
@article{bauer2004,
  title   = {The function of word-formation and the inflection-derivation distinction},
  author  = {Bauer, Laurie},
  journal = {Words and their Places. A Festschrift for J. Lachlan Mackenzie. Amsterdam: Vrije Universiteit},
  pages   = {283--292},
  year    = {2004}
}
@article{Beard1982,
  title   = {The plural as a lexical derivation},
  author  = {Beard, Robert},
  journal = {Glossa},
  volume  = {16},
  number  = {2},
  pages   = {133--148},
  year    = {1982}
}
@article{benjamini-control-2001,
  title      = {The {{Control}} of the {{False Discovery Rate}} in {{Multiple Testing}} under {{Dependency}}},
  author     = {Benjamini, Yoav and Yekutieli, Daniel},
  year       = {2001},
  journal    = {The Annals of Statistics},
  volume     = {29},
  number     = {4},
  eprint     = {2674075},
  eprinttype = {jstor},
  pages      = {1165--1188},
  publisher  = {Institute of Mathematical Statistics},
  issn       = {0090-5364},
  urldate    = {2024-10-14},
  abstract   = {Benjamini and Hochberg suggest that the false discovery rate may be the appropriate error rate to control in many applied multiple testing problems. A simple procedure was given there as an FDR controlling procedure for independent test statistics and was shown to be much more powerful than comparable procedures which control the traditional familywise error rate. We prove that this same procedure also controls the false discovery rate when the test statistics have positive regression dependency on each of the test statistics corresponding to the true null hypotheses. This condition for positive dependency is general enough to cover many problems of practical interest, including the comparisons of many treatments with a single control, multivariate normal test statistics with positive correlation matrix and multivariate t. Furthermore, the test statistics may be discrete, and the tested hypotheses composite without posing special difficulties. For all other forms of dependency, a simple conservative modification of the procedure controls the false discovery rate. Thus the range of problems for which a procedure with proven FDR control can be offered is greatly increased.},
  file       = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Benjamini_Yekutieli_2001_The Control of the False Discovery Rate in Multiple Testing under Dependency.pdf}
}
@misc{berger2024crosslingualcrossculturalvariationimage,
  title         = {Cross-Lingual and Cross-Cultural Variation in Image Descriptions},
  author        = {Uri Berger and Edoardo M. Ponti},
  year          = {2024},
  eprint        = {2409.16646},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2409.16646}
}

@inproceedings{bergmanis_segmentation_2017,
  author    = {Bergmanis, Toms and Goldwater, Sharon},
  title     = {From segmentation to analyses: a probabilistic model for unsupervised morphology induction},
  booktitle = {Proceedings of EACL},
  year      = 2017,
  address   = {Valencia, Spain}
}

@inproceedings{bert,
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author    = {Devlin, Jacob  and
               Chang, Ming-Wei  and
               Lee, Kenton  and
               Toutanova, Kristina},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = jun,
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N19-1423},
  doi       = {10.18653/v1/N19-1423},
  pages     = {4171--4186},
  abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}
@inproceedings{bertsyntax1,
  title     = {{A} Structural Probe for Finding Syntax in Word Representations},
  author    = {Hewitt, John  and
               Manning, Christopher D.},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = jun,
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N19-1419},
  doi       = {10.18653/v1/N19-1419},
  pages     = {4129--4138},
  abstract  = {Recent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network{'}s word representation space. The probe identifies a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models{'} vector geometry.}
}
@misc{beyer2024paligemmaversatile3bvlm,
  title  = {{PaliGemma}: A versatile 3B {VLM} for transfer},
  author = {Lucas Beyer and Andreas Steiner and André Susano Pinto and Alexander Kolesnikov and Xiao Wang and Daniel Salz and Maxim Neumann and Ibrahim Alabdulmohsin and Michael Tschannen and Emanuele Bugliarello and Thomas Unterthiner and Daniel Keysers and Skanda Koppula and Fangyu Liu and Adam Grycner and Alexey Gritsenko and Neil Houlsby and Manoj Kumar and Keran Rong and Julian Eisenschlos and Rishabh Kabra and Matthias Bauer and Matko Bošnjak and Xi Chen and Matthias Minderer and Paul Voigtlaender and Ioana Bica and Ivana Balazevic and Joan Puigcerver and Pinelopi Papalampidi and Olivier Henaff and Xi Xiong and Radu Soricut and Jeremiah Harmsen and Xiaohua Zhai},
  year   = {2024}
}
@article{bird-verbs-2003,
  title    = {Verbs and Nouns: The Importance of Being Imageable},
  author   = {Bird, Helen and Howard, David and Franklin, Sue},
  year     = {2003},
  month    = mar,
  journal  = {Journal of Neurolinguistics},
  volume   = {16},
  number   = {2},
  pages    = {113--149},
  issn     = {0911-6044},
  doi      = {10.1016/S0911-6044(02)00016-7},
  abstract = {There are many differences between verbs and nouns---semantic, syntactic and phonological. We focus on the semantic distinctions and examine differences in performance in both normal control subjects and individuals with aphasia. In tasks requiring production of particular semantic categories and categorisation of given verbs and nouns, control subjects produced fewer verbs than nouns and were slower and less accurate in verb categorisation. Patients who had shown a verb deficit in naming also had particular difficulties producing both verbs and nouns of relatively low imageability. In reading and writing, some patients exhibited poorer performance with verbs than nouns, even when verb/noun homonyms were used. When imageability was controlled, however, no dissociation was shown. We conclude that in simple single word tasks imageability must be controlled to eliminate this as a factor in apparent verb deficits. Other semantic factors, however, could affect performance, particularly when tasks involve the relationships between category exemplars.},
  keywords = {Aphasia,Imageability,Nouns,Reading,Semantics,Verbs,Writing}
}
@incollection{bisang-grammaticalization-2017,
  title        = {Grammaticalization},
  booktitle    = {Oxford {{Research Encyclopedia}} of {{Linguistics}}},
  author       = {Bisang, Walter},
  year         = {2017},
  month        = mar,
  publisher    = {Oxford University Press},
  doi          = {10.1093/acrefore/9780199384655.013.103},
  urldate      = {2024-10-07},
  abstract     = {Linguistic change not only affects the lexicon and the phonology of words, it also operates on the grammar of a language. In this context, grammaticalization is concerned with the development of lexical items into markers of grammatical categories or, more generally, with the development of markers used for procedural cueing of abstract relationships out of linguistic items with concrete referential meaning. A well-known example is the English verb               go               in its function of a future marker, as in               She is going to visit her friend               . Phenomena like these are very frequent across the world's languages and across many different domains of grammatical categories. In the last 50 years, research on grammaticalization has come up with a plethora of (a) generalizations, (b) models of how grammaticalization works, and (c) methodological refinements.                          On (a): Processes of grammaticalization develop gradually, step by step, and the sequence of the individual stages follows certain clines as they have been generalized from cross-linguistic comparison (unidirectionality). Even though there are counterexamples that go against the directionality of various clines, their number seems smaller than assumed in the late 1990s.             On (b): Models or scenarios of grammaticalization integrate various factors. Depending on the theoretical background, grammaticalization and its results are motivated either by the competing motivations of economy vs. iconicity/explicitness in functional typology or by a change from movement to merger in the minimalist program. Pragmatic inference is of central importance for initiating processes of grammaticalization (and maybe also at later stages), and it activates mechanisms like reanalysis and analogy, whose status is controversial in the literature. Finally, grammaticalization does not only work within individual languages/varieties, it also operates across languages. In situations of contact, the existence of a certain grammatical category may induce grammaticalization in another language.             On (c): Even though it is hard to measure degrees of grammaticalization in terms of absolute and exact figures, it is possible to determine relative degrees of grammaticalization in terms of the autonomy of linguistic signs. Moreover, more recent research has come up with criteria for distinguishing grammaticalization and lexicalization (defined as the loss of productivity, transparency, and/or compositionality of former productive, transparent, and compositional structures).             In spite of these findings, there are still quite a number of questions that need further research. Two questions to be discussed address basic issues concerning the overall properties of grammaticalization. (1) What is the relation between constructions and grammaticalization? In the more traditional view, constructions are seen as the syntactic framework within which linguistic items are grammaticalized. In more recent approaches based on construction grammar, constructions are defined as combinations of form and meaning. Thus, grammaticalization can be seen in the light of constructionalization, i.e., the creation of new combinations of form and meaning. Even though constructionalization covers many apects of grammaticalization, it does not exhaustively cover the domain of grammaticalization. (2) Is grammaticalization cross-linguistically homogeneous, or is there a certain range of variation? There is evidence from East and mainland Southeast Asia that there is cross-linguistic variation to some extent.},
  collaborator = {Bisang, Walter},
  isbn         = {978-0-19-938465-5},
  langid       = {english}
}
@incollection{bisang-word-2010,
  title     = {Word {{Classes}}},
  booktitle = {The {{Oxford Handbook}} of {{Linguistic Typology}}},
  author    = {Bisang, Walter},
  editor    = {Song, Jae Jung},
  year      = {2010},
  month     = nov,
  pages     = {0},
  publisher = {Oxford University Press},
  doi       = {10.1093/oxfordhb/9780199281251.013.0015},
  urldate   = {2024-05-15},
  abstract  = {This article introduces the four prerequisites for distinguishing word classes: semantic criteria; pragmatic criteria/criteria of discourse function; formal criteria; and distinction between lexical and syntactic levels of analysis. The most important approaches to word classes based on the first three prerequisites are addressed. The article also deals with the distinction between content words and function words. It then takes up the discussion of the universal status of the noun/verb distinction by integrating the fourth prerequisite. The languages discussed are Classical Nahuatl, Late Archaic Chinese, and Tongan. The distinction between content words and function words is not identical to the distinction between open and closed word classes. The article reviews Dixon's seminal approach to adjectives. The sub-classes of adverbs are considered. The definition of word classes integrates all the central elements that make language structure, and it integrates a whole paradigm of constructions.},
  isbn      = {978-0-19-928125-1}
}

@book{Blake2001,
  title     = {Case},
  author    = {Blake, Barry J},
  year      = {2001},
  publisher = {Cambridge University Press}
}
@article{bonami,
  author  = {Olivier Bonami and Denis Paperno},
  year    = {2018},
  title   = {Inflection vs. derivation in a distributional vector space},
  journal = {Lingue e
             linguaggio},
  volume  = {17},
  number  = {2},
  pages   = {173--196}
}
@article{bonami_paradigm_2019,
  title    = {Paradigm structure and predictability in derivational morphology},
  volume   = {29},
  issn     = {1871-5656},
  url      = {https://doi.org/10.1007/s11525-018-9322-6},
  doi      = {10.1007/s11525-018-9322-6},
  abstract = {In this paper we address the usefulness of the notion of a paradigm in the context of derivational morphology. We first define a notion of paradigmatic system that extends conservatively the notion as it is used in inflection so as to be applicable to collections of structured families of derivationally-related words. We then build on this definition in an empirical quantitative study of derivational families of verbs in French. We apply information-theoretic measures of predictability initially designed by Ackerman et al. (2009) in the context of inflection. We conclude that key quantitative properties are common to inflectional and derivational paradigmatic systems, and hence that (partial) paradigms are an important ingredient of the study of derivation.},
  number   = {2},
  journal  = {Morphology},
  author   = {Bonami, Olivier and Strnadová, Jana},
  month    = may,
  year     = {2019},
  pages    = {167--197}
}
@incollection{booij-inflection-2007,
  title      = {Inflection},
  shorttitle = {The {{Grammar}} of {{Words}}},
  booktitle  = {The {{Grammar}} of {{Words}}: {{An Introduction}} to {{Linguistic Morphology}}},
  author     = {Booij, Geert},
  editor     = {Booij, Geert},
  year       = {2007},
  month      = jul,
  pages      = {99--124},
  publisher  = {Oxford University Press},
  doi        = {10.1093/acprof:oso/9780199226245.003.0005},
  urldate    = {2024-10-15},
  abstract   = {Inflection is the expression of morphosyntactic properties on words. Examples are case and number marking on nouns, and number and person marking on verbs. These properties play a role in computing the correct form of word in a sentence. Unlike derivation, inflectional processes do not create new words but forms of a word. There are different theoretical models for inflection: Word-and-Paradigm, Item-and-Arrangement, and Item-and-Process models.},
  isbn       = {978-0-19-922624-5},
  file       = {/Users/coleman/Zotero/storage/9GZ3YNIL/337167067.html}
}
@incollection{booij1996,
  title     = {Inherent versus contextual inflection and the split
               morphology hypothesis},
  author    = {Booij, Geert},
  editors   = {Booij, Geert and van Marle, Jaap},
  booktitle = {Yearbook of Morphology 1995},
  year      = {1996},
  pages     = {1-16},
  publisher = {Springer},
  location  = {Kluwer, Dordrecht}
}
@article{boschlootest,
  title     = {Raised conditional level of significance for the 2$\times$ 2-table when testing the equality of two probabilities},
  author    = {Boschloo, RD},
  journal   = {Statistica Neerlandica},
  volume    = {24},
  number    = {1},
  pages     = {1--9},
  year      = {1970},
  publisher = {Wiley Online Library}
}
@article{brysbaert-concreteness-2014,
  title    = {Concreteness Ratings for 40 Thousand Generally Known {{English}} Word Lemmas},
  author   = {Brysbaert, Marc and Warriner, Amy Beth and Kuperman, Victor},
  year     = {2014},
  month    = sep,
  journal  = {Behavior Research Methods},
  volume   = {46},
  number   = {3},
  pages    = {904--911},
  issn     = {1554-3528},
  doi      = {10.3758/s13428-013-0403-5},
  urldate  = {2024-10-15},
  abstract = {Concreteness ratings are presented for 37,058 English words and 2,896 two-word expressions (such as zebra crossing and zoom in), obtained from over 4,000 participants by means of a norming study using Internet crowdsourcing for data collection. Although the instructions stressed that the assessment of word concreteness would be based on experiences involving all senses and motor responses, a comparison with the existing concreteness norms indicates that participants, as before, largely focused on visual and haptic experiences. The reported data set is a subset of a comprehensive list of English lemmas and contains all lemmas known by at least 85~\% of the raters. It can be used in future research as a reference list of generally known English lemmas.},
  langid   = {english},
  keywords = {Concreteness,Crowdsourcing,Ratings,Word recognition},
  file     = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Brysbaert et al_2014_Concreteness ratings for 40 thousand generally known English word lemmas.pdf}
}

@incollection{brysbaert2024concreteness,
  title     = {Concreteness and Imageability Norms},
  booktitle = {International Encyclopedia of Language and Linguistics},
  author    = {Brysbaert, Marc},
  year      = {to appear},
  edition   = {3rd},
  publisher = {Elsevier}
}

@book{bybee-1985-morphology,
  address   = {Amsterdam},
  author    = {Bybee, Joan L},
  key       = {Bybee 1985},
  publisher = {John Benjamins},
  title     = {Morphology: A study of the relation between meaning and form},
  year      = {1985}
}

@inproceedings{chen-pali-2023,
  title     = {{{PaLI}}: {{A}} Jointly-Scaled Multilingual Language-Image Model},
  booktitle = {The Eleventh International Conference on Learning Representations, {{ICLR}} 2023, Kigali, Rwanda, May 1-5, 2023},
  author    = {Chen, Xi and Wang, Xiao and Changpinyo, Soravit and Piergiovanni, A. J. and Padlewski, Piotr and Salz, Daniel and Goodman, Sebastian and Grycner, Adam and Mustafa, Basil and Beyer, Lucas and Kolesnikov, Alexander and Puigcerver, Joan and Ding, Nan and Rong, Keran and Akbari, Hassan and Mishra, Gaurav and Xue, Linting and Thapliyal, Ashish V. and Bradbury, James and Kuo, Weicheng},
  year      = {2023},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  timestamp = {Wed, 24 Jul 2024 16:50:33 +0200}
}

@article{chiarello-imageability-1999,
  title    = {Imageability and Distributional Typicality Measures of Nouns and Verbs in Contemporary {{English}}},
  author   = {Chiarello, Christine and Shears, Connie and Lund, Kevin},
  year     = {1999},
  month    = dec,
  journal  = {Behavior Research Methods, Instruments, \& Computers},
  volume   = {31},
  number   = {4},
  pages    = {603--637},
  issn     = {1532-5970},
  doi      = {10.3758/BF03200739},
  abstract = {Dissociations between noun and verb processing are not uncommon after brain injury; yet, precise psycholinguistic comparisons of nouns and verbs are hampered by the underrepresentation of verbs in published semantic word norms and by the absence of contemporary estimates for part-of-speech usage. We report herein imageability ratings and rating response times (RTs) for 1,197 words previously categorized as pure nouns, pure verbs, or words of balanced noun-verb usage on the basis of the Francis and Ku{\v c}era (1982) norms. Nouns and verbs differed in rated imageability, and there was a stronger correspondence between imageability rating and RT for nouns than for verbs. For all word types, the image-rating-RT function implied that subjects employed an image generation process to assign ratings. We also report a new measure of noun-verbtypicality that used the Hyperspace Analog to Language (HAL; Lund \& Burgess, 1996) context vectors (derived from a large sample of Usenet text) to compute the mean context distance between each word and all of thepure nouns andpure verbs. For a subset of the items, the resulting HAL noun-verb difference score was compared with part-of-speech usage in a representative sample of the Usenet corpus. It is concluded that this score can be used to estimate the extent to which a given word occurs in typical noun or verb sentence contexts in informal contemporary English discourse. The item statistics given in Appendix B will enable experimenters to select representative examples of nouns and verbs or to compare typical with atypical nouns (or verbs), while holding constant or covarying rated imageability.}
}
@book{chomsky-1957-syntactic,
  title     = {Syntactic {{Structures}}},
  author    = {Chomsky, Noam},
  year      = {1957},
  publisher = {De Gruyter Mouton},
  doi       = {10.1515/9783112316009},
  urldate   = {2024-05-14},
  isbn      = {978-3-11-231600-9}
}
@book{colarusso-northwest-1988,
  title     = {The {{Northwest Caucasian Languages}}: {{A Phonological Survey}}},
  author    = {Colarusso, John},
  year      = {1988},
  edition   = {0},
  publisher = {Garland},
  location  = {New York},
  doi       = {10.4324/9781315852263},
  url       = {https://www.taylorfrancis.com/books/9781317918172},
  urldate   = {2024-10-07},
  isbn      = {978-1-317-91817-2},
  langid    = {english},
  pagetotal = {520}
}
@book{comrie-language-1988,
  title      = {Language Universals and Linguistic Typology},
  shorttitle = {Language Universals and Linguistic Typology},
  author     = {Comrie, Bernard},
  year       = {1988},
  edition    = {2nd},
  publisher  = {The University of Chicago Press},
  address    = {Chicago},
  isbn       = {978-0-226-11433-0},
  langid     = {english}
}
@inbook{ComriePolinsky1998,
  title        = {The Great Dagestanian Case Hoax},
  booktitle    = {Case, Typology, and Grammar},
  year         = {1998},
  pages        = {95-114},
  publisher    = {John Benjamins},
  organization = {John Benjamins},
  address      = {Amsterdam},
  author       = {Comrie, Bernard and Polinsky, Maria}
}
@article{connell-strength-2012,
  title    = {Strength of Perceptual Experience Predicts Word Processing Performance Better than Concreteness or Imageability},
  author   = {Connell, Louise and Lynott, Dermot},
  year     = {2012},
  month    = dec,
  journal  = {Cognition},
  volume   = {125},
  number   = {3},
  pages    = {452--465},
  issn     = {0010-0277},
  doi      = {10.1016/j.cognition.2012.07.010},
  abstract = {Abstract concepts are traditionally thought to differ from concrete concepts by their lack of perceptual information, which causes them to be processed more slowly and less accurately than perceptually-based concrete concepts. In two studies, we examined this assumption by comparing concreteness and imageability ratings to a set of perceptual strength norms in five separate modalities: sound, taste, touch, smell and vision. Results showed that concreteness and imageability do not reflect the perceptual basis of concepts: concreteness ratings appear to be based on two different intersecting decision criteria, while imageability ratings are visually biased. Analysis of lexical decision and word naming performance showed that maximum perceptual strength (i.e., strength in the dominant perceptual modality) consistently outperformed both concreteness and imageability ratings in accounting for variance in response latency and accuracy. We conclude that so-called concreteness effects in word processing emerge from the perceptual strength of a concept's representation and discuss the implications for theories of conceptual representation.},
  keywords = {Abstract and concrete concepts,Concreteness effects,Context availability,Dual coding,Imageability,Lexical decision,Perceptual strength,Situated simulation,Word naming}
}
@article{copot-et-al-2022-idiosyncratic
  title        = {Idiosyncratic frequency as a measure of derivation vs. inflection},
  volume       = {10},
  url          = {https://jlm.ipipan.waw.pl/index.php/JLM/article/view/301},
  doi          = {10.15398/jlm.v10i2.301},
  abstractnote = {&amp;lt;p&amp;gt;There is ongoing discussion about how to conceptualize the nature of the distinction between inflection and derivation. A common approach relies on qualitative differences in the semantic relationship between inflectionally versus derivationally related words: inflection yields ways to discuss the same concept in different syntactic contexts, while derivation gives rise to words for related concepts. This differential can be expected to manifest in the predictability of word frequency between words that are related derivationally or inflectionally: predicting the token frequency of a word based on information about its base form or about related words should be easier when the two words are in an inflectional relationship, rather than a derivational one. We compare prediction error magnitude for statistical models of token frequency based on distributional and frequency information of inflectionally or derivationally related words in French. The results conform to expectations: it is easier to predict the frequency of a word from properties of an inflectionally related word than from those of a derivationally related word. Prediction error provides a quantitative, continuous method to explore differences between individual processes and differences yielded by employing different predicting information, which in turn can be used to draw conclusions about the nature and manifestation of the inflection–derivation distinction.&amp;lt;/p&amp;gt;},
  number       = {2},
  journal      = {Journal of Language Modelling},
  author       = {Copot, Maria and Mickus, Timothee and Bonami, Olivier},
  year         = {2022},
  month        = {Dec.},
  pages        = {193–240}
}
@article{corbett2010,
  title     = {Canonical derivational morphology},
  author    = {Corbett, Greville G},
  journal   = {Word structure},
  volume    = {3},
  number    = {2},
  pages     = {141--155},
  year      = {2010},
  publisher = {Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK}
}
@incollection{corver-semilexical-2001,
  title     = {Semi-Lexical Categories},
  booktitle = {Semi-Lexical {{Categories}}},
  author    = {Corver, Norbert and Riemsdijk, Henk Van},
  editor    = {Corver, Norbert and Riemsdijk, Henk Van},
  year      = {2001},
  month     = dec,
  pages     = {1--20},
  publisher = {de {G}ruyter},
  doi       = {10.1515/9783110874006.1},
  urldate   = {2024-10-07},
  isbn      = {978-3-11-016685-9}
}
@article{cotterell-et-al-2019-complexity,
  title     = {On the {{Complexity}} and {{Typology}} of {{Inflectional Morphological Systems}}},
  author    = {Cotterell, Ryan and Kirov, Christo and Hulden, Mans and Eisner, Jason},
  editor    = {Lee, Lillian and Johnson, Mark and Roark, Brian and Nenkova, Ani},
  year      = {2019},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {7},
  pages     = {327--342},
  publisher = {MIT Press},
  address   = {Cambridge, MA},
  doi       = {10.1162/tacl_a_00271},
  urldate   = {2025-05-14},
  abstract  = {We quantify the linguistic complexity of different languages' morphological systems. We verify that there is a statistically significant empirical trade-off between paradigm size and irregularity: A language`s inflectional paradigms may be either large in size or highly irregular, but never both. We define a new measure of paradigm irregularity based on the conditional entropy of the surface realization of a paradigm--- how hard it is to jointly predict all the word forms in a paradigm from the lemma. We estimate irregularity by training a predictive model. Our measurements are taken on large morphological paradigms from 36 typologically diverse languages.},
  file      = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Cotterell et al_2019_On the Complexity and Typology of Inflectional Morphological Systems.pdf}
}
@article{croft-2016,
  url         = {https://doi.org/10.1515/lingty-2016-0012},
  title       = {Comparative concepts and language-specific categories: Theory and practice},
  title       = {},
  author      = {William Croft},
  pages       = {377--393},
  volume      = {20},
  number      = {2},
  journal     = {Linguistic Typology},
  doi         = {doi:10.1515/lingty-2016-0012},
  year        = {2016},
  lastchecked = {2025-04-24}
}

@book{croft-2001-radical,
  title     = {Radical {{Construction Grammar}}: {{Syntactic Theory}} in {{Typological Perspective}}},
  author    = {Croft, William},
  year      = {2001},
  month     = oct,
  publisher = {Oxford University Press},
  doi       = {10.1093/acprof:oso/9780198299554.001.0001},
  urldate   = {2024-05-14},
  abstract  = {This book presents a profound critique of syntactic theory and syntactic argumentation. Recent syntactic theories are essentially formal models for the representation of grammatical knowledge. These theories posit complex syntactic structures in the analysis of sentences, consisting of atomic primitive syntactic categories and relations. The result of this approach to syntax has been an endless cycle of new and revised theories of syntactic representation. The book argues that these types of syntactic theories are incompatible with the grammatical variation found within and across languages. The extent of grammatical variation demonstrates that no scheme of atomic primitive syntactic categories and relations can form the basis of an empirically adequate syntactic theory. This book defends three theses: (i) constructions are the primitive units of syntactic representation, and grammatical categories are derivative; (ii) the only syntactic structures are the relations between a construction and the elements that make it up (that is, there is no need to posit syntactic relations); and (iii) constructions are language-specific. Constructions are complex units pairing form and meaning. Grammatical categories within and across languages are mapped onto a universal conceptual space, following the semantic map model in typology. The structure of conceptual space constrains how meaning is encoded in linguistic form, and reflects the structure of the human mind.},
  isbn      = {978-0-19-829955-4}
}
@book{croft-typology-2002,
  title     = {Typology and {{Universals}}},
  author    = {Croft, William},
  year      = {2002},
  month     = nov,
  edition   = {2nd},
  series     = {Cambridge Textbooks in Linguistics},
  publisher = {Cambridge University Press},
  doi       = {10.1017/CBO9780511840579},
  urldate   = {2024-10-07},
  abstract  = {Comparison of the grammars of human languages reveals systematic patterns of variation. Typology and universals research uncovers those patterns to formulate universal constraints on language and seek their exploration. In this essential textbook, William Croft presents a comprehensive introduction to the method and theory used in studying typology and universals. The theoretical issues discussed range from the most fundamental to the most abstract. The book provides students and researchers with extensive examples of language universals in phonology, morphology, syntax and semantics. This second edition has been thoroughly rewritten and updated to reflect advances in typology and universals in the past decade, including: new methodologies such as the semantic map model and questions of syntactic argumentation; discussion of current debates over deeper explanations for specific classes of universals; and comparison of the typological and generative approaches to language.},
  copyright = {https://www.cambridge.org/core/terms},
  isbn      = {978-0-521-00499-2 978-0-521-80884-2 978-0-511-84057-9}
}
@book{croft1991,
  title     = {Syntactic Categories and Grammatical Relations: {{The}} Cognitive Organization of Information},
  author    = {Croft, W.},
  year      = {1991},
  series    = {Emersion: {{Emergent}} Village Resources for Communities of Faith Series},
  publisher = {University of Chicago Press},
  isbn      = {978-0-226-12090-4},
  lccn      = {90038349}
}
@article{cutler-1981-degrees,
  title     = {Degrees of transparency in word formation},
  author    = {Cutler, Anne},
  journal   = {Canadian Journal of Linguistics/Revue canadienne de linguistique},
  volume    = {26},
  number    = {1},
  pages     = {73--77},
  year      = {1981},
  publisher = {Cambridge University Press}
}
@inproceedings{czech,
  title     = {Attempting to separate inflection and derivation using vector space representations},
  author    = {Rosa, Rudolf  and
               {\v{Z}}abokrtsk{\'y}, Zden{\v{e}}k},
  booktitle = {Proceedings of the Second International Workshop on Resources and Tools for Derivational Morphology},
  month     = sep,
  year      = {2019},
  address   = {Prague, Czechia},
  publisher = {Charles University, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics},
  url       = {https://aclanthology.org/W19-8508},
  pages     = {61--70}
}
@article{demarneffe-universal-2021,
  title    = {Universal {{Dependencies}}},
  author   = {{d}e Marneffe, Marie-Catherine and Manning, Christopher D. and Nivre, Joakim and Zeman, Daniel},
  year     = {2021},
  month    = may,
  journal  = {Computational Linguistics},
  volume   = {47},
  number   = {2},
  pages    = {255--308},
  issn     = {0891-2017, 1530-9312},
  doi      = {10.1162/coli_a_00402},
  urldate  = {2024-10-07},
  abstract = {Abstract             Universal dependencies (UD) is a framework for morphosyntactic annotation of human language, which to date has been used to create treebanks for more than 100 languages. In this article, we outline the linguistic theory of the UD framework, which draws on a long tradition of typologically oriented grammatical theories. Grammatical relations between words are centrally used to explain how predicate--argument structures are encoded morphosyntactically in different languages while morphological features and part-of-speech classes give the properties of words. We argue that this theory is a good basis for crosslinguistically consistent annotation of typologically diverse languages in a way that supports computational natural language understanding as well as broader linguistic studies.},
  langid   = {english},
  file     = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/De Marneffe et al_2021_Universal Dependencies.pdf;/Users/coleman/Zotero/storage/RK2MTTGY/De Marneffe et al. - 2021 - Universal Dependencies.pdf}
}
@article{distributionalsem,
  author  = {Zellig Harris},
  year    = {1954},
  title   = {Distributional structure},
  journal = {Word},
  volume  = {10},
  number  = {23},
  pages   = {146--162}
}
@article{dressler1989,
  title     = {Prototypical differences between inflection and derivation},
  author    = {Dressler, Wolfgang U},
  journal   = {STUF-Language Typology and Universals},
  volume    = {42},
  number    = {1},
  pages     = {3--10},
  year      = {1989},
  publisher = {De Gruyter (A)}
}
@incollection{dryer-are-1997,
  title     = {Are {{Grammatical Relations Universal}}?},
  booktitle = {Essays on {{Language Function}} and {{Language Type}}},
  author    = {Dryer, Matthew S.},
  editor    = {Bybee, Joan L. and Haiman, John and Thompson, Sandra A.},
  year      = {1997},
  pages     = {115},
  publisher = {John Benjamins Publishing Company},
  address   = {Amsterdam},
  doi       = {10.1075/z.82.09dry},
  urldate   = {2024-05-15},
  isbn      = {978-90-272-2168-1 978-1-55619-522-8 978-90-272-7421-2},
  langid    = {english}
}
@article{dryer1989,
  title     = {Large linguistic areas and language sampling},
  author    = {Dryer, Matthew S},
  journal   = {Studies in Language. International Journal sponsored by the Foundation “Foundations of Language”},
  volume    = {13},
  number    = {2},
  pages     = {257--292},
  year      = {1989},
  publisher = {John Benjamins}
}
@article{dube-independent-2014,
  title    = {Independent Effects of Imageability and Grammatical Class in Synonym Judgement in Aphasia.},
  author   = {Dub{\'e}, Catherine and Monetta, Laura and {Mart{\'i}nez-Cuiti{\~n}o}, Mar{\'i}a Macarena and Wilson, Maximiliano A.},
  year     = {2014},
  journal  = {Psicothema},
  volume   = {26},
  number   = {4},
  pages    = {449--456},
  address  = {Spain},
  issn     = {1886-144X 0214-9915},
  doi      = {10.7334/psicothema2014.31},
  abstract = {BACKGROUND: The grammatical class effect in aphasia, i.e. dissociated processing of words according to their respective grammatical class, has been attributed to  either grammatical, lexical or semantic (i.e., imageability) deficits. This study  explores the hypotheses of impaired semantic treatment as the source of the  grammatical class effect in aphasia. METHOD: A synonym judgement task that  includes nouns and verbs of high and low imageability has been administered to 30  Spanish-speaking patients suffering from receptive or productive aphasia and 30  controls. RESULTS: Normal controls performed significantly better than aphasic  patients. Although globally the productive aphasics performed significantly  better than the receptive aphasics, grammatical class (nouns better than verbs)  and imageability (high imageability better than low imageability) affected  performance in both subgroups. No significant interaction emerged between these  two factors. CONCLUSION: The results suggest that the grammatical class effect  may emerge from semantic impairment and that it is -at least partially-  independent of the imageability of words.},
  langid   = {english},
  pmid     = {25340890},
  keywords = {*Imagination,*Linguistics,*Vocabulary,Aphasia/*psychology,Argentina,Female,Humans,Male,Middle Aged}
}
@article{fasttext,
  title     = {Enriching Word Vectors with Subword Information},
  author    = {Bojanowski, Piotr  and
               Grave, Edouard  and
               Joulin, Armand  and
               Mikolov, Tomas},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {5},
  year      = {2017},
  address   = {Cambridge, MA},
  publisher = {MIT Press},
  url       = {https://aclanthology.org/Q17-1010},
  doi       = {10.1162/tacl\_a\_00051},
  pages     = {135--146},
  abstract  = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.}
}
@article{floyd-rediscovering-2011,
  title   = {Re-Discovering the {{Quechua}} Adjective},
  author  = {Floyd, Simeon},
  year    = {2011},
  month   = jan,
  journal = {Linguistic Typology},
  volume  = {15},
  number  = {1},
  issn    = {1430-0532, 1613-415X},
  doi     = {10.1515/lity.2011.003},
  pages   = {25--63},
  urldate = {2024-10-07},
  file    = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Floyd_2011_Re-discovering the Quechua adjective.pdf}
}
@incollection{function,
  title     = {The function of word-formation and the inflection-derivation distinction},
  author    = {Laurie Bauer},
  booktitle = {Words in their Places. A {F}estschrift for {J}. {L}achlan {M}ackenzie},
  publisher = {Vrije Universiteit},
  address   = {Amsterdam},
  year      = {2004},
  pages     = {283--292}
}

@article{futrell-2015-largescale,
  title     = {Large-Scale Evidence of Dependency Length Minimization in 37 Languages},
  author    = {Futrell, Richard and Mahowald, Kyle and Gibson, Edward},
  year      = {2015},
  month     = aug,
  journal   = {Proceedings of the National Academy of Sciences},
  volume    = {112},
  number    = {33},
  pages     = {10336--10341},
  publisher = {Proceedings of the National Academy of Sciences},
  doi       = {10.1073/pnas.1502134112},
  urldate   = {2025-05-14},
  abstract  = {Explaining the variation between human languages and the constraints on that variation is a core goal of linguistics. In the last 20 y, it has been claimed that many striking universals of cross-linguistic variation follow from a hypothetical principle that dependency length---the distance between syntactically related words in a sentence---is minimized. Various models of human sentence production and comprehension predict that long dependencies are difficult or inefficient to process; minimizing dependency length thus enables effective communication without incurring processing difficulty. However, despite widespread application of this idea in theoretical, empirical, and practical work, there is not yet large-scale evidence that dependency length is actually minimized in real utterances across many languages; previous work has focused either on a small number of languages or on limited kinds of data about each language. Here, using parsed corpora of 37 diverse languages, we show that overall dependency lengths for all languages are shorter than conservative random baselines. The results strongly suggest that dependency length minimization is a universal quantitative property of human languages and support explanations of linguistic variation in terms of general properties of human information processing.},
  file      = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Futrell et al_2015_Large-scale evidence of dependency length minimization in 37 languages.pdf}
}
@inproceedings{futrell-quantifying-2015,
  title     = {Quantifying {{Word Order Freedom}} in {{Dependency Corpora}}},
  booktitle = {Proceedings of the {{Third International Conference}} on {{Dependency Linguistics}} ({{Depling}} 2015)},
  author    = {Futrell, Richard and Mahowald, Kyle and Gibson, Edward},
  editor    = {Nivre, Joakim and Haji{\v c}ov{\'a}, Eva},
  year      = {2015},
  month     = aug,
  pages     = {91--100},
  publisher = {Uppsala University, Uppsala, Sweden},
  address   = {Uppsala, Sweden},
  urldate   = {2025-05-14},
  file      = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Futrell et al_2015_Quantifying Word Order Freedom in Dependency Corpora.pdf}
}
@misc{gemmateam2024gemmaopenmodelsbased,
  title         = {Gemma: {O}pen Models Based on {G}emini Research and Technology},
  author        = {Team Gemma},
  year          = {2024},
  eprint        = {2403.08295},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2403.08295}
}
@article{gerdes-et-al-2021-typometrics,
  title      = {Typometrics: {{From Implicational}} to {{Quantitative Universals}} in {{Word Order Typology}}},
  shorttitle = {Typometrics},
  author     = {Gerdes, Kim and Kahane, Sylvain and Chen, Xinying},
  year       = {2021},
  month      = feb,
  journal    = {Glossa: a journal of general linguistics},
  volume     = {6},
  number     = {1},
  publisher  = {Open Library of Humanities},
  issn       = {2397-1835},
  doi        = {10.5334/gjgl.764},
  urldate    = {2025-05-14},
  abstract   = {This paper develops the concept of word order universals based on a data analysis of the~Universal Dependencies project, which proposes treebanks of more than 90 languages~encoded with the same annotation scheme. The nature of the data we work on allows~us to extract rich details for testing well-known typological implicational universals~and, further, explore new kinds of universals that we call quantitative universals. We~show how such quantitative universals are in essence different from implicational~universals, including statistical universals, by the fact that they no longer lay down~any claims on categorical statements, but rather on continuous parameters, opening~a new field of research we propose to call typometrics.},
  copyright  = {Copyright: {\copyright} 2021 The Author(s).                     This is an open-access article distributed under the terms of the                        Creative Commons Attribution 4.0 International License (CC-BY 4.0), which                        permits unrestricted use, distribution, and reproduction in any medium,                        provided the original author and source are credited. See http://creativecommons.org/licenses/by/4.0/.},
  langid     = {english},
  file       = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Gerdes et al_2021_Typometrics.pdf}
}
@book{givon1979,
  title     = {On {{Understanding Grammar}}},
  author    = {Giv{\'o}n, T.},
  year      = {1979},
  series    = {Perspectives in Neurolinguistics and Psycholinguistics},
  publisher = {Academic Press},
  isbn      = {978-0-12-285450-7},
  lccn      = {78067876}
}
@inproceedings{glaff,
  title     = {{GL{\`A}FF}, a Large Versatile {F}rench Lexicon},
  author    = {Hathout, Nabil  and
               Sajous, Franck  and
               Calderone, Basilio},
  booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)},
  month     = may,
  year      = {2014},
  address   = {Reykjavik, Iceland},
  publisher = {European Language Resources Association (ELRA)},
  url       = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/58_Paper.pdf},
  pages     = {1007--1012},
  abstract  = {This paper introduces GLAFF, a large-scale versatile French lexicon extracted from Wiktionary, the collaborative online dictionary. GLAFF contains, for each entry, inflectional features and phonemic transcriptions. It distinguishes itself from the other available French lexicons by its size, its potential for constant updating and its copylefted license. We explain how we have built GLAFF and compare it to other known resources in terms of coverage and quality of the phonemic transcriptions. We show that its size and quality are strong assets that could allow GLAFF to become a reference lexicon for French NLP and linguistics. Moreover, other derived lexicons can easily be based on GLAFF to satisfy specific needs of various fields such as psycholinguistics.}
}
@inproceedings{glove,
  title     = {{G}lo{V}e: Global Vectors for Word Representation},
  author    = {Pennington, Jeffrey  and
               Socher, Richard  and
               Manning, Christopher},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
  month     = oct,
  year      = {2014},
  address   = {Doha, Qatar},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D14-1162},
  doi       = {10.3115/v1/D14-1162},
  pages     = {1532--1543}
}
@book{gordon-phonological-2016,
  title     = {Phonological {{Typology}}},
  author    = {Gordon, Matthew K.},
  year      = {2016},
  month     = apr,
  publisher = {Oxford University Press},
  doi       = {10.1093/acprof:oso/9780199669004.001.0001},
  urldate   = {2024-10-07},
  isbn      = {978-0-19-966900-4}
}
@book{greenberg-1966-universals,
  title      = {Universals of Language},
  shorttitle = {Universals of Language},
  editor     = {Greenberg, Joseph Harold},
  year       = {1966},
  series     = {The {{M}}.{{I}}.{{T}}. {{Press}} Paperback Series},
  edition    = {2nd},
  number     = {37},
  publisher  = {M.I.T Pr},
  address    = {Cambridge, Mass.},
  isbn       = {978-0-262-57008-4},
  langid     = {english}
}
@book{greenberg1966,
  title     = {Universals of language},
  editor    = {Greenberg, Joseph H.},
  edition   = {2},
  year      = {1966},
  publisher = {M.I.T. Press}
}
@article{haiman1980,
  title      = {The {{Iconicity}} of {{Grammar}}: {{Isomorphism}} and {{Motivation}}},
  shorttitle = {The {{Iconicity}} of {{Grammar}}},
  author     = {Haiman, John},
  year       = {1980},
  journal    = {Language},
  volume     = {56},
  number     = {3},
  eprint     = {414448},
  eprinttype = {jstor},
  pages      = {515--540},
  publisher  = {Linguistic Society of America},
  issn       = {0097-8507},
  doi        = {10.2307/414448},
  urldate    = {2025-01-28},
  abstract   = {Although linguistic signs in isolation are symbolic, the system or grammar which relates them may be diagrammatically iconic in two ways: (a) by isomorphism, a bi-unique correspondence tends to be established between signans and signatum; (b) by motivation, the structure of language directly reflects some aspect of the structure of reality. Isomorphism is so nearly universal that deviations from it require explanation. Motivation, although widespread, establishes a typology of languages, as indicated in Saussure's Cours. The evidence of artificial taboo languages suggests that degree of motivation co-varies inversely with the number of 'prima onomata' in the lexicon.},
  keywords   = {thesislit},
  file       = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Haiman_1980_The Iconicity of Grammar.pdf}
}
@article{haley-corpusbased-2023,
  title     = {{Corpus-based measures discriminate inflection and derivation cross-linguistically}},
  author    = {Haley, Coleman and Ponti, Edoardo M. and Goldwater, Sharon},
  year      = {2023},
  month     = jun,
  journal   = {Society for Computation in Linguistics},
  volume    = {6},
  number    = {1},
  publisher = {University of Massachusetts Amherst Libraries},
  issn      = {2834-1007},
  pages     = {403--407},
  doi       = {10.7275/z5z0-xx64},
  urldate   = {2024-10-15},
  abstract  = {Japanese passives are traditionally considered to have two types: direct and indirect passives. However, more recent studies, such as Ishizuka (2012), suggest the two types can be unified un- der the same syntactic movement analysis. Uti- lizing the Balanced Corpus of Contemporary Written Japanese (BCCWJ; Maekawa, 2008; Maekawa et al., 2014), this study aims to in- vestigate how likely different types of passives appear in the naturally occurring texts, espe- cially in relation to markedness-based hierar- chy called Noun Phrase Accessibility Hierar- chy (NPAH; Keenan and Comrie, 1977), and to investigate if true indirect passives occur in contemporary written Japanese.},
  langid    = {None},
  file      = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Haley et al_2023_Corpus-based measures discriminate inflection and derivation.pdf}
}
@article{haspelmath-2010-comparative,
  title      = {Comparative Concepts and Descriptive Categories in Crosslinguistic Studies},
  author     = {Haspelmath, Martin},
  year       = {2010},
  journal    = {Language},
  volume     = {86},
  number     = {3},
  eprint     = {40961695},
  eprinttype = {jstor},
  pages      = {663--687},
  publisher  = {Linguistic Society of America},
  issn       = {0097-8507},
  urldate    = {2024-10-07},
  abstract   = {In this discussion note, I argue that we need to distinguish carefully between descriptive categories, that is, categories of particular languages, and comparative concepts, which are used for crosslinguistic comparison and are specifically created by typologists for the purposes of comparison. Descriptive formal categories cannot be equated across languages because the criteria for category assignment are different from language to language. This old structuralist insight (called CATEGORIAL PARTICULARISM) has recently been emphasized again by several linguists, but the idea that linguists need to identify 'crosslinguistic categories' before they can compare languages is still widespread, especially (but not only) in generative linguistics. Instead, what we have to do (and normally do in practice) is to create comparative concepts that allow us to identify comparable phenomena across languages and to formulate crosslinguistic generalizations. Comparative concepts have to be universally applicable, so they can only be based on other universally applicable concepts: conceptual-semantic concepts, general formal concepts, and other comparative concepts. Comparative concepts are not always purely semantically based concepts, but outside of phonology they usually contain a semantic component. The fact that typologists compare languages in terms of a separate set of concepts that is not taxonomically superordinate to descriptive linguistic categories means that typology and language-particular analysis are more independent of each other than is often thought.},
  file       = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Haspelmath_2010_Comparative concepts and descriptive categories in crosslinguistic studies.pdf}
}

@incollection{haspelmath-2003-geometry,
  title     = {The Geometry of Grammatical Meaning: {{Semantic}} Maps and Cross-Linguistic Comparison},
  booktitle = {The New Psychology of Language},
  author    = {Haspelmath, Martin},
  editor    = {Tomasello, Michael},
  year      = {2003},
  volume    = {2},
  pages     = {211--242},
  publisher = {Lawrence Erlbaum Associates},
  address   = {Mahwah, NJ, USA}
}
@article{haspelmath-2012-how,
  title     = {How to Compare Major Word-Classes across the World's Languages},
  author    = {Haspelmath, Martin},
  year      = {2012},
  month     = feb,
  journal   = {UCLA Working Papers in Linguistics},
  volume    = {17},
  pages     = {109--130},
  doi       = {10.5281/ZENODO.3678496},
  urldate   = {2024-05-15},
  abstract  = {In this paper, I argue that major word-classes, such as nouns, verbs and adjectives, cannot be compared across languages by asking questions such as "Does language X have a noun-verb distinction?". Such questions are routinely asked by linguists (functionalists and generativists alike), but these are the wrong questions (cf. Croft 2000), because they make presuppositions which are not valid.},
  copyright = {Creative Commons Attribution 4.0 International, Open Access}
}
@article{haspelmath-2007-preestablished,
  title   = {Pre-Established Categories Don't Exist: {{Consequences}} for Language Description and Typology},
  author  = {Haspelmath, Martin},
  year    = {2007},
  journal = {Linguistic {{Typology}}},
  volume  = {11},
  number  = {1},
  pages   = {119--132},
  doi     = {10.1515/LINGTY.2007.011},
  urldate = {2024-05-14}
}
@article{haspelmath1996,
  title     = {Word-class-changing inflection and morphological theory},
  author    = {Haspelmath, Martin},
  journal   = {Yearbook of morphology 1995},
  pages     = {43--66},
  year      = {1996},
  publisher = {Springer}
}
@article{haspelmath-2024-inflection,
  url         = {https://doi.org/10.1515/ling-2022-0086},
  title       = {Inflection and derivation as traditional comparative concepts},
  author      = {Martin Haspelmath},
  pages       = {43--77},
  volume      = {62},
  number      = {1},
  journal     = {Linguistics},
  doi         = {doi:10.1515/ling-2022-0086},
  year        = {2024},
  lastchecked = {2024-04-24}
}
@inproceedings{hessel-quantifying-2018,
  title     = {Quantifying the {{Visual Concreteness}} of {{Words}} and {{Topics}} in {{Multimodal Datasets}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  author    = {Hessel, Jack and Mimno, David and Lee, Lillian},
  editor    = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
  year      = {2018},
  month     = jun,
  pages     = {2194--2205},
  publisher = {Association for Computational Linguistics},
  address   = {New Orleans, Louisiana},
  doi       = {10.18653/v1/N18-1199},
  urldate   = {2024-07-23},
  abstract  = {Multimodal machine learning algorithms aim to learn visual-textual correspondences. Previous work suggests that concepts with concrete visual manifestations may be easier to learn than concepts with abstract ones. We give an algorithm for automatically computing the visual concreteness of words and topics within multimodal datasets. We apply the approach in four settings, ranging from image captions to images/text scraped from historical books. In addition to enabling explorations of concepts in multimodal datasets, our concreteness scores predict the capacity of machine learning algorithms to learn textual/visual relationships. We find that 1) concrete concepts are indeed easier to learn; 2) the large number of algorithms we consider have similar failure cases; 3) the precise positive relationship between concreteness and performance varies between datasets. We conclude with recommendations for using concreteness scores to facilitate future multimodal research.},
  file      = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Hessel et al_2018_Quantifying the Visual Concreteness of Words and Topics in Multimodal Datasets.pdf}
}
@article{hsieh-distinguishing-2019,
  title      = {Distinguishing Nouns and Verbs: {{A Tagalog}} Case Study},
  shorttitle = {Distinguishing Nouns and Verbs},
  author     = {Hsieh, Henrison},
  year       = {2019},
  month      = may,
  journal    = {Natural Language \& Linguistic Theory},
  volume     = {37},
  number     = {2},
  pages      = {523--569},
  issn       = {0167-806X, 1573-0859},
  doi        = {10.1007/s11049-018-9422-3},
  urldate    = {2024-10-07},
  langid     = {english}
}
@article{HuDongDaiTong+2017,
  url         = {https://doi.org/10.1515/ijb-2017-0013},
  title       = {A Comparison of Methods for Estimating the Determinant of High-Dimensional Covariance Matrix},
  author      = {Zongliang Hu and Kai Dong and Wenlin Dai and Tiejun Tong},
  pages       = {20170013},
  volume      = {13},
  number      = {2},
  journal     = {The International Journal of Biostatistics},
  doi         = {doi:10.1515/ijb-2017-0013},
  year        = {2017},
  lastchecked = {2023-02-15}
}
@article{kaufman-austronesian-2009,
  title   = {Austronesian {{Nominalism}} and Its Consequences: {{A Tagalog}} Case Study},
  author  = {Kaufman, Daniel},
  year    = {2009},
  journal = {Theoretical {{Linguistics}}},
  volume  = {35},
  number  = {1},
  pages   = {1--49},
  doi     = {10.1515/THLI.2009.001},
  urldate = {2024-05-15}
}
@article{kirkici_clahsen_2013,
  title     = {Inflection and derivation in native and non-native language processing: Masked priming experiments on Turkish},
  volume    = {16},
  doi       = {10.1017/S1366728912000648},
  number    = {4},
  journal   = {Bilingualism: Language and Cognition},
  publisher = {Cambridge University Press},
  author    = {Kirkici, Bilal and Clahsen, Harald},
  year      = {2013},
  pages     = {776–791}
}
@article{kolgomorov-smirnov,
  author    = { Frank J.   Massey   Jr. },
  title     = {The {K}olmogorov-{S}mirnov Test for Goodness of Fit},
  journal   = {Journal of the American Statistical Association},
  volume    = {46},
  number    = {253},
  pages     = {68-78},
  year      = {1951},
  publisher = {Taylor & Francis},
  doi       = {10.1080/01621459.1951.10500769},
  url       = { 
               https://www.tandfonline.com/doi/abs/10.1080/01621459.1951.10500769
               
               },
  eprint    = { 
               https://www.tandfonline.com/doi/pdf/10.1080/01621459.1951.10500769
               
               }
}
@article{konig2006marked,
  title     = {Marked nominative in Africa},
  author    = {K{\"o}nig, Christa},
  journal   = {Studies in Language. International Journal sponsored by the Foundation “Foundations of Language”},
  volume    = {30},
  number    = {4},
  pages     = {655--732},
  year      = {2006},
  publisher = {John Benjamins}
}



@inproceedings{koper-automatically-2016,
  title     = {Automatically {{Generated Affective Norms}} of {{Abstractness}}, {{Arousal}}, {{Imageability}} and {{Valence}} for 350 000 {{German Lemmas}}},
  booktitle = {Proceedings of the {{Tenth International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}}'16)},
  author    = {K{\"o}per, Maximilian and {Schulte im Walde}, Sabine},
  editor    = {Calzolari, Nicoletta and Choukri, Khalid and Declerck, Thierry and Goggi, Sara and Grobelnik, Marko and Maegaard, Bente and Mariani, Joseph and Mazo, Helene and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
  year      = {2016},
  month     = may,
  pages     = {2595--2598},
  publisher = {European Language Resources Association (ELRA)},
  address   = {Portoro{\v z}, Slovenia},
  urldate   = {2024-10-07},
  abstract  = {This paper presents a collection of 350,000 German lemmatised words, rated on four psycholinguistic affective attributes. All ratings were obtained via a supervised learning algorithm that can automatically calculate a numerical rating of a word. We applied this algorithm to abstractness, arousal, imageability and valence. Comparison with human ratings reveals high correlation across all rating types. The full resource is publically available at: http://www.ims.uni-stuttgart.de/data/affective\_norms/},
  file      = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Köper_Schulte im Walde_2016_Automatically Generated Affective Norms of Abstractness, Arousal, Imageability.pdf}
}

@inproceedings{kumarsouheil2017,
  author    = {Kasthuri, M. and Kumar, S. Britto Ramesh and Khaddaj, Souheil},
  booktitle = {2017 World Congress on Computing and Communication Technologies (WCCCT)},
  title     = {PLIS: Proposed Language Independent Stemmer for Information Retrieval Systems Using Dynamic Programming},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {132-135},
  doi       = {10.1109/WCCCT.2016.39}
}
@article{laksnamer2022,
  title   = {Hebrewnette--A New Derivational Resource for Non-concatenative Morphology: Principles, Design and Implementation},
  author  = {Laks, Lior and Namer, Fiammetta},
  journal = {The Prague Bulletin of Mathematical Linguistics},
  volume  = {118},
  pages   = {25--53},
  year    = {2022}
}

@incollection{larasati2011indonesian,
  title     = {Indonesian Morphology Tool ({{MorphInd}}): {{Towards}} an Indonesian Corpus},
  booktitle = {Systems and Frameworks for Computational Morphology},
  author    = {Larasati, Septina Dian and Kubo{\v n}, Vladislav and Zeman, Daniel},
  editor    = {Mahlow, Cerstin and Piotrowski, Michael},
  year      = {2011},
  pages     = {119--129},
  publisher = {{Springer Berlin Heidelberg}},
  address   = {{Berlin, Heidelberg}},
  doi       = {10.1007/978-3-642-23138-4_8}
}

@article{laudanna-et-al-1992-processing,
  title     = {Processing inflectional and derivational morphology},
  author    = {Laudanna, Alessandro and Badecker, William and Caramazza, Alfonso},
  journal   = {Journal of Memory and Language},
  volume    = {31},
  number    = {3},
  pages     = {333--348},
  year      = {1992},
  publisher = {Elsevier}
}

@article{levenshtein,
  title   = {Binary Codes Capable of Correcting Deletions, Insertions and Reversals},
  author  = {Vladimir Levenshtein},
  journal = {Soviet Physics Doklady},
  volume  = {10},
  pages   = {707},
  year    = {1966}
}
@inproceedings{levshina-2020-how,
  title      = {How Tight Is Your Language? {{A}} Semantic Typology Based on {{Mutual Information}}},
  shorttitle = {How Tight Is Your Language?},
  booktitle  = {Proceedings of the 19th {{International Workshop}} on {{Treebanks}} and {{Linguistic Theories}}},
  author     = {Levshina, Natalia},
  editor     = {Evang, Kilian and Kallmeyer, Laura and Ehren, Rafael and Petitjean, Simon and Seyffarth, Esther and Seddah, Djam{\'e}},
  year       = {2020},
  month      = oct,
  pages      = {70--78},
  publisher  = {Association for Computational Linguistics},
  address    = {D{\"u}sseldorf, Germany},
  doi        = {10.18653/v1/2020.tlt-1.7},
  urldate    = {2025-05-14},
  file       = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Levshina_2020_How tight is your language.pdf}
}
@article{liljencrants-numerical-1972,
  title      = {Numerical {{Simulation}} of {{Vowel Quality Systems}}: {{The Role}} of {{Perceptual Contrast}}},
  shorttitle = {Numerical {{Simulation}} of {{Vowel Quality Systems}}},
  author     = {Liljencrants, Johan and Lindblom, Bj{\"o}rn and Lindblom, Bjorn},
  year       = {1972},
  month      = dec,
  journal    = {Language},
  volume     = {48},
  number     = {4},
  eprint     = {411991},
  eprinttype = {jstor},
  pages      = {839--862},
  issn       = {00978507},
  doi        = {10.2307/411991},
  urldate    = {2024-10-07}
}
@article{lin-word-2022,
  title    = {Word Imageability Is Associated with Expressive Vocabulary in Children with Autism Spectrum Disorder},
  author   = {Lin, Kimberly R and Wisman Weil, Lisa and Thurm, Audrey and Lord, Catherine and Luyster, Rhiannon J},
  year     = {2022},
  month    = mar,
  journal  = {Autism \& Developmental Language Impairments},
  volume   = {7},
  issn     = {2396-9415},
  doi      = {10.1177/23969415221085827},
  urldate  = {2024-07-23},
  abstract = {Background \& aims Throughout typical development, children prioritize different perceptual, social, and linguistic cues to learn words. The earliest acquired words are often those that are perceptually salient and highly imageable. Imageability, the ease in which a word evokes a mental image, is a strong predictor for word age of acquisition in typically developing (TD) children, independent of other lexicosemantic features such as word frequency. However, little is known about the effects of imageability in children with autism spectrum disorder (ASD), who tend to have differences in linguistic processing and delayed language acquisition compared to their TD peers. This study explores the extent to which imageability and word frequency are associated with early noun and verb acquisition in children with ASD. Methods Secondary analyses were conducted on previously collected data of 156 children (78 TD, 78 ASD) matched on sex and parent-reported language level. Total expressive vocabulary, as measured by the MacArthur Bates Communicative Development Inventory (MB-CDI), included 123 words (78 nouns, 45 verbs) that overlapped with previously published imageability ratings and word input frequencies. A two-step hierarchical linear regression was used to examine the relationship between word input frequency, imageability, and total expressive vocabulary. An F-test was then used to assess the unique contribution of imageability on total expressive vocabulary when controlling for word input frequency. Results In both the TD and ASD groups, imageability uniquely explained a portion of the variance in total expressive vocabulary size, independent of word input frequency. Notably, imageability was significantly associated with noun vocabulary and verb vocabulary size alone, with imageability explaining a greater portion of the variance in total nouns produced than in total verbs produced. Conclusions Imageability was identified as a significant lexicosemantic feature for describing expressive vocabulary size in children with ASD. Consistent with literature on TD children, children with ASD who have small vocabularies primarily produce words that are highly imageable. Children who are more proficient word learners with larger vocabularies produce words that are less imageable, indicating a potential shift away from reliance on perceptual-based language processing. This was consistent across both noun and verb vocabularies. Implications Our findings contribute to a growing body of literature describing early word learning in children with ASD and provide a basis for exploring the use of multisensory language learning strategies.},
  pmcid    = {PMC9620684},
  pmid     = {36382067},
  file     = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Lin et al_2022_Word imageability is associated with expressive vocabulary in children with.pdf}
}

@inproceedings{ljubesic-predicting-2018,
  title     = {Predicting {{Concreteness}} and {{Imageability}} of {{Words Within}} and {{Across Languages}} via {{Word Embeddings}}},
  booktitle = {Proceedings of the {{Third Workshop}} on {{Representation Learning}} for {{NLP}}},
  author    = {Ljube{\v s}i{\'c}, Nikola and Fi{\v s}er, Darja and {Peti-Stanti{\'c}}, Anita},
  editor    = {Augenstein, Isabelle and Cao, Kris and He, He and Hill, Felix and Gella, Spandana and Kiros, Jamie and Mei, Hongyuan and Misra, Dipendra},
  year      = {2018},
  month     = jul,
  pages     = {217--222},
  publisher = {Association for Computational Linguistics},
  address   = {Melbourne, Australia},
  doi       = {10.18653/v1/W18-3028},
  urldate   = {2024-10-07},
  abstract  = {The notions of concreteness and imageability, traditionally important in psycholinguistics, are gaining significance in semantic-oriented natural language processing tasks. In this paper we investigate the predictability of these two concepts via supervised learning, using word embeddings as explanatory variables. We perform predictions both within and across languages by exploiting collections of cross-lingual embeddings aligned to a single vector space. We show that the notions of concreteness and imageability are highly predictable both within and across languages, with a moderate loss of up to 20\% in correlation when predicting across languages. We further show that the cross-lingual transfer via word embeddings is more efficient than the simple transfer via bilingual dictionaries.},
  file      = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Ljubešić et al_2018_Predicting Concreteness and Imageability of Words Within and Across Languages.pdf}
}
@article{lynott-lancaster-2020,
  title      = {The {{Lancaster Sensorimotor Norms}}: Multidimensional Measures of Perceptual and Action Strength for 40,000 {{English}} Words},
  shorttitle = {The {{Lancaster Sensorimotor Norms}}},
  author     = {Lynott, Dermot and Connell, Louise and Brysbaert, Marc and Brand, James and Carney, James},
  year       = {2020},
  month      = jun,
  journal    = {Behavior Research Methods},
  volume     = {52},
  number     = {3},
  pages      = {1271--1291},
  issn       = {1554-3528},
  doi        = {10.3758/s13428-019-01316-z},
  urldate    = {2024-10-15},
  abstract   = {Sensorimotor information plays a fundamental role in cognition. However, the existing materials that measure the sensorimotor basis of word meanings and concepts have been restricted in terms of their sample size and breadth of sensorimotor experience. Here we present norms of sensorimotor strength for 39,707 concepts across six perceptual modalities (touch, hearing, smell, taste, vision, and interoception) and five action effectors (mouth/throat, hand/arm, foot/leg, head excluding mouth/throat, and torso), gathered from a total of 3,500 individual participants using Amazon's Mechanical Turk platform. The Lancaster Sensorimotor Norms are unique and innovative in a number of respects: They represent the largest-ever set of semantic norms for English, at 40,000 words {\texttimes} 11 dimensions (plus several informative cross-dimensional variables), they extend perceptual strength norming to the new modality of interoception, and they include the first norming of action strength across separate bodily effectors. In the first study, we describe the data collection procedures, provide summary descriptives of the dataset, and interpret the relations observed between sensorimotor dimensions. We then report two further studies, in which we (1) extracted an optimal single-variable composite of the 11-dimension sensorimotor profile (Minkowski 3 strength) and (2) demonstrated the utility of both perceptual and action strength in facilitating lexical decision times and accuracy in two separate datasets. These norms provide a valuable resource to researchers in diverse areas, including psycholinguistics, grounded cognition, cognitive semantics, knowledge representation, machine learning, and big-data approaches to the analysis of language and conceptual representations. The data are accessible via the Open Science Framework (http://osf.io/7emr6/) and an interactive web application (https://www.lancaster.ac.uk/psychology/lsnorms/).},
  langid     = {english},
  file       = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Lynott et al_2020_The Lancaster Sensorimotor Norms.pdf}
}
@article{mackay-1978-derivational,
  title     = {Derivational rules and the internal lexicon},
  author    = {MacKay, Donald G},
  journal   = {Journal of verbal learning and verbal behavior},
  volume    = {17},
  number    = {1},
  pages     = {61--71},
  year      = {1978},
  publisher = {Elsevier}
}
@inproceedings{malouf2020,
  title     = {Lexical databases for computational analyses: A linguistic perspective},
  author    = {Malouf, Robert  and
               Ackerman, Farrell  and
               Semenuks, Arturs},
  editor    = {Ettinger, Allyson  and
               Jarosz, Gaja  and
               Pater, Joe},
  booktitle = {Proceedings of the Society for Computation in Linguistics 2020},
  month     = jan,
  year      = {2020},
  address   = {New York, New York},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.scil-1.52},
  pages     = {446--456}
}
@misc{martinez-using-2024,
  title      = {Using Large Language Models to Estimate Features of Multi-Word Expressions: {{Concreteness}}, Valence, Arousal},
  shorttitle = {Using Large Language Models to Estimate Features of Multi-Word Expressions},
  author     = {Mart{\'i}nez, Gonzalo and Molero, Juan Diego and Gonz{\'a}lez, Sandra and Conde, Javier and Brysbaert, Marc and Reviriego, Pedro},
  year       = {2024}
}
@inproceedings{mccarthy-etal-2020-unimorp,
  title        = {UniMorph 3.0: Universal Morphology},
  author       = {McCarthy, Arya D and Kirov, Christo and Grella, Matteo and Nidhi, Amrit and Xia, Patrick and Gorman, Kyle and Vylomova, Ekaterina and Mielke, Sabrina J and Nicolai, Garrett and Silfverberg, Miikka and others},
  booktitle    = {Proceedings of The 12th language resources and evaluation conference},
  pages        = {3922--3931},
  year         = {2020},
  organization = {European Language Resources Association}
}
@inproceedings{mccarthy-etal-2020-unimorph,
  title     = {{U}ni{M}orph 3.0: {U}niversal {M}orphology},
  author    = {McCarthy, Arya D.  and
               Kirov, Christo  and
               Grella, Matteo  and
               Nidhi, Amrit  and
               Xia, Patrick  and
               Gorman, Kyle  and
               Vylomova, Ekaterina  and
               Mielke, Sabrina J.  and
               Nicolai, Garrett  and
               Silfverberg, Miikka  and
               Arkhangelskiy, Timofey  and
               Krizhanovsky, Nataly  and
               Krizhanovsky, Andrew  and
               Klyachko, Elena  and
               Sorokin, Alexey  and
               Mansfield, John  and
               Ern{\v{s}}treits, Valts  and
               Pinter, Yuval  and
               Jacobs, Cassandra L.  and
               Cotterell, Ryan  and
               Hulden, Mans  and
               Yarowsky, David},
  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
  month     = may,
  year      = {2020},
  address   = {Marseille, France},
  publisher = {European Language Resources Association},
  url       = {https://aclanthology.org/2020.lrec-1.483},
  pages     = {3922--3931},
  abstract  = {The Universal Morphology (UniMorph) project is a collaborative effort providing broad-coverage instantiated normalized morphological paradigms for hundreds of diverse world languages. The project comprises two major thrusts: a language-independent feature schema for rich morphological annotation and a type-level resource of annotated data in diverse languages realizing that schema. We have implemented several improvements to the extraction pipeline which creates most of our data, so that it is both more complete and more correct. We have added 66 new languages, as well as new parts of speech for 12 languages. We have also amended the schema in several ways. Finally, we present three new community tools: two to validate data for resource creators, and one to make morphological data available from the command line. UniMorph is based at the Center for Language and Speech Processing (CLSP) at Johns Hopkins University in Baltimore, Maryland. This paper details advances made to the schema, tooling, and dissemination of project resources since the UniMorph 2.0 release described at LREC 2018.},
  language  = {English},
  isbn      = {979-10-95546-34-4}
}
@misc{minixhofer-zeroshot-2024,
  title         = {Zero-{{Shot Tokenizer Transfer}}},
  author        = {Minixhofer, Benjamin and Ponti, Edoardo Maria and Vuli{\'c}, Ivan},
  year          = {2024},
  month         = may,
  number        = {arXiv:2405.07883},
  eprint        = {2405.07883},
  primaryclass  = {cs},
  publisher     = {arXiv},
  urldate       = {2024-05-15},
  abstract      = {Language models (LMs) are bound to their tokenizer, which maps raw text to a sequence of vocabulary items (tokens). This restricts their flexibility: for example, LMs trained primarily on English may still perform well in other natural and programming languages, but have vastly decreased efficiency due to their English-centric tokenizer. To mitigate this, we should be able to swap the original LM tokenizer with an arbitrary one, on the fly, without degrading performance. Hence, in this work we define a new problem: Zero-Shot Tokenizer Transfer (ZeTT). The challenge at the core of ZeTT is finding embeddings for the tokens in the vocabulary of the new tokenizer. Since prior heuristics for initializing embeddings often perform at chance level in a ZeTT setting, we propose a new solution: we train a hypernetwork taking a tokenizer as input and predicting the corresponding embeddings. We empirically demonstrate that the hypernetwork generalizes to new tokenizers both with encoder (e.g., XLM-R) and decoder LLMs (e.g., Mistral-7B). Our method comes close to the original models' performance in cross-lingual and coding tasks while markedly reducing the length of the tokenized sequence. We also find that the remaining gap can be quickly closed by continued training on less than 1B tokens. Finally, we show that a ZeTT hypernetwork trained for a base (L)LM can also be applied to fine-tuned variants without extra training. Overall, our results make substantial strides toward detaching LMs from their tokenizer.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computation and Language},
  file          = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Minixhofer et al_2024_Zero-Shot Tokenizer Transfer.pdf;/Users/coleman/Zotero/storage/63WZHVK7/2405.html}
}
  month = aug,
  journal = {arXiv.org},
  urldate = {2024-10-07},
  abstract = {This study investigates the potential of large language models (LLMs) to provide accurate estimates of concreteness, valence and arousal for multi-word expressions. Unlike previous artificial intelligence (AI) methods, LLMs can capture the nuanced meanings of multi-word expressions. We systematically evaluated ChatGPT-4o's ability to predict concreteness, valence and arousal. In Study 1, ChatGPT-4o showed strong correlations with human concreteness ratings (r = .8) for multi-word expressions. In Study 2, these findings were repeated for valence and arousal ratings of individual words, matching or outperforming previous AI models. Study 3 extended the prevalence and arousal analysis to multi-word expressions and showed promising results despite the lack of large-scale human benchmarks. These findings highlight the potential of LLMs for generating valuable psycholinguistic data related to multiword expressions. To help researchers with stimulus selection, we provide datasets with AI norms of concreteness, valence and arousal for 126,397 English single words and 63,680 multi-word expressions},
  howpublished = {https://arxiv.org/abs/2408.16012v1},
  langid = {english},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Martínez et al_2024_Using large language models to estimate features of multi-word expressions.pdf}
}
@inproceedings{morphynet,
  title     = {{M}orphy{N}et: a Large Multilingual Database of Derivational and Inflectional Morphology},
  author    = {Batsuren, Khuyagbaatar  and
               Bella, G{\'a}bor  and
               Giunchiglia, Fausto},
  booktitle = {Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology},
  month     = aug,
  year      = {2021},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.sigmorphon-1.5},
  doi       = {10.18653/v1/2021.sigmorphon-1.5},
  pages     = {39--48},
  abstract  = {Large-scale morphological databases provide essential input to a wide range of NLP applications. Inflectional data is of particular importance for morphologically rich (agglutinative and highly inflecting) languages, and derivations can be used, e.g. to infer the semantics of out-of-vocabulary words. Extending the scope of state-of-the-art multilingual morphological databases, we announce the release of MorphyNet, a high-quality resource with 15 languages, 519k derivational and 10.1M inflectional entries, and a rich set of morphological features. MorphyNet was extracted from Wiktionary using both hand-crafted and automated methods, and was manually evaluated to be of a precision higher than 98{\%}. Both the resource generation logic and the resulting database are made freely available and are reusable as stand-alone tools or in combination with existing resources.}
}
@inproceedings{mscoco,
  author    = {Lin, Tsung-Yi
               and Maire, Michael
               and Belongie, Serge
               and Hays, James
               and Perona, Pietro
               and Ramanan, Deva
               and Doll{\'a}r, Piotr
               and Zitnick, C. Lawrence},
  editor    = {Fleet, David
               and Pajdla, Tomas
               and Schiele, Bernt
               and Tuytelaars, Tinne},
  title     = {Microsoft COCO: Common Objects in Context},
  booktitle = {Computer Vision -- ECCV 2014},
  year      = {2014},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {740--755},
  abstract  = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  isbn      = {978-3-319-10602-1}
}

@article{multisimlex,
  title    = {Multi-{S}im{L}ex: A Large-Scale Evaluation of Multilingual and Crosslingual Lexical Semantic Similarity},
  author   = {Vuli{\'c}, Ivan  and
              Baker, Simon  and
              Ponti, Edoardo Maria  and
              Petti, Ulla  and
              Leviant, Ira  and
              Wing, Kelly  and
              Majewska, Olga  and
              Bar, Eden  and
              Malone, Matt  and
              Poibeau, Thierry  and
              Reichart, Roi  and
              Korhonen, Anna},
  journal  = {Computational Linguistics},
  volume   = {46},
  number   = {4},
  month    = dec,
  year     = {2020},
  url      = {https://aclanthology.org/2020.cl-4.5},
  doi      = {10.1162/coli_a_00391},
  pages    = {847--897},
  abstract = {We introduce Multi-SimLex, a large-scale lexical resource and evaluation benchmark covering data sets for 12 typologically diverse languages, including major languages (e.g., Mandarin Chinese, Spanish, Russian) as well as less-resourced ones (e.g., Welsh, Kiswahili). Each language data set is annotated for the lexical relation of semantic similarity and contains 1,888 semantically aligned concept pairs, providing a representative coverage of word classes (nouns, verbs, adjectives, adverbs), frequency ranks, similarity intervals, lexical fields, and concreteness levels. Additionally, owing to the alignment of concepts across languages, we provide a suite of 66 crosslingual semantic similarity data sets. Because of its extensive size and language coverage, Multi-SimLex provides entirely novel opportunities for experimental evaluation and analysis. On its monolingual and crosslingual benchmarks, we evaluate and analyze a wide array of recent state-of-the-art monolingual and crosslingual representation models, including static and contextualized word embeddings (such as fastText, monolingual and multilingual BERT, XLM), externally informed lexical representations, as well as fully unsupervised and (weakly) supervised crosslingual word embeddings. We also present a step-by-step data set creation protocol for creating consistent, Multi-Simlex{--}style resources for additional languages. We make these contributions{---}the public release of Multi-SimLex data sets, their creation protocol, strong baseline results, and in-depth analyses which can be helpful in guiding future developments in multilingual lexical semantics and representation learning{---}available via a Web site that will encourage community effort in further expansion of Multi-Simlex to many more languages. Such a large-scale semantic resource could inspire significant further advances in NLP across languages.}
}
@article{narasimhan_unsupervised_2015,
  title   = {An unsupervised method for uncovering morphological chains},
  volume  = {3},
  journal = {Transactions of the Association for Computational Linguistics},
  author  = {Narasimhan, Karthik and Barzilay, Regina and Jaakkola, Tommi},
  month   = {December},
  year    = {2015},
  pages   = {157--167}
}

@article{newmeyer-2007-linguistic,
  title   = {Linguistic Typology Requires Crosslinguistic Formal Categories},
  author  = {Newmeyer, Frederick J},
  year    = {2007},
  series  = {Linguistic {{Typology}}},
  volume  = {11},
  number  = {1},
  pages   = {133--157},
  doi     = {10.1515/LINGTY.2007.012},
  urldate = {2024-05-14}
}
@misc{oh2024leadingwhitespaceslanguagemodels,
  title         = {Leading Whitespaces of Language Models' Subword Vocabulary Poses a Confound for Calculating Word Probabilities},
  author        = {Byung-Doh Oh and William Schuler},
  year          = {2024},
  eprint        = {2406.10851},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2406.10851}
}
@inproceedings{oscar,
  author    = {Julien Abadji and Pedro Javier Ortiz Su{\'a}rez and Laurent Romary and Beno{\^i}t Sagot},
  title     = {Ungoliant: An optimized pipeline for the generation of a very large-scale multilingual web corpus},
  series    = {Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-9) 2021. Limerick, 12 July 2021 (Online-Event)},
  editor    = {Harald L{\"u}ngen and Marc Kupietz and Piotr Bański and Adrien Barbaresi and Simon Clematide and Ines Pisetta},
  publisher = {Leibniz-Institut f{\"u}r Deutsche Sprache},
  address   = {Mannheim},
  doi       = {10.14618/ids-pub-10468},
  url       = {https://nbn-resolving.org/urn:nbn:de:bsz:mh39-104688},
  pages     = {1 -- 9},
  year      = {2021},
  abstract  = {Since the introduction of large language models in Natural Language Processing, large raw corpora have played a crucial role in Computational Linguistics. However, most of these large raw corpora are either available only for English or not available to the general public due to copyright issues. Nevertheless, there are some examples of freely available multilingual corpora for training Deep Learning NLP models, such as the OSCAR and Paracrawl corpora. However, they have quality issues, especially for low-resource languages. Moreover, recreating or updating these corpora is very complex. In this work, we try to reproduce and improve the goclassy pipeline used to create the OSCAR corpus. We propose a new pipeline that is faster, modular, parameterizable, and well documented. We use it to create a corpus similar to OSCAR but larger and based on recent data. Also, unlike OSCAR, the metadata information is at the document level. We release our pipeline under an open source license and publish the corpus under a research-only license.},
  language  = {en}
}
@inproceedings{ostling-2015-word,
  title     = {Word {{Order Typology}} through {{Multilingual Word Alignment}}},
  booktitle = {Proceedings of the 53rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 7th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 2: {{Short Papers}})},
  author    = {{\"O}stling, Robert},
  editor    = {Zong, Chengqing and Strube, Michael},
  year      = {2015},
  month     = jul,
  pages     = {205--211},
  publisher = {Association for Computational Linguistics},
  address   = {Beijing, China},
  doi       = {10.3115/v1/P15-2034},
  urldate   = {2025-05-14},
  file      = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Östling_2015_Word Order Typology through Multilingual Word Alignment.pdf}
}
@incollection{oxford,
  author    = {Bisang, Walter},
  isbn      = {9780199281251},
  title     = {{Word Classes}},
  booktitle = {{The Oxford Handbook of Linguistic Typology}},
  publisher = {Oxford University Press},
  year      = {2010},
  month     = {11},
  abstract  = {{This article introduces the four prerequisites for distinguishing word classes: semantic criteria; pragmatic criteria/criteria of discourse function; formal criteria; and distinction between lexical and syntactic levels of analysis. The most important approaches to word classes based on the first three prerequisites are addressed. The article also deals with the distinction between content words and function words. It then takes up the discussion of the universal status of the noun/verb distinction by integrating the fourth prerequisite. The languages discussed are Classical Nahuatl, Late Archaic Chinese, and Tongan. The distinction between content words and function words is not identical to the distinction between open and closed word classes. The article reviews Dixon's seminal approach to adjectives. The sub-classes of adverbs are considered. The definition of word classes integrates all the central elements that make language structure, and it integrates a whole paradigm of constructions.}},
  doi       = {10.1093/oxfordhb/9780199281251.013.0015},
  url       = {https://doi.org/10.1093/oxfordhb/9780199281251.013.0015},
  eprint    = {https://academic.oup.com/book/0/chapter/335277661/chapter-ag-pdf/44444033/book\_38630\_section\_335277661.ag.pdf}
}
@inproceedings{pawley2006where,
  title     = {Where Have All the Verbs Gone? {{Remarks}} on the Organisation of Languages with Small, Closed Verb Classes},
  booktitle = {11th Biennial Rice University Linguistics Symposium},
  author    = {Pawley, Andrew K.},
  year      = {2006}
}
@article{perlmutter-1988-split,
  title     = {The split morphology hypothesis: Evidence from Yiddish},
  author    = {Perlmutter, David},
  journal   = {Theoretical morphology},
  pages     = {79--100},
  year      = {1988},
  publisher = {Academic Press San Diego, CA}
}


@misc{pimentel2024computeprobabilityword,
  title         = {How to Compute the Probability of a Word},
  author        = {Tiago Pimentel and Clara Meister},
  year          = {2024},
  eprint        = {2406.14561},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2406.14561}
}
@article{plank-extent-2017,
  title   = {Extent and Limits of Linguistic Diversity as the Remit of Typology -- but through Constraints on What Is Diversity Limited?},
  author  = {Plank, Frans},
  year    = {2017},
  journal = {Linguistic Typology},
  volume  = {21},
  number  = {2017},
  pages   = {43--68},
  doi     = {doi:10.1515/lingty-2017-1004},
  urldate = {2024-05-14}
}
@incollection{plank-inflection-1994,
  title     = {Inflection and Derivation},
  author    = {Frans Plank},
  booktitle = {The Encyclopedia of Language and Linguistics},
  publisher = {Elsevier Science and Technology},
  address   = {Amsterdam},
  year      = {1994},
  pages     = {1671--1679}
}

@incollection{plank-1994-inflection,
  title     = {Inflection and Derivation},
  author    = {Frans Plank},
  booktitle = {The Encyclopedia of Language and Linguistics},
  publisher = {Elsevier Science and Technology},
  address   = {Amsterdam},
  year      = {1994},
  pages     = {1671--1679}
}

@inproceedings{qi-stanza-2020,
  title      = {Stanza: {{A Python Natural Language Processing Toolkit}} for {{Many Human Languages}}},
  shorttitle = {Stanza},
  booktitle  = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{System Demonstrations}}},
  author     = {Qi, Peng and Zhang, Yuhao and Zhang, Yuhui and Bolton, Jason and Manning, Christopher D.},
  editor     = {Celikyilmaz, Asli and Wen, Tsung-Hsien},
  year       = {2020},
  month      = jul,
  pages      = {101--108},
  publisher  = {Association for Computational Linguistics},
  address    = {Online},
  doi        = {10.18653/v1/2020.acl-demos.14},
  urldate    = {2024-10-07},
  abstract   = {We introduce Stanza, an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, and named entity recognition. We have trained Stanza on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested. Additionally, Stanza includes a native Python interface to the widely used Java Stanford CoreNLP software, which further extends its functionality to cover other tasks such as coreference resolution and relation extraction. Source code, documentation, and pretrained models for 66 languages are available at https://stanfordnlp.github.io/stanza/.},
  file       = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Qi et al_2020_Stanza.pdf}
}

@misc{quz-fst,
  title     = {Analizador morf\'{o}logico de la lengua {Q}uechua basado en software libre {H}elsinkifinite-statetransducer ({HFST})},
  author    = {Vilca, Hugo David Calderon and Mariñ\'{o}, Flor Cagniy C\'{a}rdenas and Calderon, Edwin Fredy Mamani},
  year      = {2012},
  publisher = {COMTEL}
}

@article{richards-nouns-2009,
  title   = {Nouns, Verbs, and Hidden Structure in {{Tagalog}}},
  author  = {Richards, Norvin},
  year    = {2009},
  month   = jul,
  journal = {Theoretical Linguistics},
  volume  = {35},
  number  = {1},
  pages   = {139--152},
  issn    = {0301-4428, 1613-4060},
  doi     = {10.1515/THLI.2009.008},
  urldate = {2024-10-07},
  langid  = {english},
  file    = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Richards_2009_Nouns, verbs, and hidden structure in Tagalog.pdf}
}

@article{rofes-imageability-2018,
  title    = {Imageability Ratings across Languages},
  author   = {Rofes, Adri{\`a} and Zakari{\'a}s, Lilla and Ceder, Klaudia and Lind, Marianne and Johansson, Monica Blom and {de Aguiar}, V{\^a}nia and Bjeki{\'c}, Jovana and Fyndanis, Valantis and Gavarr{\'o}, Anna and Simonsen, Hanne Gram and Sacrist{\'a}n, Carlos Hern{\'a}ndez and Kambanaros, Maria and Kraljevi{\'c}, Jelena Kuva{\v c} and {Mart{\'i}nez-Ferreiro}, Silvia and Mavis, {\.I}lknur and Orellana, Carolina M{\'e}ndez and S{\"o}r, Ingrid and Luk{\'a}cs, {\'A}gnes and Tun{\c c}er, M{\"u}ge and Vuksanovi{\'c}, Jasmina and Ibarrola, Amaia Munarriz and Pourquie, Marie and Varlokosta, Spyridoula and Howard, David},
  year     = {2018},
  month    = jun,
  journal  = {Behavior Research Methods},
  volume   = {50},
  number   = {3},
  pages    = {1187--1197},
  issn     = {1554-3528},
  doi      = {10.3758/s13428-017-0936-0},
  abstract = {Imageability is a psycholinguistic variable that indicates how well a word gives rise to a mental image or sensory experience. Imageability ratings are used extensively in psycholinguistic, neuropsychological, and aphasiological studies. However, little formal knowledge exists about whether and how these ratings are associated between and within languages. Fifteen imageability databases were cross-correlated using nonparametric statistics. Some of these corresponded to unpublished data collected within a European research network---the Collaboration of Aphasia Trialists (COST IS1208). All but four of the correlations were significant. The average strength of the correlations (rho = .68) and the variance explained (R2 = 46\%) were moderate. This implies that factors other than imageability may explain 54\% of the results. Imageability ratings often correlate across languages. Different possibly interacting factors may explain the moderate strength and variance explained in the correlations: (1) linguistic and cultural factors; (2) intrinsic differences between the databases; (3) range effects; (4) small numbers of words in each database, equivalent words, and participants; and (5) mean age of the participants. The results suggest that imageability ratings may be used cross-linguistically. However, further understanding of the factors explaining the variance in the correlations will be needed before research and practical recommendations can be made.}
}

@inproceedings{Ross1972,
  author    = {Ross, John R.},
  title     = {The category squish: Endstation Hauptwort},
  booktitle = {Proceedings of the Eighth Regional Meeting of the Chicago Linguistic Society},
  editor    = {Paul M. Peranteau and Judith N. Levi and Gloria C. Phares},
  pages     = {316--328},
  year      = {1972},
  publisher = {Chicago Linguistic Society, University of Chicago},
  address   = {Chicago, Illinois}
}
@article{saffranetal1996,
  title     = {Statistical learning by 8-month-old infants},
  author    = {Saffran, Jenny R and Aslin, Richard N and Newport, Elissa L},
  journal   = {Science},
  volume    = {274},
  number    = {5294},
  pages     = {1926--1928},
  year      = {1996},
  publisher = {American Association for the Advancement of Science}
}
@article{schakel2015measuring,
  title   = {Measuring Word Significance using Distributed Representations of Words},
  author  = {Adriaan M. J. Schakel and Benjamin J. Wilson},
  journal = {Computing Research Repository},
  volume  = {arXiv:1508.02297},
  year    = {2015},
  url     = {http://arxiv.org/abs/1508.02297}
}
@phdthesis{schultze-berndt-simple-2000,
  title      = {Simple and Complex Verbs in {{Jaminjung}}: {{A}} Study of Event Categorisation in an {{Australian}} Language},
  shorttitle = {Simple and Complex Verbs in {{Jaminjung}}},
  author     = {{Schultze-Berndt}, Eva},
  year       = {2000},
  address    = {Nijmegen},
  urldate    = {2024-10-07},
  langid     = {english},
  school     = {Radboud University}
}

@article{schwartz-dispersionfocalization-1997,
  title     = {The {{Dispersion-Focalization Theory}} of Vowel Systems},
  author    = {Schwartz, Jean-Luc and Bo{\"e}, Louis-Jean and Vall{\'e}e, Nathalie and Abry, Christian},
  year      = {1997},
  month     = jul,
  journal   = {Journal of Phonetics},
  volume    = {25},
  number    = {3},
  pages     = {255--286},
  issn      = {00954470},
  doi       = {10.1006/jpho.1997.0043},
  urldate   = {2024-10-07},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid    = {english}
}
@article{scott-glasgow-2019,
  title      = {The {{Glasgow Norms}}: {{Ratings}} of 5,500 Words on Nine Scales},
  shorttitle = {The {{Glasgow Norms}}},
  author     = {Scott, Graham G. and Keitel, Anne and Becirspahic, Marc and Yao, Bo and Sereno, Sara C.},
  year       = {2019},
  month      = jun,
  journal    = {Behavior Research Methods},
  volume     = {51},
  number     = {3},
  pages      = {1258--1270},
  issn       = {1554-3528},
  doi        = {10.3758/s13428-018-1099-3},
  urldate    = {2024-10-15},
  abstract   = {The Glasgow Norms are a set of normative ratings for 5,553 English words on nine psycholinguistic dimensions: arousal, valence, dominance, concreteness, imageability, familiarity, age of acquisition, semantic size, and gender association. The Glasgow Norms are unique in several respects. First, the corpus itself is relatively large, while simultaneously providing norms across a substantial number of lexical dimensions. Second, for any given subset of words, the same participants provided ratings across all nine dimensions (33 participants/word, on average). Third, two novel dimensions---semantic size and gender association---are included. Finally, the corpus contains a set of 379 ambiguous words that are presented either alone (e.g., toast) or with information that selects an alternative sense (e.g., toast (bread), toast (speech)). The relationships between the dimensions of the Glasgow Norms were initially investigated by assessing their correlations. In addition, a principal component analysis revealed four main factors, accounting for 82\% of the variance (Visualization, Emotion, Salience, and Exposure). The validity of the Glasgow Norms was established via comparisons of our ratings to 18 different sets of current psycholinguistic norms. The dimension of size was tested with megastudy data, confirming findings from past studies that have explicitly examined this variable. Alternative senses of ambiguous words (i.e., disambiguated forms), when discordant on a given dimension, seemingly led to appropriately distinct ratings. Informal comparisons between the ratings of ambiguous words and of their alternative senses showed different patterns that likely depended on several factors (the number of senses, their relative strengths, and the rating scales themselves). Overall, the Glasgow Norms provide a valuable resource---in particular, for researchers investigating the role of word recognition in language comprehension.},
  langid     = {english},
  keywords   = {Age of acquisition,Arousal,Concreteness,Dominance,Familiarity,Gender association,Imageability,Psycholinguistic norms,Semantic size,Valence},
  file       = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Scott et al_2019_The Glasgow Norms.pdf}
}

@incollection{shopen_1985,
  place     = {Cambridge},
  title     = {Inflectional Morphology},
  author    = {Anderson, Stephen R.},
  editors   = {Timothy Shopen},
  edition   = {1},
  booktitle = {Language Typology and Syntactic Description},
  volume    = {3},
  publisher = {Cambridge University Press},
  year      = {1985},
  pages     = {150--201}
}
@inbook{silverstein1986,
  url         = {https://doi.org/10.1515/9783110871661-008},
  title       = {7. Hierarchy of Features and Ergativity},
  booktitle   = {Features and Projections},
  author      = {Michael Silverstein},
  publisher   = {De Gruyter Mouton},
  address     = {Berlin, Boston},
  pages       = {163--232},
  doi         = {doi:10.1515/9783110871661-008},
  isbn        = {9783110871661},
  year        = {1986},
  lastchecked = {2023-10-13}
}
@book{spencer,
  title     = {Lexical Relatedness},
  author    = {Andrew Spencer},
  publisher = {Oxford University Press},
  address   = {Oxford},
  year      = {2013}
}

@article{spreen-parameters-1966,
  title     = {Parameters of Abstraction, Meaningfulness, and Pronunciability for 329 Nouns},
  author    = {Spreen, Otfried and Schulz, Rudolph W.},
  year      = {1966},
  month     = oct,
  journal   = {Journal of Verbal Learning and Verbal Behavior},
  volume    = {5},
  number    = {5},
  pages     = {459--468},
  issn      = {00225371},
  doi       = {10.1016/S0022-5371(66)80061-0},
  urldate   = {2024-10-07},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid    = {english}
}

@book{stassen1997,
  title     = {Intransitive {Predication}},
  isbn      = {978-0-19-823693-1},
  url       = {https://doi.org/10.1093/oso/9780198236931.001.0001},
  abstract  = {Intransitive Predication constitutes a major contribution to the study of typological linguistics and theoretical linguistics in general. Basing his analysis on a sample of 410 languages, Leon Stassen investigates cross-linguistic variation in one of the core domains of all natural languages. The author views this domain as a `cognitive space', the topography of which is the same for all languages. It is assumed to consist of four subdomains, which correspond to a four-way distinction between the semantic classes of event predicates, property predicates, class predicates, and locational predicates. Leon Stassen offers a typology of the structural manifestations of this domain, in terms of the nature and number of the formal strategies used in its encoding. He discusses a number of abstract principles which can be employed in explaining the cross-linguistic variation embodied by the typology. In the final chapter, he brings together the research results in a universally applicable model, which can be read as a `flow chart' for the encoding of intransitive predications in different language types.},
  publisher = {Oxford University Press},
  author    = {Stassen, Leon},
  month     = sep,
  year      = {1997},
  doi       = {10.1093/oso/9780198236931.001.0001}
}

@article{staub-predictability-2024a,
  title      = {Predictability in {{Language Comprehension}}: {{Prospects}} and {{Problems}} for {{Surprisal}}},
  shorttitle = {Predictability in {{Language Comprehension}}},
  author     = {Staub, Adrian},
  year       = {Forthcoming},
  journal    = {Annual Review of Linguistics},
  publisher  = {Annual Reviews},
  doi        = {10.1146/annurev-linguistics-011724-121517},
  urldate    = {2024-10-14},
  abstract   = {Surprisal theory proposes that a word\&apos;s predictability influences processing difficulty because each word requires the comprehender to update a probability distribution over possible sentences. This article first considers the theory\&apos;s detailed predictions regarding the effects of predictability on reading time and N400 amplitude. Two rather unintuitive predictions appear to be correct based on the current evidence: There is no specific cost when an unpredictable word is encountered in a context where another word is predictable, and the function relating predictability to processing difficulty is logarithmic, not linear. Next, the article addresses the viability of the claim, also associated with Surprisal, that conditional probability is the ``causal bottleneck'' mediating all effects on incremental processing difficulty. This claim fares less well as conditional probability does not account for the difficulty associated with encountering a low-frequency word or the difficulty associated with garden path disambiguation. Surprisal provides a compelling account of predictability effects but does not provide a complete account of incremental processing difficulty.},
  langid     = {english},
  file       = {/Users/coleman/Zotero/storage/7LE5JCVZ/annurev-linguistics-011724-121517.html}
}

@incollection{stekauer2015,
  title     = {14. The delimitation of derivation and inflection},
  author    = {{\v{S}}tekauer, Pavol},
  editor    = {Peter O. Müller and Ingeborg Ohnheiser and Susan Olsen and Franz Rainer},
  booktitle = {Volume 1 Word-Formation},
  pages     = {218--235},
  year      = {2015},
  publisher = {De Gruyter Mouton}
}

@article{striklievers-linguistic-2021,
  title   = {The Linguistic Dimensions of Concrete and Abstract Concepts: Lexical Category, Morphological Structure, Countability, and Etymology},
  author  = {Strik Lievers, Francesca and Bolognesi, Marianna and Winter, Bodo},
  year    = {2021},
  journal = {Cognitive {{Linguistics}}},
  volume  = {32},
  number  = {4},
  pages   = {641--670},
  doi     = {10.1515/cog-2021-0007},
  urldate = {2024-05-15}
}

@book{strunk2020finite,
  title     = {A Finite-State Morphological Analyzer for Central Alaskan Yup'Ik},
  author    = {Strunk, Lonny Alaskuk},
  year      = {2020},
  publisher = {University of Washington}
}

@article{swingley2005,
  title     = {Statistical clustering and the contents of the infant vocabulary},
  author    = {Swingley, Daniel},
  journal   = {Cognitive psychology},
  volume    = {50},
  number    = {1},
  pages     = {86--132},
  year      = {2005},
  publisher = {Elsevier}
}

@misc{Sylak-Glassman2016,
  title  = {The Composition and Use of the Universal Morphological Feature Schema (UniMorph Schema)},
  author = {Sylak-Glassman, John},
  year   = {2016},
  url    = {https://unimorph.github.io/doc/unimorph-schema.pdf}
}


@book{tenhacken-1994-defining,
  title     = {Defining Morphology: A Principled Approach to Determining the Boundaries of Compounding, Derivation, and Inflection},
  author    = {Hacken, P.},
  isbn      = {9783487098913},
  lccn      = {lc95155890},
  series    = {Altertumswissenschaftliche Texte Und Studien},
  url       = {https://books.google.co.uk/books?id=E8mWh\_6mRAcC},
  year      = {1994},
  publisher = {G. Olms Verlag}
}
@inproceedings{thapliyal-crossmodal3600-2022,
  title      = {Crossmodal-3600: {{A Massively Multilingual Multimodal Evaluation Dataset}}},
  shorttitle = {Crossmodal-3600},
  booktitle  = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author     = {Thapliyal, Ashish V. and Pont Tuset, Jordi and Chen, Xi and Soricut, Radu},
  editor     = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
  year       = {2022},
  month      = dec,
  pages      = {715--729},
  publisher  = {Association for Computational Linguistics},
  address    = {Abu Dhabi, United Arab Emirates},
  doi        = {10.18653/v1/2022.emnlp-main.45},
  urldate    = {2024-10-07},
  abstract   = {Research in massively multilingual image captioning has been severely hampered by a lack of high-quality evaluation datasets. In this paper we present the Crossmodal-3600 dataset (XM3600 in short), a geographically diverse set of 3600 images annotated with human-generated reference captions in 36 languages. The images were selected from across the world, covering regions where the 36 languages are spoken, and annotated with captions that achieve consistency in terms of style across all languages, while avoiding annotation artifacts due to direct translation. We apply this benchmark to model selection for massively multilingual image captioning models, and show superior correlation results with human evaluations when using XM3600 as golden references for automatic metrics.},
  file       = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Thapliyal et al_2022_Crossmodal-3600.pdf;/Users/coleman/Zotero/storage/V7P868ZV/Thapliyal et al. - 2022 - Crossmodal-3600 A Massively Multilingual Multimod.pdf}
}

@article{theil-estimation-1970,
  title      = {On the {{Estimation}} of {{Relationships Involving Qualitative Variables}}},
  author     = {Theil, Henri},
  year       = {1970},
  journal    = {American Journal of Sociology},
  volume     = {76},
  number     = {1},
  eprint     = {2775440},
  eprinttype = {jstor},
  pages      = {103--154},
  publisher  = {The University of Chicago Press},
  issn       = {0002-9602},
  urldate    = {2024-10-15},
  abstract   = {This article is concerned with the specification and estimation of relationships whose dependent variable is qualitative in nature (such as "yes" or "no"). It discusses logit equations with and without interaction, and the estimation procedure is generalized least squares. Part I deals with dependent variables that take only two values, Par II with variables taking more than two values, and part III describes informational measures for the explanatory power of the determining factors. The discussion of more advanced technical matters is contained in various appendixes.},
  file       = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Theil_1970_On the Estimation of Relationships Involving Qualitative Variables.pdf}
}

@article{thiessenetal2013,
  title     = {The extraction and integration framework: a two-process account of statistical learning.},
  author    = {Thiessen, Erik D and Kronstein, Alexandra T and Hufnagle, Daniel G},
  journal   = {Psychological bulletin},
  volume    = {139},
  number    = {4},
  pages     = {792},
  year      = {2013},
  publisher = {American Psychological Association}
}

@article{thiessensaffran2003,
  title     = {When cues collide: use of stress and statistical cues to word boundaries by 7-to 9-month-old infants.},
  author    = {Thiessen, Erik D and Saffran, Jenny R},
  journal   = {Developmental psychology},
  volume    = {39},
  number    = {4},
  pages     = {706},
  year      = {2003},
  publisher = {American Psychological Association}
}

@article{thompsonnewport2007,
  title     = {Statistical learning of syntax: The role of transitional probability},
  author    = {Thompson, Susan P and Newport, Elissa L},
  journal   = {Language learning and development},
  volume    = {3},
  number    = {1},
  pages     = {1--42},
  year      = {2007},
  publisher = {Taylor \& Francis}
}

@article{demarneffe-et-al-2021-universal,
  title     = {Universal {{Dependencies}}},
  author    = {{de Marneffe}, Marie-Catherine and Manning, Christopher D. and Nivre, Joakim and Zeman, Daniel},
  year      = {2021},
  month     = jun,
  journal   = {Computational Linguistics},
  volume    = {47},
  number    = {2},
  pages     = {255--308},
  publisher = {MIT Press},
  address   = {Cambridge, MA},
  doi       = {10.1162/coli_a_00402},
  urldate   = {2025-05-14},
  abstract  = {Universal dependencies (UD) is a framework for morphosyntactic annotation of human language, which to date has been used to create treebanks for more than 100 languages. In this article, we outline the linguistic theory of the UD framework, which draws on a long tradition of typologically oriented grammatical theories. Grammatical relations between words are centrally used to explain how predicate--argument structures are encoded morphosyntactically in different languages while morphological features and part-of-speech classes give the properties of words. We argue that this theory is a good basis for crosslinguistically consistent annotation of typologically diverse languages in a way that supports computational natural language understanding as well as broader linguistic studies.},
  file      = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/de Marneffe et al_2021_Universal Dependencies2.pdf}
}

@article{unider,
  title     = {Universal {D}erivations 1.0, A Growing Collection of Harmonised Word-Formation Resources},
  author    = {Kyjánek, Lukáš and Žabokrtský, Zdeněk and Ševčíková, Magda and Vidra, Jonáš},
  journal   = {The Prague Bulletin of Mathematical Linguistics},
  number    = {115},
  volume    = {2},
  pages     = {333--348},
  year      = {2020},
  publisher = {Karolinum Press},
  address   = {Prague, Czech Republic}
}

@inproceedings{mccarthy-et-al-2020-unimorph,
  title     = {{U}ni{M}orph 3.0: {U}niversal {M}orphology},
  author    = {McCarthy, Arya D.  and
               Kirov, Christo  and
               Grella, Matteo  and
               Nidhi, Amrit  and
               Xia, Patrick  and
               Gorman, Kyle  and
               Vylomova, Ekaterina  and
               Mielke, Sabrina J.  and
               Nicolai, Garrett  and
               Silfverberg, Miikka  and
               Arkhangelskiy, Timofey  and
               Krizhanovsky, Nataly  and
               Krizhanovsky, Andrew  and
               Klyachko, Elena  and
               Sorokin, Alexey  and
               Mansfield, John  and
               Ern{\v{s}}treits, Valts  and
               Pinter, Yuval  and
               Jacobs, Cassandra L.  and
               Cotterell, Ryan  and
               Hulden, Mans  and
               Yarowsky, David},
  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
  month     = may,
  year      = {2020},
  address   = {Marseille, France},
  publisher = {European Language Resources Association},
  url       = {https://aclanthology.org/2020.lrec-1.483},
  pages     = {3922--3931},
  abstract  = {The Universal Morphology (UniMorph) project is a collaborative effort providing broad-coverage instantiated normalized morphological paradigms for hundreds of diverse world languages. The project comprises two major thrusts: a language-independent feature schema for rich morphological annotation and a type-level resource of annotated data in diverse languages realizing that schema. We have implemented several improvements to the extraction pipeline which creates most of our data, so that it is both more complete and more correct. We have added 66 new languages, as well as new parts of speech for 12 languages. We have also amended the schema in several ways. Finally, we present three new community tools: two to validate data for resource creators, and one to make morphological data available from the command line. UniMorph is based at the Center for Language and Speech Processing (CLSP) at Johns Hopkins University in Baltimore, Maryland. This paper details advances made to the schema, tooling, and dissemination of project resources since the UniMorph 2.0 release described at LREC 2018.},
  language  = {English},
  isbn      = {979-10-95546-34-4}
}
@inproceedings{batsuren-et-al-2022-unimorph,
  title     = {{U}ni{M}orph 4.0: {U}niversal {M}orphology},
  author    = {Batsuren, Khuyagbaatar  and
               Goldman, Omer  and
               Khalifa, Salam  and
               Habash, Nizar  and
               Kiera{\'s}, Witold  and
               Bella, G{\'a}bor  and
               Leonard, Brian  and
               Nicolai, Garrett  and
               Gorman, Kyle  and
               Ate, Yustinus Ghanggo  and
               Ryskina, Maria  and
               Mielke, Sabrina  and
               Budianskaya, Elena  and
               El-Khaissi, Charbel  and
               Pimentel, Tiago  and
               Gasser, Michael  and
               Lane, William Abbott  and
               Raj, Mohit  and
               Coler, Matt  and
               Samame, Jaime Rafael Montoya  and
               Camaiteri, Delio Siticonatzi  and
               Rojas, Esa{\'u} Zumaeta  and
               L{\'o}pez Francis, Didier  and
               Oncevay, Arturo  and
               L{\'o}pez Bautista, Juan  and
               Villegas, Gema Celeste Silva  and
               Hennigen, Lucas Torroba  and
               Ek, Adam  and
               Guriel, David  and
               Dirix, Peter  and
               Bernardy, Jean-Philippe  and
               Scherbakov, Andrey  and
               Bayyr-ool, Aziyana  and
               Anastasopoulos, Antonios  and
               Zariquiey, Roberto  and
               Sheifer, Karina  and
               Ganieva, Sofya  and
               Cruz, Hilaria  and
               Karah{\'o}{\v{g}}a, Ritv{\'a}n  and
               Markantonatou, Stella  and
               Pavlidis, George  and
               Plugaryov, Matvey  and
               Klyachko, Elena  and
               Salehi, Ali  and
               Angulo, Candy  and
               Baxi, Jatayu  and
               Krizhanovsky, Andrew  and
               Krizhanovskaya, Natalia  and
               Salesky, Elizabeth  and
               Vania, Clara  and
               Ivanova, Sardana  and
               White, Jennifer  and
               Maudslay, Rowan Hall  and
               Valvoda, Josef  and
               Zmigrod, Ran  and
               Czarnowska, Paula  and
               Nikkarinen, Irene  and
               Salchak, Aelita  and
               Bhatt, Brijesh  and
               Straughn, Christopher  and
               Liu, Zoey  and
               Washington, Jonathan North  and
               Pinter, Yuval  and
               Ataman, Duygu  and
               Wolinski, Marcin  and
               Suhardijanto, Totok  and
               Yablonskaya, Anna  and
               Stoehr, Niklas  and
               Dolatian, Hossep  and
               Nuriah, Zahroh  and
               Ratan, Shyam  and
               Tyers, Francis M.  and
               Ponti, Edoardo M.  and
               Aiton, Grant  and
               Arora, Aryaman  and
               Hatcher, Richard J.  and
               Kumar, Ritesh  and
               Young, Jeremiah  and
               Rodionova, Daria  and
               Yemelina, Anastasia  and
               Andrushko, Taras  and
               Marchenko, Igor  and
               Mashkovtseva, Polina  and
               Serova, Alexandra  and
               Prud{'}hommeaux, Emily  and
               Nepomniashchaya, Maria  and
               Giunchiglia, Fausto  and
               Chodroff, Eleanor  and
               Hulden, Mans  and
               Silfverberg, Miikka  and
               McCarthy, Arya D.  and
               Yarowsky, David  and
               Cotterell, Ryan  and
               Tsarfaty, Reut  and
               Vylomova, Ekaterina},
  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},
  month     = jun,
  year      = {2022},
  address   = {Marseille, France},
  publisher = {European Language Resources Association},
  url       = {https://aclanthology.org/2022.lrec-1.89},
  pages     = {840--855},
  abstract  = {The Universal Morphology (UniMorph) project is a collaborative effort providing broad-coverage instantiated normalized morphological inflection tables for hundreds of diverse world languages. The project comprises two major thrusts: a language-independent feature schema for rich morphological annotation, and a type-level resource of annotated data in diverse languages realizing that schema. This paper presents the expansions and improvements on several fronts that were made in the last couple of years (since McCarthy et al. (2020)). Collaborative efforts by numerous linguists have added 66 new languages, including 24 endangered languages. We have implemented several improvements to the extraction pipeline to tackle some issues, e.g., missing gender and macrons information. We have amended the schema to use a hierarchical structure that is needed for morphological phenomena like multiple-argument agreement and case stacking, while adding some missing morphological features to make the schema more inclusive.In light of the last UniMorph release, we also augmented the database with morpheme segmentation for 16 languages. Lastly, this new release makes a push towards inclusion of derivational morphology in UniMorph by enriching the data and annotation schema with instances representing derivational processes from MorphyNet.}
}

@inproceedings{unsup-pos-vec,
  title     = {Unsupervised Learning of Syntactic Structure with Invertible Neural Projections},
  author    = {He, Junxian  and
               Neubig, Graham  and
               Berg-Kirkpatrick, Taylor},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  month     = oct # {-} # nov,
  year      = {2018},
  address   = {Brussels, Belgium},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D18-1160},
  doi       = {10.18653/v1/D18-1160},
  pages     = {1292--1302},
  abstract  = {Unsupervised learning of syntactic structure is typically performed using generative models with discrete latent variables and multinomial parameters. In most cases, these models have not leveraged continuous word representations. In this work, we propose a novel generative model that jointly learns discrete syntactic structure and continuous word representations in an unsupervised fashion by cascading an invertible neural network with a structured generative prior. We show that the invertibility condition allows for efficient exact inference and marginal likelihood computation in our model so long as the prior is well-behaved. In experiments we instantiate our approach with both Markov and tree-structured priors, evaluating on two tasks: part-of-speech (POS) induction, and unsupervised dependency parsing without gold POS annotation. On the Penn Treebank, our Markov-structured model surpasses state-of-the-art results on POS induction. Similarly, we find that our tree-structured model achieves state-of-the-art performance on unsupervised dependency parsing for the difficult training condition where neither gold POS annotation nor punctuation-based constraints are available.}
}
@article{unsuplemm,
  author     = {Rudolf Rosa and
                Zdenek Zabokrtsk{\'{y}}},
  title      = {Unsupervised Lemmatization as Embeddings-Based Word Clustering},
  journal    = {CoRR},
  volume     = {abs/1908.08528},
  year       = {2019},
  url        = {http://arxiv.org/abs/1908.08528},
  eprinttype = {arXiv},
  eprint     = {1908.08528},
  timestamp  = {Sat, 23 Jan 2021 01:11:13 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1908-08528.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}
@book{vogel2011approaches,
  title     = {Approaches to the typology of word classes},
  author    = {Vogel, Petra M and Comrie, Bernard},
  volume    = {23},
  year      = {2011},
  publisher = {Walter de Gruyter}
}

@inproceedings{wartena2013distributional,
  title     = {Distributional Similarity of Words with Different Frequencies},
  author    = {Wartena, Christian},
  booktitle = {Proceedings of the 13th edition of the Dutch-Belgian information retrieval Workshop (DIR 2013)},
  pages     = {8--11},
  year      = {2013},
  publisher = {Hochschule Hannover}
}

@phdthesis{weber-grammar-1983,
  title     = {A {{Grammar}} of {{Huallaga}} (Huanuco) {{Quechua}}.},
  author    = {Weber, David John},
  address   = {United States -- California},
  urldate   = {2024-10-07},
  abstract  = {This is a reference grammar of Huallaga (Huanuco) Quechua, an American Indian language spoken in central Peru. After (1) a general introduction and (2) an introduction to HgQ syntax, it contains chapters of the following topics: on word and suffix classes for (3) verbs, (4) substantives, (5) adverbs, and (6) other classes; on morphology: (7) word formation generally, (8) the "transitions," i.e., the complex which indicates the person of the subject and object, and (9) the suffixes which occur between the root and the transition; on grammatical relations: (10) case markers (11) and passives; (12) on substantive phrases; (13) on relative clauses and complements; (14) on adverbial clauses; (15) on reduplication; (16) on question formation; (17) on negation; (18) on conjunction; on the post-transition suffixes: (19) the "shading" suffixes (-lla, -pis, -na, and -raq), (20) the (so-called) "topic" marker -qa, and (21) the evidential suffixes (-mi, -shi and -chi); (22) on idiomatic and formulaic expressions; and (23) on phonology and loan processes.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  year      = {1983},
  isbn      = {9798403416092},
  langid    = {english},
  school    = {University of California, Los Angeles},
  keywords  = {Language literature and linguistics,Linguistics},
  file      = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/WEBER_A Grammar of Huallaga (huanuco) Quechua.pdf}
}
@book{wetzer_typology_2013,
  title     = {The {Typology} of {Adjectival} {Predication}},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  isbn      = {978-3-11-081358-6},
  url       = {https://www.degruyterbrill.com/document/doi/10.1515/9783110813586/html?lang=en&srsltid=AfmBOopO-jMOVe12th7PUlYdT-lMYbXysoiLUh3x0gQrsjocyc10DfOl},
  abstract  = {The Typology of Adjectival Predication by Harrie Wetzer was published on March 1, 2013 by De Gruyter Mouton.},
  language  = {en},
  urldate   = {2025-08-09},
  publisher = {De Gruyter Mouton},
  author    = {Wetzer, Harrie},
  month     = mar,
  year      = {1996},
  doi       = {10.1515/9783110813586},
  keywords  = {Adjektiv, Kontrastive Grammatik, adjectives, cognitive}
}

@book{wiltschko-2014-universal,
  title     = {The {{Universal Structure}} of {{Categories}}},
  author    = {Wiltschko, Martina},
  year      = {2014},
  series    = {Cambridge {{Studies}} in {{Linguistics}}},
  publisher = {Cambridge University Press},
  address   = {Cambridge},
  doi       = {10.1017/CBO9781139833899},
  abstract  = {Using data from a variety of languages such as Blackfoot, Halkomelem, and Upper Austrian German, this book explores a range of grammatical categories and constructions, including tense, aspect, subjunctive, case and demonstratives. It presents a new theory of grammatical categories - the Universal Spine Hypothesis - and reinforces generative notions of Universal Grammar while accommodating insights from linguistic typology. In essence, this new theory shows that language-specific categories are built from a small set of universal categories and language-specific units of language. Throughout the book the Universal Spine Hypothesis is compared to two alternative theories - the Universal Base Hypothesis and the No Base Hypothesis. This valuable addition to the field will be welcomed by graduate students and researchers in linguistics.},
  isbn      = {978-1-107-03851-6}
}
@inproceedings{word2vec,
  author    = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  title     = {Distributed Representations of Words and Phrases and Their Compositionality},
  year      = {2013},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  abstract  = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
  booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
  pages     = {3111–3119},
  numpages  = {9},
  location  = {Lake Tahoe, Nevada},
  series    = {NIPS'13}
}
@inproceedings{wu-composition-2023,
  title      = {Composition and {{Deformance}}: {{Measuring Imageability}} with a {{Text-to-Image Model}}},
  shorttitle = {Composition and {{Deformance}}},
  booktitle  = {Proceedings of the 5th {{Workshop}} on {{Narrative Understanding}}},
  author     = {Wu, Si and Smith, David},
  editor     = {Akoury, Nader and Clark, Elizabeth and Iyyer, Mohit and Chaturvedi, Snigdha and Brahman, Faeze and Chandu, Khyathi},
  year       = {2023},
  month      = jul,
  pages      = {106--117},
  publisher  = {Association for Computational Linguistics},
  address    = {Toronto, Canada},
  doi        = {10.18653/v1/2023.wnu-1.16},
  urldate    = {2024-07-23},
  abstract   = {Although psycholinguists and psychologists have long studied the tendency of linguistic strings to evoke mental images in hearers or readers, most computational studies have applied this concept of imageability only to isolated words. Using recent developments in text-to-image generation models, such as DALLE mini, we propose computational methods that use generated images to measure the imageability of both single English words and connected text. We sample text prompts for image generation from three corpora: human-generated image captions, news article sentences, and poem lines. We subject these prompts to different deformances to examine the model's ability to detect changes in imageability caused by compositional change. We find high correlation between the proposed computational measures of imageability and human judgments of individual words. We also find the proposed measures more consistently respond to changes in compositionality than baseline approaches. We discuss possible effects of model training and implications for the study of compositionality in text-to-image models.},
  file       = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Wu_Smith_2023_Composition and Deformance.pdf}
}
@inproceedings{xlmr,
  title     = {Unsupervised Cross-lingual Representation Learning at Scale},
  author    = {Conneau, Alexis  and
               Khandelwal, Kartikay  and
               Goyal, Naman  and
               Chaudhary, Vishrav  and
               Wenzek, Guillaume  and
               Guzm{\'a}n, Francisco  and
               Grave, Edouard  and
               Ott, Myle  and
               Zettlemoyer, Luke  and
               Stoyanov, Veselin},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.acl-main.747},
  doi       = {10.18653/v1/2020.acl-main.747},
  pages     = {8440--8451},
  abstract  = {This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6{\%} average accuracy on XNLI, +13{\%} average F1 score on MLQA, and +2.4{\%} F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7{\%} in XNLI accuracy for Swahili and 11.4{\%} for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.}
}

@misc{ye2024computervisiondatasetsmodels,
  title         = {Computer Vision Datasets and Models Exhibit Cultural and Linguistic Diversity in Perception},
  author        = {Andre Ye and Sebastin Santy and Jena D. Hwang and Amy X. Zhang and Ranjay Krishna},
  year          = {2024},
  eprint        = {2310.14356},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2310.14356}
}

@inproceedings{zhai-sigmoid-2023,
  title     = {Sigmoid Loss for Language Image Pre-Training},
  booktitle = {2023 {{IEEE}}/{{CVF}} International Conference on Computer Vision ({{ICCV}})},
  author    = {Zhai, X. and Mustafa, B. and Kolesnikov, A. and Beyer, L.},
  year      = {2023},
  month     = oct,
  pages     = {11941--11952},
  publisher = {IEEE Computer Society},
  address   = {Los Alamitos, CA, USA},
  doi       = {10.1109/ICCV51070.2023.01100},
  abstract  = {We propose a simple pairwise sigmoid loss for imagetext pre-training. Unlike standard contrastive learning with softmax normalization, the sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. With only four TPUv4 chips, we can train a Base CLIP model at 4k batch size and a Large LiT model at 20k batch size, the latter achieves 84.5},
  keywords  = {computer vision,memory management,robustness,self-supervised learning,standards}
}



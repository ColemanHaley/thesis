

%%%%
%% Load the class. Put any options that you want here (see the documentation
%% for the list of options). The following are samples for each type of
%% thesis:
%%
%% Note: you can also specify any of the following options:
%%  logo: put a University of Edinburgh logo onto the title page
%%  frontabs: put the abstract onto the title page
%%  deptreport: produce a title page that fits into a Computer Science
%%      departmental cover [not sure if this actually works]
%%  singlespacing, fullspacing, doublespacing: choose line spacing
%%  oneside, twoside: specify a one-sided or two-sided thesis
%%  10pt, 11pt, 12pt: choose a font size
%%  centrechapter, leftchapter, rightchapter: alignment of chapter headings
%%  sansheadings, normalheadings: headings and captions in sans-serif
%%      (default) or in the same font as the rest of the thesis
%%  [no]listsintoc: put list of figures/tables in table of contents (default:
%%      not)
%%  romanprepages, plainprepages: number the preliminary pages with Roman
%%      numerals (default) \or consecutively with the rest of the thesis
%%  parskip: don't indent paragraphs, put a blank line between instead
%%  abbrevs: define a list of useful abbreviations (see documentation)
%%  draft: produce a single-spaced, double-sided thesis with narrow margins
%%

%% For a PhD thesis -- you must also specify a research institute:
\documentclass[phd,ilcc,twoside,logo,leftchapter,12pt,listsintoc,normalheadings, doublespacing]{infthesis}


\usepackage[authormarkup=none,final]{changes}

\usepackage{tcolorbox}

% \usepackage[T1]{fontenc}
\usepackage{fontspec}

% \usepackage{domitian}

\usepackage{svg}
\usepackage{pdflscape}
\usepackage{longtable}
%\setmainfont{Linux Libertine}
% \setsansfont{Helvetica}
%\setmainfont{Clara}
%\setmainfont{Domitian}
\usepackage[scheme=plain]{ctex}
\setmainfont{TeX Gyre Pagella}
\setCJKmainfont{Noto Serif CJK SC}
% \setmonofont{IBM Plex Mono}
% \setmainfont{Gentium}
\usepackage{todonotes}

% \setmainfont{EB Garamond     }
% \setmainfont{Crimson}
%\usepackage[osf,sc]{mathpazo}  % used for right + bottom left

% \setmainfont{CanelaText}[
%   Path=./fonts/,
%   Extension = .ttf,
%   UprightFont=*-Regular-Web,
%   BoldFont=*-Bold-Web,
%   ItalicFont=*-RegularItalic-Web,
%   BoldItalicFont=*-BoldItalic-Web
% ]
% \setsansfont{RegModn}[
%     Path=./fonts/,
%     Extension = .ttf,
%     UprightFont=*-Regular,
%     BoldFont=*-Regular,
%     ]
% fix foodnotes for tcolorbox
% \def\tcb@restore@footnote{%
%   \def\@mpfn{footnote}%
%   \def\thempfn{\arabic{footnote}}%
%   \let\@footnotetext\tcb@footnote@collect
% }

% % collect footnote text
% \long\def\tcb@footnote@collect#1{%
%   % expand \@thefnmark before appending before app to \tcb@footnote@acc
%   \expandafter\gappto\expandafter\tcb@footnote@acc\expandafter{%
%     \expandafter\footnotetext\expandafter[\@thefnmark]{#1}%
%   }%
% }

% \def\tcb@footnote@use{%
%   \tcb@footnote@acc
%   \global\let\tcb@footnote@acc\@empty
% }

\usepackage{footnotehyper}
\usepackage{bibentry}
\makesavenoteenv{tcolorbox}
\usepackage{natbib}
\usepackage[british]{babel}
\usepackage{graphicx}
\usepackage{amsmath,amsfonts,amssymb}  % for maths
\usepackage{booktabs}  % for pretty lines in tables
\usepackage{covington}
\usepackage{xspace}
\newcommand{\chgform}{\ensuremath{M_{\text{Form}}}\xspace}
\newcommand{\varform}{\ensuremath{V_{\text{Form}}}\xspace}
\newcommand{\chgemb}{\ensuremath{M_{\text{Embed}}}\xspace}
\newcommand{\varemb}{\ensuremath{V_{\text{Embed}}}\xspace}
\usepackage{nth}  % for easy ordinals
\usepackage{csquotes}
\usepackage[colorlinks=true,linkcolor=black,anchorcolor=black,citecolor=black,filecolor=black,menucolor=black,runcolor=black,urlcolor=black]{hyperref}  % for urls
\usepackage{subcaption}
\expandafter\def\csname ver@subfig.sty\endcsname{}

% Standard package includes
\usepackage{latexsym}
% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
% maths
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{microtype}
\usepackage{fontawesome}
\usepackage{rotating}

\def\vcontext{\mathbf{w}_{<t}}
\def\vclass{\mathcal{C}_i}
\def\vword{w_{t}}
\def\vvocab{\mathcal{W}}

% graphs
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usetikzlibrary{patterns}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\usepackage{epigraph}

% floats
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{arydshln}
\usepackage{booktabs}
\usepackage{tabulary}
\usepackage{colortbl}

\usepackage{subfig}
\usepackage{xspace}
\usepackage[table]{xcolor}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\newcolumntype{P}{>{\raggedleft\arraybackslash}X}

\usepackage{multirow}
\usepackage{stfloats}
\usepackage{diagbox}

% algorithm
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\newcommand{\bos}{\textsc{bos}\xspace}

\usepackage[noabbrev,capitalise]{cleveref}
\usepackage{blkarray}
\usepackage{pgffor}
\usepackage{cleveref} % for references to sections/figures/tables...
\newcommand{\seq}{\,{=}\,}
\newcommand{\slt}{\,{<}\,}

%%%% MACROS %%%%

\newcommand{\edin}{\epsilon}
\newcommand{\cam}{\kappa}
\usepackage{plex-mono}

%% Information about the title, etc.
\title{A computational approach to typological comparative concepts for lexicality}
\author{Coleman Haley}

%% If the year of submission is not the current year, uncomment this line and
%% specify it here:
\submityear{2025}

%% Optionally, specify the graduation month and year:
%\graduationdate{June 2024}

%% Specify the abstract here.
\abstract{One major dimension of linguistic organization is the notion that there are more lexical linguistic units, which express meanings, and more functional linguistic units, which are determined by syntax and/or discourse and serve to organize and clarify the relationships between lexical elements. This dichotomy has been described at levels of linguistic structure and motivates at least two classical distinctions in linguistics. At the level of words, it motivates the so-called lexical-functional distinction, while within morphology, a related distinction is drawn between derivation (which forms new lexical items) and inflection (which produces forms of lexical items). These dichotomies have many noted boundary cases, which have led to many linguists rejecting them, or treating them as gradient.  In this thesis, I refer to this gradient of semantic weight at different levels of formal structure as lexicality.

  There is substantial neurological and psychological evidence for the importance of lexicality to human language processing. Further, lexicality dichotomies also emerge in cross-linguistic trends in grammatical organization, such as asymmetries between inflection and derivation, or between the properties of functional and lexical word classes. Yet the lexicality of a particular linguistic unit varies contextually and diachronically.
  I develop quantitative methods to test the consistency of these concepts across typologically diverse languages. First, I show inflection vs. derivation can be predicted with high accuracy from formal and distributional properties.

  In linguistic practices that proceed from analysis of language-particular data to a language-general analysis, issues of lexicality have played a role of central importance. However, in the functional—typological tradition, which proceeds from cross-linguistic analysis to the language particular, the relationship of this dimension to linguistic organization has had little theoretical impact. A major factor is that typological research must be conducted with cross-linguistically applicable comparative concepts. In this thesis, I leverage deep learning models to produce empirically grounded measures for lexicality, which I argue can serve as interesting and useful comparative concepts for typological study.

  In the first part of the thesis, I focus on inflection and derivation, operationalizing a four-dimensional framework for formal and distributional properties of the distinction. I show that formal and distributional variability are strong correlates of this traditional distinction across a sample of 26 languages, and that the four measures can predict inflection vs. derivation with 90\% accuracy

  In the second part of the thesis, I introduce a novel groundedness measure, which aims to provide a cross-linguistic empirical ground for language function to quantify contextual semantic contentfulness. To do so, I leverage image–caption datasets and vision–language models. This measure captures the lexical–functional distinction in word classes across 30 languages but diverges substantially from related measures like concreteness.

  Interestingly, groundedness displays asymmetries not just between lexical and functional items, but also among the major lexical classes of nouns, verbs, and adjectives. I argue that this suggests a connection between ideas of lexical word class continua in cognitive linguistics and the lexical–functional distinction. I apply groundedness to deviations from prototypical lexical class organization. I show that groundedness predicts the split between Japanese {\em na}- and {\em i}-adjectives, which has previously been thought to have little synchronic relevance. On the other hand, an investigation of the Tensedness Hypothesis shows the challenges with certain types of cross-linguistic comparisons of groundedness with current methods.
}

% \usepackage{luatexja-fontspec}
% \setmainjfont{Baoli SC}
%% Now we start with the actual document.

% \interfootnotelinepenalty=10000
\setmonofont{IBM Plex Mono}
\begin{document}

% \global\let\tcb@footnote@acc\@empty

% \tcbset{
%   % restore for every box
%   every box/.style={
%     before title pre=\tcb@restore@footnote, % just in case
%     before upper pre=\tcb@restore@footnote,
%     before lower pre=\tcb@restore@footnote,
%   },
%   % use for layer 1 boxes only
%   every box on layer 1/.append style={
%     % use \footnotetext befere the default /tcb/after ends the current paragraph
%     after pre=\tcb@footnote@use
%   }
% }
% \makeatother

%% First, the preliminary pages
\begin{preliminary}

  %% This creates the title page
  \maketitle

  % Lay summary
  \cleardoublepage
  \begin{center}
    \textsf{\textbf{\LARGE Lay Summary}}
  \end{center}
  Lay summary here

  \cleardoublepage

  %% Acknowledgements
  \begin{acknowledgements}
    Acknowledgements here
  \end{acknowledgements}

  %% Next we need to have the declaration.
  \standarddeclaration

  %% Finally, a dedication (this is optional -- uncomment the following line if
  %% you want one).
  % \dedication{}

  %% Create the table of contents
  \tableofcontents

  %% If you want a list of figures or tables, uncomment the appropriate line(s)
  \listoffigures
  \listoftables

\end{preliminary}

%%%%%%%%
%% Include your chapter files here. See the sample chapter file for the basic
%% format.

\include{chapters/1introduction}
\include{chapters/background}

\part{Inflection and Derivation}
\include{chapters/2corpus}
\include{chapters/25predicting}

\part{Word Classes}
\include{chapters/3grounded}
\include{chapters/4threepieces}

\include{chapters/conclusion}
%%%%%%%%
%% Any appendices should go here. The appendix files should look just like the
%% chapter files.
%\appendix
%\include{chapters/99appendix}
\appendix
\chapter{Chapter 4: Results using Word2Vec embeddings}\label{app:word2vec}

As discussed in Sections~\ref{sec:embedding} and \ref{sec:featureimp}, our study's reliance on FastText vectors is potentially problematic for dissociating the contribution of form from syntax and semantics, because FastText includes distributional sub-word information that could cause distance metrics in embedding space to measure formal and not just word-level distributional similarity. While we still use their vectors for the benefits they provide in terms of quality, reliability, and representation of rare and morphologically complex forms, we here compare performance of FastText vectors to Word2Vec vectors, which do not have this sub-word level distributional information. As such, the Word2Vec results presented here can be seen as a lower bound on how much semantic/syntactic factors explain the performance of our FastText results.

Unlike FastText, a large multilingual pre-trained set of Word2Vec vectors does not exist. As a result, we  must train our own Word2Vec representations, for which we use the 2021 dump of the OSCAR corpus \citep{abadji-etal-2022-towards}. Resource constraints prevent us from training on over 10 GB of data for any given language, significantly less than many of the languages in our dataset were trained on for FastText. We use as much data as was available (up to 10 GB) per language and train for 5 epochs. Due to this limited amount of data, the performance of these vectors is not directly comparable. Further compounding this issue is one of vocabulary; the precise minimum count used in FastText models is unclear, but appears to be greater than the default 5, which we used for our models, judging by the ratio of vocabulary size to data size. As such, our Word2Vec vectors may contain more lower-quality vectors. Finally, due to differences in vocabulary, we must exclude some of our training and testing sets. We train and evaluate both Word2Vec and FastText MLP classifiers on only the portions of the train and test sets which are in-vocab for both models. This naturally leads to somewhat worse performance for the FastText-based MLP than the results in the paper.

\begin{figure}
  \small
  \centering
  \renewcommand{\arraystretch}{0.75}
  \setlength{\tabcolsep}{2pt}
  \begin{tabularx}{\textwidth}{>{\centering}m{1cm}>{\centering}m{1.5cm}>{\centering}m{1.5cm}>{\centering}m{1.5cm}>{\centering}m{1.5cm}m{6.65cm}}
    \toprule
    &\multicolumn{4}{l}{Features}                    & Accuracy (
      \begin{tikzpicture}[x=1.2ex,y=1.2ex]
        \filldraw[pattern=north east lines](0,0)rectangle(1,1);
      \end{tikzpicture} = Logistic,
      \begin{tikzpicture}[x=1.2ex,y=1.2ex]
        \filldraw[pattern=dots](0,0)rectangle(1,1);
    \end{tikzpicture} = MLP) \\
    \midrule
    & \multicolumn{4}{l}{Majority cl ass (Inflection)} & \otherchart{0.58}                  \\
    \hdashline
    & \chgform & –       & –        & –               & \chart{0.61}{0}{0.61}{0}     \\
    & –        & \chgemb & –        & –               & \chart{0.62}{0}{0.70}{0}     \\
    & –        & –       & \varform & –               & \chart{0.71}{0}{0.71}{0}     \\
    & –        & –       & –        & \varemb         & \chart{0.70}{0}{0.70}{0.0}     \\

    (A) & –        & –       & \varform & \varemb         & \chart{0.76}{0.0}{0.80}{0.0}     \\
    (B) & \chgform & \chgemb & –        & –               & \chart{0.60}{0.0}{0.67}{0.00}     \\
    (C) & \chgform & –       & \varform & –               & \chart{0.75}{0.0}{0.75}{0.0}     \\
    (D) & –        & \chgemb & –        & \varemb         & \chart{0.71}{0.0}{0.73}{0.0}     \\
    (E) & \chgform & \chgemb & \varform & \varemb         & \boldchart{0.79}{0.0}{0.85}{0.0} \\
    \arrayrulecolor{black}\bottomrule
  \end{tabularx}
  % \end{tabular}
  \caption{Accuracy in reconstructing UniMorph's inflection–derivation distinction by MLP classifiers using Word2Vec- vs. FastText-based distributional features. Hypotheses referred to in the main text are denoted with letters.}
  \label{tab:featresultsw2v}
\end{figure}

From the set of results in \cref{tab:featresultsw2v}, we do see non-trivial contributions of word-level distributional information, particularly for the $\varemb$ measure, even with this lower bound, indicating our FastText measure captures some semantic and syntactic information.
\chapter{Part II: Model performance by language}\label{app:performance}
See Table~\ref{tab:mperf} for per-language captioning performance, part-of-speech (POS) tagging accuracy, and perplexity of the base Gemma-2B model, the PaliGemma captioning model, and our fine-tuned language model (LM).
\def\mr#1{\multirow{2}{*}{#1}}
% make a shortcut to multicolumn
\def\mc#1#2{\multicolumn{#1}{c}{#2}}
% usage: \mc{2}{text}

\begin{landscape}
  {\small
    \renewcommand{\arraystretch}{1}

    \begin{longtable}{llrrrrrrrrrrrr}
\caption{Per-language performance metrics for the models used. A) CIDEr scores on Crossmodal-3600 (XM3600) and COCO-35L for the \texttt{paligemma-3b-ft-coco35-224} model. B) Perplexity scores for the base Gemma-2B model (Gemma), PaliGemma (PG) and our finetuned PaliGemma-based LM. As expected, PaliGemma has the lowest perplexity, and our fine-tuned model particularly improves perplexity on COCO-35L and for languages with different orthographies. C) Average POS tagging accuracy for the Stanza models on the Universal Dependencies treebank test sets for each language.}\label{tab:mperf}\\
\toprule
\cellcolor{white}\mr{Language} & \multirow{2}{1cm}[0em]{ISO 639-1}   &   \mc{2}{CIDEr}     &   \mc{3}{Perplexity (COCO-35L)} &   \mc{3}{Perplexity (XM3600)} &   \mc{3}{Perplexity (Multi30K)} & \multirow{2}{1.5cm}[0em]{Tagging Acc.}                                                                    \\
\cmidrule(lr){3-4}    \cmidrule(lr){5-7}              \cmidrule(lr){8-10}               \cmidrule(lr){11-13}
&            &   COCO-35L &   XM3600 &   Gemma &      PG &     FT-LM &   Gemma &       PG &      FT-LM &   Gemma     &      PG &   FT-LM &              \\
\midrule
\endfirsthead
\caption[]{Per-language performance metrics for the models used (continuted).}\label{tab:mperf}\\
\toprule

\mr{Language} & \multirow{2}{1cm}[0em]{ISO 639-1}   &   \mc{2}{CIDEr}     &   \mc{3}{Perplexity (COCO-35L)} &   \mc{3}{Perplexity (XM3600)} &   \mc{3}{Perplexity (Multi30K)} & \multirow{2}{1.5cm}[0em]{Tagging Acc.}                                                                    \\
\cmidrule(lr){3-4}    \cmidrule(lr){5-7}              \cmidrule(lr){8-10}               \cmidrule(lr){11-13}
&            &   COCO-35L &   XM3600 &   Gemma &      PG &     FT-LM &   Gemma &       PG &      FT-LM &   Gemma     &      PG &   FT-LM &              \\
\midrule
\endhead
Arabic        & \texttt{ar}         &    93.73 &     33.20 &    4.86 &     1.48 &       3.09 &   5.12 &    2.87 &      4.63 &   4.51      &    1.94 &   3.46  & 95.18        \\
Bengali       & \texttt{bn}         &    91.23 &     24.07 &    2.85 &     0.88 &       1.61 &   2.65 &    1.56 &      2.16 &   --        &    --    &   --     & --            \\
Czech         & \texttt{cs}         &    85.57 &     30.12 &    5.07 &     1.40 &       3.04 &   4.94 &    2.45 &      4.38 &   4.61      &    2.24 &   4.04  & 98.31        \\
Danish        & \texttt{da}         &   117.94 &     47.57 &    5.79 &     1.46 &       3.02 &   5.74 &    2.96 &      5.06 &   --        &    --    &   --     & 98.30        \\
German        & \texttt{de}         &    93.78 &     33.13 &    5.23 &     1.59 &       3.47 &   5.50 &    3.14 &      5.55 &   4.73      &    2.16 &   4.22  & 96.96        \\
Greek  & \texttt{el}      &   119.99 &     21.90 &   3.54 &    2.13 &      3.55 &    3.32 &     0.90 &       1.75 &   --        &    --    &   --     & 97.12        \\
English       & \texttt{en}         &   138.15 &     68.30 &    4.74 &     1.73 &       3.62 &   4.88 &    3.51 &      5.72 &   4.13      &    3.02 &   4.79  & 97.56        \\
Spanish       & \texttt{es}         &   138.51 &     48.69 &    4.85 &     1.55 &       3.36 &   5.40 &    3.23 &      5.51 &   --        &    --    &   --     & 98.01        \\
Persian       & \texttt{fa}         &   122.99 &     45.62 &    4.86 &     1.45 &       2.88 &   4.96 &    2.84 &      4.47 &   --        &    --    &   --     & 97.43        \\
Finnish       & \texttt{fi}         &    35.76 &     10.86 &    5.31 &     1.39 &       2.91 &   4.95 &    2.70 &      4.49 &   --        &    --    &   --     & 97.20        \\
French        & \texttt{fr}         &   137.79 &     53.35 &    4.96 &     1.44 &       3.15 &   5.13 &    3.12 &      5.08 &   4.36      &    2.73 &   4.50  & 97.55        \\
Hebrew        & \texttt{he}         &    97.94 &     36.59 &    4.36 &     1.34 &       2.71 &   3.84 &    2.30 &      3.74 &   --        &    --    &   --     & 90.84        \\
Hindi         & \texttt{hi}         &   104.52 &     26.98 &    3.75 &     1.19 &       2.28 &   3.86 &    2.68 &      3.54 &   --        &    --    &   --     & 97.95        \\
Croatian      & \texttt{hr}         &    89.42 &     25.95 &    5.24 &     1.37 &       2.88 &   4.68 &    2.49 &      4.33 &   --        &    --    &   --     & 98.21        \\
Hungarian     & \texttt{hu}         &    78.90 &     21.96 &    4.94 &     1.46 &       3.05 &   4.88 &    2.84 &      4.88 &   --        &    --    &   --     & 95.80        \\
Indonesian    & \texttt{id}         &   146.38 &     37.46 &    6.01 &     1.63 &       3.51 &   4.98 &    3.16 &      5.18 &   --        &    --    &   --     & 95.03        \\
Italian       & \texttt{it}         &   131.15 &     37.98 &    5.21 &     1.50 &       3.34 &   5.44 &    3.36 &      5.43 &   --        &    --    &   --     & 96.98        \\
Japanese      & \texttt{ja}         &   125.07 &     35.90 &    5.95 &     1.34 &       2.81 &   6.07 &    2.60 &      4.60 &   --        &    --    &   --     & 95.74        \\
Korean        & \texttt{ko}         &   112.40 &     42.82 &    4.89 &     1.29 &       2.61 &   4.80 &    2.37 &      3.95 &   --        &    --    &   --     & 95.86        \\
Norwegian     & \texttt{no}         &   118.02 &     39.67 &    6.13 &     1.50 &       3.07 &   5.70 &    2.90 &      4.75 &   --        &    --    &   --     & 98.38        \\
Dutch         & \texttt{nl}         &   114.76 &     47.19 &    4.96 &     1.54 &       3.24 &   5.34 &    3.15 &      5.55 &   --        &    --    &   --     & 96.71        \\
Polish        & \texttt{pl}         &    86.99 &     29.50 &    5.10 &     1.41 &       3.06 &   4.70 &    2.45 &      4.66 &   --        &    --    &   --     & 98.80        \\
Portuguese    & \texttt{pt}         &   136.40 &     42.76 &    5.52 &     1.53 &       3.30 &   5.56 &    3.38 &      5.49 &   --        &    --    &   --     & 97.74        \\
Romanian      & \texttt{ro}         &   118.57 &     22.36 &    5.15 &     1.30 &       2.73 &   4.62 &    2.63 &      4.18 &   --        &    --    &   --     & 97.98        \\
Russian       & \texttt{ru}         &    98.45 &     28.23 &    4.67 &     1.39 &       3.21 &   4.21 &    2.50 &      5.12 &   --        &    --    &   --     & 97.34        \\
Swedish       & \texttt{sv}         &   120.08 &     45.93 &    5.77 &     1.51 &       3.11 &   6.03 &    2.99 &      5.37 &   --        &    --    &   --     & 97.81        \\
Swahili       & \texttt{sw}         &   111.15 &     29.45 &    5.59 &     1.28 &       2.57 &   5.17 &    2.96 &      4.10 &   --        &    --    &   --     & --            \\
Maori         & \texttt{mi}         &   156.26 &     40.81 &    5.59 &     1.07 &       2.14 &   5.78 &    3.12 &      3.96 &   --        &    --    &   --     & --            \\
Telugu        & \texttt{te}         &    76.35 &     25.80 &    2.93 &     0.79 &       1.48 &   2.98 &    1.60 &      2.32 &   --        &    --    &   --     & 93.97        \\
Thai          & \texttt{th}         &   146.17 &     67.49 &    4.80 &     1.08 &       2.00 &   4.60 &    1.70 &      2.90 &   --        &    --    &   --     & --            \\
Turkish       & \texttt{tr}         &    86.26 &     27.58 &    6.05 &     1.62 &       3.42 &   5.61 &    3.00 &      5.00 &   --        &    --    &   --     & 95.26        \\
Ukrainian     & \texttt{uk}         &    92.90 &     22.47 &    4.26 &     1.23 &       2.67 &   4.01 &    2.48 &      4.38 &   --        &    --    &   --     & 97.52        \\
Vietnamese    & \texttt{vi}         &   159.82 &     51.57 &    4.83 &     1.48 &       3.02 &   4.66 &    3.02 &      4.86 &   --        &    --    &   --     & 81.48        \\
Chinese       & \texttt{zh}         &   103.19 &     26.41 &    6.01 &     1.55 &       3.21 &   5.86 &    3.06 &      4.97 &   --        &    --    &   --     & 88.82        \\
\bottomrule

\end{longtable}
}
\end{landscape}
\chapter{Groundedness correlation plots for other psycholinguistic norms}
Figure~\ref{fig:norms_extra} shows the relationship between our measure and concreteness, as well as the uncertainty coefficient, which normalizes our measure by the language model surprisal. While concreteness is most strongly associated with our measure/its normalized variant, for completeness we show the relationships between our measure and the other psycholinguistic norms (imageability and strength of visual experience) we investigate here.
\begin{figure*}
\centering

\begin{subfigure}{.49\textwidth}
\centering
\includegraphics[width=0.9\linewidth]{figures/grounded/imag_pmi.pdf}
\caption{$\rho=0.288$}
\label{fig:sub3}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
\centering
\includegraphics[width=0.9\linewidth]{figures/grounded/imag_ratio.pdf}
\caption{$\rho=0.548$}
\label{fig:sub4}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
\centering
\includegraphics[width=0.9\linewidth]{figures/grounded/visual_pmi.pdf}
\caption{$\rho=0.212$}
\label{fig:sub5}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
\centering
\includegraphics[width=0.9\linewidth]{figures/grounded/visual_ratio.pdf}
\caption{$\rho=0.320$}
\label{fig:sub6}
\end{subfigure}
\caption{Correlation between English psycholinguistic norms  and type-level groundedness  (left) or uncertainty coefficent (right): i.e., the average ratio between LM surprisal and captioning model surprisal. Type-level measures were computed by averaging scores across the COCO-dev dataset for types which occur at least 30 times.}
\label{fig:norms_extra}
\end{figure*}
\chapter{Groundedness distributions by language and dataset}
\section{Crossmodal-3600}\label{results:xm}
Results are ordered by descending mutual information estimate within the dataset (average groundedness/PMI). Hue indicates the average cross-linguistic ranking of a part of speech.

\vspace{3em}
\noindent
\foreach \langtwo in {ar,cs,da,de,el,en,es,fa,fi,fr,he,hi,hr,hu,id,it,ja,ko,nl,no,pl,pt,ro,ru,sv,te,tr,uk,vi,zh} {
\includegraphics[width=\linewidth]{figures/xm3600-2/pos_\langtwo.pdf}
%\caption{Word-token-level distribution of PMIs for \texttt{\lang}}
}
\section{Multi30K}\label{results:multi}
Results are ordered by descending mutual information estimate within the dataset (average groundedness/PMI). Hue indicates the average cross-linguistic ranking of a part of speech.

\vspace{3em}
\noindent
\foreach \langthree in {ar,cs,de,en,fr} {
\includegraphics[width=\linewidth]{figures/multi30k/pos_\langthree.pdf}
%\caption{Word-token-level distribution of PMIs for \texttt{\lang}}
}

\section{COCO-35L Development Set}\label{results:coco}
Results are ordered by descending mutual information estimate within the dataset (average groundedness/PMI). Hue indicates the average cross-linguistic ranking of a part of speech.

\vspace{3em}
\noindent
\foreach \lang in {ar,cs,da,de,el,en,es,fa,fi,fr,he,hi,hr,hu,id,it,ja,ko,nl,no,pl,pt,ro,ru,sv,te,tr,uk,vi,zh} {
\includegraphics[width=\linewidth]{figures/coco35/pos_\lang.pdf}
}
%% Choose your favourite bibliography style here.
\bibliographystyle{acl_natbib}
% \bibliographystyle{acm}

%% If you want the bibliography single-spaced (which is allowed), uncomment
%% the next line.
%\singlespace

\bibliography{thesis,anthology}

\end{document}

\babel@toc {british}{}\relax 
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {3.1}{\ignorespaces Sample of an inflectional construction (upper table, German nominative plural) and derivational construction (lower table, English verbal nominalization with {\em â€“ion}) in our data}}{66}{table.caption.30}%
\contentsline {table}{\numberline {3.2}{\ignorespaces Descriptive statistics of our filtered dataset by language.}}{76}{table.caption.31}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {5.1}{\ignorespaces We match the data points on which the language model and image captioning model were trained. The three datasets are the Gemma pre-training mixture, PaliGemma multimodal data for continued training , and COCO-35L image--caption pairs for fine-tuning. Symbols indicate whether models are trained on text data ({{\FA \symbol {"F031}}}) or on multimodal data ({{{\FA \symbol {"F03E}}}}\,{{\FA \symbol {"F031}}}).}}{130}{table.caption.40}%
\addvspace {10\p@ }
\contentsline {table}{\numberline {6.1}{\ignorespaces \citet {croft-1991-syntactic}'s analysis of the conceptual categories of the major parts of speech and their semantic properties.}}{147}{table.caption.53}%
\contentsline {table}{\numberline {6.2}{\ignorespaces Differences in groundedness between adjective classes across datasets. ``MT?'' indicates whether the captions were machine-translated from English. The effect size is the increase in groundedness (in bits) associated with {\em na}-adjective\xspace -hood, estimated using a linear mixed effects model with fixed effects of word class and position and a random effect for word type. Overall, {\em na}-adjectives\xspace tend to be more grounded than {\em i}-adjectives\xspace . (\textbf {\underline {Significant results}})}}{155}{table.caption.55}%
\contentsline {table}{\numberline {6.3}{\ignorespaces The effect of adjective class on LM surprisal and captioning surprisal. We find that {\em na}-adjectives\xspace tend to be more surprising in the language model than {\em i}-adjectives\xspace , but this effect is reduced by conditioning on the images, resulting in higher overall groundedness. (\textbf {\underline {Significant results}})}}{157}{table.caption.57}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {B.1}{\ignorespaces Per-language performance metrics for the models used. A) CIDEr scores on Crossmodal-3600 (XM3600) and COCO-35L for the \texttt {paligemma-3b-ft-coco35-224} model. B) Perplexity scores for the base Gemma-2B model (Gemma), PaliGemma (PG) and our finetuned PaliGemma-based LM. As expected, PaliGemma has the lowest perplexity, and our fine-tuned model particularly improves perplexity on COCO-35L and for languages with different orthographies. C) Average POS tagging accuracy for the Stanza models on the Universal Dependencies treebank test sets for each language.}}{198}{table.B.1}%
\addvspace {10\p@ }
\addvspace {10\p@ }

% LTeX: enabled=false
@book{greenberg-1966-universals,
  title = {Universals of Language},
  shorttitle = {Universals of Language},
  editor = {Greenberg, Joseph Harold},
  year = {1966},
  series = {The {{M}}.{{I}}.{{T}}. {{Press}} Paperback Series},
  edition = {2nd},
  number = {37},
  publisher = {M.I.T Pr},
  address = {Cambridge, Mass.},
  isbn = {978-0-262-57008-4},
  langid = {english}
}

@misc{-categories,
  title = {{{CATEGORIES AND RELATIONS IN SYNTAX}}: {{THE CLAUSE-LEVEL ORGANIZATION OF INFORMATION}} ({{LEXICON}}, {{TYPOLOGY}}, {{SEMANTICS}}) - {{ProQuest}}},
  shorttitle = {{{CATEGORIES AND RELATIONS IN SYNTAX}}},
  url = {https://www.proquest.com/openview/e39c5893fc6bcb1dd9d04677cd642267/1?pq-origsite=gscholar&cbl=18750&diss=y},
  urldate = {2025-09-06},
  abstract = {Explore millions of resources from scholarly journals, books, newspapers, videos and more, on the ProQuest Platform.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/9BD53DJT/1.html}
}

@misc{-heidegger,
  title = {Heidegger and {{Duns Scotus}} on {{Truth}} and {{Language}} on {{JSTOR}}},
  eprint = {20131978},
  eprinttype = {jstor},
  url = {https://www.jstor.org/stable/20131978?seq=3},
  urldate = {2025-09-06}
}

@misc{-kategorien,
  title = {Die {{Kategorien-}} Und {{Bedeutungslehre}} Des {{Duns Scotus}} : {{Martin Heidegger}} : {{Free Download}}, {{Borrow}}, and {{Streaming}} : {{Internet Archive}}},
  url = {https://archive.org/details/Bedeutungslehre/page/n143/mode/2up},
  urldate = {2025-09-06}
}

@misc{-request,
  title = {Request: Comparison to Other Tokenizers/{{PoS}} Taggers {$\cdot$} {{Issue}} \#6 {$\cdot$} Taishi-i/Nagisa},
  shorttitle = {Request},
  journal = {GitHub},
  url = {https://github.com/taishi-i/nagisa/issues/6},
  urldate = {2025-09-06},
  abstract = {Could you include some notes briefly comparing this to other parses like Mecab? Mecab includes a comparison to other tokenizers/parsers. I think users would greatly benefit from knowing things like...},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/NJC5CA6N/6.html}
}

@inproceedings{abadji-et-al-2021-ungoliant,
  title = {Ungoliant: {{An}} Optimized Pipeline for the Generation of a Very Large-Scale Multilingual Web Corpus},
  author = {Abadji, Julien and Su{\'a}rez, Pedro Javier Ortiz and Romary, Laurent and Sagot, Beno{\^i}t},
  editor = {L{\"u}ngen, Harald and Kupietz, Marc and Ba{\'n}ski, Piotr and Barbaresi, Adrien and Clematide, Simon and Pisetta, Ines},
  year = {2021},
  series = {Proceedings of the Workshop on Challenges in the Management of Large Corpora ({{CMLC-9}}) 2021. {{Limerick}}, 12 July 2021 (Online-Event)},
  pages = {1--9},
  publisher = {Leibniz-Institut f{\"u}r Deutsche Sprache},
  address = {Mannheim},
  doi = {10.14618/ids-pub-10468},
  abstract = {Since the introduction of large language models in Natural Language Processing, large raw corpora have played a crucial role in Computational Linguistics. However, most of these large raw corpora are either available only for English or not available to the general public due to copyright issues. Nevertheless, there are some examples of freely available multilingual corpora for training Deep Learning NLP models, such as the OSCAR and Paracrawl corpora. However, they have quality issues, especially for low-resource languages. Moreover, recreating or updating these corpora is very complex. In this work, we try to reproduce and improve the goclassy pipeline used to create the OSCAR corpus. We propose a new pipeline that is faster, modular, parameterizable, and well documented. We use it to create a corpus similar to OSCAR but larger and based on recent data. Also, unlike OSCAR, the metadata information is at the document level. We release our pipeline under an open source license and publish the corpus under a research-only license.},
  langid = {english}
}

@incollection{ackema-et-al-2019-default,
  title = {Default Person versus Default Number in Agreement},
  booktitle = {Agreement, Case and Locality in the Nominal and Verbal Domains},
  author = {Ackema, Peter and Neeleman, Ad},
  year = {2019},
  series = {Open Generative Syntax},
  pages = {21--54},
  publisher = {Language Science Press},
  doi = {10.5281/zenodo.3458062},
  abstract = {In this paper, we compare the behaviour of the default in the person system (third person) withthe default in the number system (singular). We argue, following Nevins (2007; 2011), thatthird person pronouns have person features, while singular DPs lack number features. Theevidence for these claims comes from situations in which a single head agrees with multiple DPs that have contrasting person and number specifications. In case the number of morphological slots in which agreement can be realized is lower than the number of agreement relations established in syntax, such contrasting specification may prove problematic. As it turns out, conflicts between singular and plural do not result in ungrammaticality, but conflicts between third person and first or second person do. Such person clashes can be avoided if the morphological realization of the relevant person features is syncretic. Alternatively, languages may make use of a person hierarchy that regulates the morphological realization of conflicting specifications for person. The argument we present is rooted in, and supports, the theory of person developed in Ackema \& Neeleman (2013; to appear).},
  isbn = {978-3-96110-201-3},
  langid = {english},
  keywords = {agreement,default,number,person,person hierarchy}
}

@article{anderson-1982-wheres,
  title = {Where's Morphology?},
  author = {Anderson, Stephen R.},
  year = {1982},
  journal = {Linguistic Inquiry},
  volume = {13},
  pages = {571--612}
}

@incollection{anderson-1985-inflectional,
  title = {Inflectional Morphology},
  booktitle = {Language Typology and Syntactic Description},
  author = {Anderson, Stephen R.},
  editor = {Shopen, Timothy},
  year = {1985},
  edition = {1},
  volume = {3},
  pages = {150--201},
  publisher = {Cambridge University Press},
  address = {Cambridge}
}

@misc{arppe-et-al-2014-finitestate,
  title = {Finite-State Transducer-Based Computational Model of {{Plains Cree}} Morphology},
  author = {Arppe, Antti and Harrigan, Atticus and Schmirler, Katherine and Antonsen, Lene and Trosterud, Trond and N{\o}rsteb{\o} Moshagen, Sjur and Silfverberg, Miikka and Wolvengrey, Arok and Snoek, Conor and Lachler, Jordan and Santos, Eddie Antonio and Okim{\=a}sis, Jean and Thunder, Dorothy},
  year = {2014--2019},
  url = {https://giellalt.uit.no/lang/crk/PlainsCreeDocumentation.html},
  urldate = {2020-11-02}
}

@inproceedings{babazhanova-et-al-10-geometric,
  title = {Geometric Probing of Word Vectors},
  booktitle = {{{ESANN}} 2021 Proceedings - 29th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
  author = {Babazhanova, Madina and Tezekbayev, Maxat and Assylbekov, Zhenisbek},
  year = {10--06 2021},
  pages = {587--592},
  publisher = {i6doc.com publication},
  address = {Virtual, Online, Belgium},
  doi = {10.14428/esann/2021.ES2021-105}
}

@incollection{backhouse-2004-inflected,
  title = {Inflected and {{Uninflected Adjectives}} in {{Japanese}}},
  booktitle = {Adjective {{Classes}}: {{A Cross-Linguistic Typology}}},
  author = {Backhouse, Anthony E},
  editor = {Dixon, R M W and Aikhenvald, Alexandra Y},
  year = {2004},
  month = sep,
  pages = {50--73},
  publisher = {Oxford University PressOxford},
  doi = {10.1093/oso/9780199270934.003.0002},
  urldate = {2024-10-07},
  abstract = {Abstract             This chapter deals with adjectives in Japanese. Typologically, Japanese is a dependent-marking language; typical constituent order in the clause is predicate-final, and modifiers precede heads. Nouns function as the head of NPs commonly followed by case markers such as ga (NOM) and o (Ace), as modifier of nouns in NPs followed by the adnominal marker no, as complement of the copula da, and as complement of other copular verbs (such as naru `become') followed by the marker ni. Verbs function as the head of intransitive and transitive predicates, and directly precede NPs in modifying structures. Unlike nouns, verbs and the copula da arc inflected, largely on an agglutinating pattern. Lexically, Japanese has dearly delineated strata. The Sino and foreign strata arc the result of borrowing from classical Chinese and (chiefly) European languages respectively; Sino words are, at least diachronically, typically bimorphemic. In addition, mimetic items form a distinct stratum within the native vocabulary.},
  isbn = {978-0-19-927093-4 978-1-383-04146-0},
  langid = {english}
}

@article{bamberg-adjective,
  title = {The {{Adjective Category}} in {{Japanese}}},
  author = {Bamberg, Otto-Friedrich-Universitat},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/HLKLSQJ3/Bamberg - The Adjective Category in Japanese.pdf}
}

@inproceedings{batsuren-et-al-2021-morphynet,
  title = {{{MorphyNet}}: A Large Multilingual Database of Derivational and Inflectional Morphology},
  booktitle = {Proceedings of the 18th {{SIGMORPHON}} Workshop on Computational Research in Phonetics, Phonology, and Morphology},
  author = {Batsuren, Khuyagbaatar and Bella, G{\'a}bor and Giunchiglia, Fausto},
  year = {2021},
  month = aug,
  pages = {39--48},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.sigmorphon-1.5},
  abstract = {Large-scale morphological databases provide essential input to a wide range of NLP applications. Inflectional data is of particular importance for morphologically rich (agglutinative and highly inflecting) languages, and derivations can be used, e.g. to infer the semantics of out-of-vocabulary words. Extending the scope of state-of-the-art multilingual morphological databases, we announce the release of MorphyNet, a high-quality resource with 15 languages, 519k derivational and 10.1M inflectional entries, and a rich set of morphological features. MorphyNet was extracted from Wiktionary using both hand-crafted and automated methods, and was manually evaluated to be of a precision higher than 98\%. Both the resource generation logic and the resulting database are made freely available and are reusable as stand-alone tools or in combination with existing resources.}
}

@inproceedings{batsuren-et-al-2022-unimorph,
  title = {{{UniMorph}} 4.0: {{Universal Morphology}}},
  booktitle = {Proceedings of the {{Thirteenth Language Resources}} and {{Evaluation Conference}}},
  author = {Batsuren, Khuyagbaatar and Goldman, Omer and Khalifa, Salam and Habash, Nizar and Kiera{\'s}, Witold and Bella, G{\'a}bor and Leonard, Brian and Nicolai, Garrett and Gorman, Kyle and Ate, Yustinus Ghanggo and Ryskina, Maria and Mielke, Sabrina and Budianskaya, Elena and {El-Khaissi}, Charbel and Pimentel, Tiago and Gasser, Michael and Lane, William Abbott and Raj, Mohit and Coler, Matt and Samame, Jaime Rafael Montoya and Camaiteri, Delio Siticonatzi and Rojas, Esa{\'u} Zumaeta and L{\'o}pez Francis, Didier and Oncevay, Arturo and L{\'o}pez Bautista, Juan and Villegas, Gema Celeste Silva and Hennigen, Lucas Torroba and Ek, Adam and Guriel, David and Dirix, Peter and Bernardy, Jean-Philippe and Scherbakov, Andrey and {Bayyr-ool}, Aziyana and Anastasopoulos, Antonios and Zariquiey, Roberto and Sheifer, Karina and Ganieva, Sofya and Cruz, Hilaria and Karah{\'o}{\v g}a, Ritv{\'a}n and Markantonatou, Stella and Pavlidis, George and Plugaryov, Matvey and Klyachko, Elena and Salehi, Ali and Angulo, Candy and Baxi, Jatayu and Krizhanovsky, Andrew and Krizhanovskaya, Natalia and Salesky, Elizabeth and Vania, Clara and Ivanova, Sardana and White, Jennifer and Maudslay, Rowan Hall and Valvoda, Josef and Zmigrod, Ran and Czarnowska, Paula and Nikkarinen, Irene and Salchak, Aelita and Bhatt, Brijesh and Straughn, Christopher and Liu, Zoey and Washington, Jonathan North and Pinter, Yuval and Ataman, Duygu and Wolinski, Marcin and Suhardijanto, Totok and Yablonskaya, Anna and Stoehr, Niklas and Dolatian, Hossep and Nuriah, Zahroh and Ratan, Shyam and Tyers, Francis M. and Ponti, Edoardo M. and Aiton, Grant and Arora, Aryaman and Hatcher, Richard J. and Kumar, Ritesh and Young, Jeremiah and Rodionova, Daria and Yemelina, Anastasia and Andrushko, Taras and Marchenko, Igor and Mashkovtseva, Polina and Serova, Alexandra and Prud'hommeaux, Emily and Nepomniashchaya, Maria and Giunchiglia, Fausto and Chodroff, Eleanor and Hulden, Mans and Silfverberg, Miikka and McCarthy, Arya D. and Yarowsky, David and Cotterell, Ryan and Tsarfaty, Reut and Vylomova, Ekaterina},
  editor = {Calzolari, Nicoletta and B{\'e}chet, Fr{\'e}d{\'e}ric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, H{\'e}l{\`e}ne and Odijk, Jan and Piperidis, Stelios},
  year = {2022},
  month = jun,
  pages = {840--855},
  publisher = {European Language Resources Association},
  address = {Marseille, France},
  url = {https://aclanthology.org/2022.lrec-1.89},
  abstract = {The Universal Morphology (UniMorph) project is a collaborative effort providing broad-coverage instantiated normalized morphological inflection tables for hundreds of diverse world languages. The project comprises two major thrusts: a language-independent feature schema for rich morphological annotation, and a type-level resource of annotated data in diverse languages realizing that schema. This paper presents the expansions and improvements on several fronts that were made in the last couple of years (since McCarthy et al. (2020)). Collaborative efforts by numerous linguists have added 66 new languages, including 24 endangered languages. We have implemented several improvements to the extraction pipeline to tackle some issues, e.g., missing gender and macrons information. We have amended the schema to use a hierarchical structure that is needed for morphological phenomena like multiple-argument agreement and case stacking, while adding some missing morphological features to make the schema more inclusive. In light of the last UniMorph release, we also augmented the database with morpheme segmentation for 16 languages. Lastly, this new release makes a push towards inclusion of derivational morphology in UniMorph by enriching the data and annotation schema with instances representing derivational processes from MorphyNet.}
}

@incollection{bauer-2004-function,
  title = {The Function of Word-Formation and the Inflection-Derivation Distinction},
  booktitle = {Words in Their {{Places}}. {{A Festschrift}} for {{J}}. {{Lachlan Mackenzie}}},
  author = {Bauer, Laurie},
  year = {2004},
  pages = {283--292},
  publisher = {Vrije Universiteit},
  address = {Amsterdam}
}

@article{beard-1982-plural,
  title = {The Plural as a Lexical Derivation},
  author = {Beard, Robert},
  year = {1982},
  journal = {Glossa-an International Journal of Linguistics},
  volume = {16},
  number = {2},
  pages = {133--148}
}

@inproceedings{beekhuizen-2025-spatial,
  title = {Spatial Relation Marking across Languages: Extraction, Evaluation, Analysis},
  shorttitle = {Spatial Relation Marking across Languages},
  booktitle = {Proceedings of the 29th {{Conference}} on {{Computational Natural Language Learning}}},
  author = {Beekhuizen, Barend},
  editor = {Boleda, Gemma and Roth, Michael},
  year = {2025},
  month = jul,
  pages = {571--585},
  publisher = {Association for Computational Linguistics},
  address = {Vienna, Austria},
  doi = {10.18653/v1/2025.conll-1.37},
  urldate = {2025-09-06},
  abstract = {This paper presents a novel task, detecting Spatial Relation Markers (SRMs, like English \_**in** the bag\_), across languages, alongside a model for this task, RUIMTE. Using a massively parallel corpus of Bible translations, the model is evaluated against existing and baseline models on the basis of a novel evaluation set. The model presents high quality SRM extraction, and an accurate identification of situations where language have zero-marked SRMs.},
  isbn = {979-8-89176-271-8},
  file = {/Users/coleman/Zotero/storage/RW36QW4K/Beekhuizen - 2025 - Spatial relation marking across languages extraction, evaluation, analysis.pdf}
}

@article{benjamini-et-al-2001-control,
  title = {The {{Control}} of the {{False Discovery Rate}} in {{Multiple Testing}} under {{Dependency}}},
  author = {Benjamini, Yoav and Yekutieli, Daniel},
  year = {2001},
  journal = {The Annals of Statistics},
  volume = {29},
  number = {4},
  eprint = {2674075},
  eprinttype = {jstor},
  pages = {1165--1188},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364},
  url = {https://www.jstor.org/stable/2674075},
  urldate = {2024-10-14},
  abstract = {Benjamini and Hochberg suggest that the false discovery rate may be the appropriate error rate to control in many applied multiple testing problems. A simple procedure was given there as an FDR controlling procedure for independent test statistics and was shown to be much more powerful than comparable procedures which control the traditional familywise error rate. We prove that this same procedure also controls the false discovery rate when the test statistics have positive regression dependency on each of the test statistics corresponding to the true null hypotheses. This condition for positive dependency is general enough to cover many problems of practical interest, including the comparisons of many treatments with a single control, multivariate normal test statistics with positive correlation matrix and multivariate t. Furthermore, the test statistics may be discrete, and the tested hypotheses composite without posing special difficulties. For all other forms of dependency, a simple conservative modification of the procedure controls the false discovery rate. Thus the range of problems for which a procedure with proven FDR control can be offered is greatly increased.},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Benjamini_Yekutieli_2001_The Control of the False Discovery Rate in Multiple Testing under Dependency.pdf;/Users/coleman/Zotero/storage/V5ZGR6RV/Benjamini_Yekutieli_2001_The Control of the False Discovery Rate in Multiple Testing under Dependency.pdf}
}

@misc{berger-et-al-2024-crosslingual,
  title = {Cross-Lingual and Cross-Cultural Variation in Image Descriptions},
  author = {Berger, Uri and Ponti, Edoardo M.},
  year = {2024},
  eprint = {2409.16646},
  primaryclass = {cs.CL},
  url = {https://arxiv.org/abs/2409.16646},
  archiveprefix = {arXiv}
}

@inproceedings{bergmanis-et-al-2017-segmentation,
  title = {From Segmentation to Analyses: A Probabilistic Model for Unsupervised Morphology Induction},
  booktitle = {Proceedings of {{EACL}}},
  author = {Bergmanis, Toms and Goldwater, Sharon},
  year = {2017},
  address = {Valencia, Spain}
}

@article{bird-et-al-2003-verbs,
  title = {Verbs and Nouns: The Importance of Being Imageable},
  author = {Bird, Helen and Howard, David and Franklin, Sue},
  year = {2003},
  month = mar,
  journal = {Journal of Neurolinguistics},
  volume = {16},
  number = {2},
  pages = {113--149},
  issn = {0911-6044},
  doi = {10.1016/S0911-6044(02)00016-7},
  abstract = {There are many differences between verbs and nouns---semantic, syntactic and phonological. We focus on the semantic distinctions and examine differences in performance in both normal control subjects and individuals with aphasia. In tasks requiring production of particular semantic categories and categorisation of given verbs and nouns, control subjects produced fewer verbs than nouns and were slower and less accurate in verb categorisation. Patients who had shown a verb deficit in naming also had particular difficulties producing both verbs and nouns of relatively low imageability. In reading and writing, some patients exhibited poorer performance with verbs than nouns, even when verb/noun homonyms were used. When imageability was controlled, however, no dissociation was shown. We conclude that in simple single word tasks imageability must be controlled to eliminate this as a factor in apparent verb deficits. Other semantic factors, however, could affect performance, particularly when tasks involve the relationships between category exemplars.},
  keywords = {Aphasia,Imageability,Nouns,Reading,Semantics,Verbs,Writing}
}

@incollection{bisang-2010-word,
  title = {Word {{Classes}}},
  booktitle = {The {{Oxford Handbook}} of {{Linguistic Typology}}},
  author = {Bisang, Walter},
  editor = {Song, Jae Jung},
  year = {2010},
  month = nov,
  pages = {0},
  publisher = {Oxford University Press},
  doi = {10.1093/oxfordhb/9780199281251.013.0015},
  urldate = {2024-05-15},
  abstract = {This article introduces the four prerequisites for distinguishing word classes: semantic criteria; pragmatic criteria/criteria of discourse function; formal criteria; and distinction between lexical and syntactic levels of analysis. The most important approaches to word classes based on the first three prerequisites are addressed. The article also deals with the distinction between content words and function words. It then takes up the discussion of the universal status of the noun/verb distinction by integrating the fourth prerequisite. The languages discussed are Classical Nahuatl, Late Archaic Chinese, and Tongan. The distinction between content words and function words is not identical to the distinction between open and closed word classes. The article reviews Dixon's seminal approach to adjectives. The sub-classes of adverbs are considered. The definition of word classes integrates all the central elements that make language structure, and it integrates a whole paradigm of constructions.},
  isbn = {978-0-19-928125-1}
}

@inbook{bisang-2017-grammaticalization,
  title = {Grammaticalization},
  booktitle = {Oxford {{Research Encyclopedia}} of {{Linguistics}}},
  author = {Bisang, Walter},
  year = {2017},
  month = mar,
  publisher = {Oxford University Press},
  doi = {10.1093/acrefore/9780199384655.013.103},
  urldate = {2024-10-07},
  abstract = {Linguistic change not only affects the lexicon and the phonology of words, it also operates on the grammar of a language. In this context, grammaticalization is concerned with the development of lexical items into markers of grammatical categories or, more generally, with the development of markers used for procedural cueing of abstract relationships out of linguistic items with concrete referential meaning. A well-known example is the English verb               go               in its function of a future marker, as in               She is going to visit her friend               . Phenomena like these are very frequent across the world's languages and across many different domains of grammatical categories. In the last 50 years, research on grammaticalization has come up with a plethora of (a) generalizations, (b) models of how grammaticalization works, and (c) methodological refinements.                          On (a): Processes of grammaticalization develop gradually, step by step, and the sequence of the individual stages follows certain clines as they have been generalized from cross-linguistic comparison (unidirectionality). Even though there are counterexamples that go against the directionality of various clines, their number seems smaller than assumed in the late 1990s.             On (b): Models or scenarios of grammaticalization integrate various factors. Depending on the theoretical background, grammaticalization and its results are motivated either by the competing motivations of economy vs. iconicity/explicitness in functional typology or by a change from movement to merger in the minimalist program. Pragmatic inference is of central importance for initiating processes of grammaticalization (and maybe also at later stages), and it activates mechanisms like reanalysis and analogy, whose status is controversial in the literature. Finally, grammaticalization does not only work within individual languages/varieties, it also operates across languages. In situations of contact, the existence of a certain grammatical category may induce grammaticalization in another language.             On (c): Even though it is hard to measure degrees of grammaticalization in terms of absolute and exact figures, it is possible to determine relative degrees of grammaticalization in terms of the autonomy of linguistic signs. Moreover, more recent research has come up with criteria for distinguishing grammaticalization and lexicalization (defined as the loss of productivity, transparency, and/or compositionality of former productive, transparent, and compositional structures).             In spite of these findings, there are still quite a number of questions that need further research. Two questions to be discussed address basic issues concerning the overall properties of grammaticalization. (1) What is the relation between constructions and grammaticalization? In the more traditional view, constructions are seen as the syntactic framework within which linguistic items are grammaticalized. In more recent approaches based on construction grammar, constructions are defined as combinations of form and meaning. Thus, grammaticalization can be seen in the light of constructionalization, i.e., the creation of new combinations of form and meaning. Even though constructionalization covers many apects of grammaticalization, it does not exhaustively cover the domain of grammaticalization. (2) Is grammaticalization cross-linguistically homogeneous, or is there a certain range of variation? There is evidence from East and mainland Southeast Asia that there is cross-linguistic variation to some extent.},
  collaborator = {Bisang, Walter},
  isbn = {978-0-19-938465-5},
  langid = {english}
}

@book{blake-2001-case,
  title = {Case},
  author = {Blake, Barry J},
  year = {2001},
  publisher = {Cambridge University Press}
}
@book{tenhacken-1994-defining,
  title = {Defining Morphology: A Principled Approach to Determining the Boundaries of Compounding, Derivation, and Inflection},
  author = {{ten Hacken}, Pius},
  year = {1994},
  series = {Altertumswissenschaftliche Texte Und Studien},
  publisher = {G. Olms Verlag},
  url = {https://books.google.co.uk/books?id=E8mWh_6mRAcC},
  isbn = {978-3-487-09891-3},
  lccn = {lc95155890}
}


@article{bojanowski-et-al-2017-enriching,
  title = {Enriching Word Vectors with Subword Information},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  year = {2017},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {5},
  pages = {135--146},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  doi = {10.1162/tacl\_a\_00051},
  abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.}
}

@article{bonami-et-al-2018-inflection,
  title = {Inflection vs. Derivation in a Distributional Vector Space},
  author = {Bonami, Olivier and Paperno, Denis},
  year = {2018},
  journal = {Lingue e linguaggio},
  volume = {17},
  number = {2},
  pages = {173--196},
  file = {/Users/coleman/Zotero/storage/9VB5CZUB/Bonami and Paperno - INFLECTION VS. DERIVATION IN A DISTRIBUTIONAL VECT.pdf}
}

@article{bonami-et-al-2019-paradigm,
  title = {Paradigm Structure and Predictability in Derivational Morphology},
  author = {Bonami, Olivier and Strnadov{\'a}, Jana},
  year = {2019},
  month = may,
  journal = {Morphology},
  volume = {29},
  number = {2},
  pages = {167--197},
  issn = {1871-5656},
  doi = {10.1007/s11525-018-9322-6},
  abstract = {In this paper we address the usefulness of the notion of a paradigm in the context of derivational morphology. We first define a notion of paradigmatic system that extends conservatively the notion as it is used in inflection so as to be applicable to collections of structured families of derivationally-related words. We then build on this definition in an empirical quantitative study of derivational families of verbs in French. We apply information-theoretic measures of predictability initially designed by Ackerman et al. (2009) in the context of inflection. We conclude that key quantitative properties are common to inflectional and derivational paradigmatic systems, and hence that (partial) paradigms are an important ingredient of the study of derivation.}
}

@incollection{booij-1996-inherent,
  title = {Inherent versus Contextual Inflection and the Split Morphology Hypothesis},
  booktitle = {Yearbook of Morphology 1995},
  author = {Booij, Geert},
  editor = {Booij, Geert and {van Marle}, Jaap},
  year = {1996},
  pages = {1--16},
  publisher = {Springer},
  address = {Kluwer, Dordrecht}
}

@incollection{booij-2007-inflection,
  title = {Inflection},
  shorttitle = {The {{Grammar}} of {{Words}}},
  booktitle = {The {{Grammar}} of {{Words}}: {{An Introduction}} to {{Linguistic Morphology}}},
  author = {Booij, Geert},
  editor = {Booij, Geert},
  year = {2007},
  month = jul,
  pages = {99--124},
  publisher = {Oxford University Press},
  doi = {10.1093/acprof:oso/9780199226245.003.0005},
  urldate = {2024-10-15},
  abstract = {Inflection is the expression of morphosyntactic properties on words. Examples are case and number marking on nouns, and number and person marking on verbs. These properties play a role in computing the correct form of word in a sentence. Unlike derivation, inflectional processes do not create new words but forms of a word. There are different theoretical models for inflection: Word-and-Paradigm, Item-and-Arrangement, and Item-and-Process models.},
  isbn = {978-0-19-922624-5},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Booij_2007_Inflection.pdf;/Users/coleman/Zotero/storage/9GZ3YNIL/337167067.html}
}

@article{boschloo-1970-raised,
  title = {Raised Conditional Level of Significance for the 2{\texttimes} 2-Table When Testing the Equality of Two Probabilities},
  author = {Boschloo, {\relax RD}},
  year = {1970},
  journal = {Statistica Neerlandica},
  volume = {24},
  number = {1},
  pages = {1--9},
  publisher = {Wiley Online Library}
}

@article{brysbaert-et-al-2014-concreteness,
  title = {Concreteness Ratings for 40 Thousand Generally Known {{English}} Word Lemmas},
  author = {Brysbaert, Marc and Warriner, Amy Beth and Kuperman, Victor},
  year = {2014},
  month = sep,
  journal = {Behavior Research Methods},
  volume = {46},
  number = {3},
  pages = {904--911},
  issn = {1554-3528},
  doi = {10.3758/s13428-013-0403-5},
  urldate = {2024-10-15},
  abstract = {Concreteness ratings are presented for 37,058 English words and 2,896 two-word expressions (such as zebra crossing and zoom in), obtained from over 4,000 participants by means of a norming study using Internet crowdsourcing for data collection. Although the instructions stressed that the assessment of word concreteness would be based on experiences involving all senses and motor responses, a comparison with the existing concreteness norms indicates that participants, as before, largely focused on visual and haptic experiences. The reported data set is a subset of a comprehensive list of English lemmas and contains all lemmas known by at least 85~\% of the raters. It can be used in future research as a reference list of generally known English lemmas.},
  langid = {english},
  keywords = {Concreteness,Crowdsourcing,Ratings,Word recognition},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Brysbaert et al_2014_Concreteness ratings for 40 thousand generally known English word lemmas.pdf;/Users/coleman/Zotero/storage/QACYSJ2W/Brysbaert et al_2014_Concreteness ratings for 40 thousand generally known English word lemmas.pdf}
}

@incollection{brysbaert-toappear-concreteness,
  title = {Concreteness and Imageability Norms},
  booktitle = {International Encyclopedia of Language and Linguistics},
  author = {Brysbaert, Marc},
  year = {to appear},
  edition = {3rd},
  publisher = {Elsevier}
}

@book{bursill-hall-1972-speculative,
  title = {Speculative Grammars of the {{Middle Ages}}; the Doctrine of {{Partes}} Orationis of the {{Modistae}}},
  author = {{Bursill-Hall}, G. L.},
  year = {1972},
  publisher = {The Hague : Mouton},
  url = {http://archive.org/details/speculativegramm0000burs},
  urldate = {2025-09-06},
  abstract = {424 p. ; 25 cm. --; Originally presented as the author's thesis, University of London, 1969; Bibliography: p. [400]-406},
  collaborator = {{Internet Archive}},
  langid = {english},
  keywords = {Speculative grammar}
}

@book{bybee-1985-morphology,
  title = {Morphology: {{A}} Study of the Relation between Meaning and Form},
  author = {Bybee, Joan L},
  year = {1985},
  publisher = {John Benjamins},
  address = {Amsterdam},
  key = {Bybee 1985}
}

@article{chan-2008-codeswitching,
  title = {Code-Switching, Word Order and the Lexical/Functional Category Distinction},
  author = {Chan, Brian Hok-Shing},
  year = {2008},
  month = jun,
  journal = {Lingua},
  series = {Formal Syntactic Approaches to Bilingual Code-Switching},
  volume = {118},
  number = {6},
  pages = {777--809},
  issn = {0024-3841},
  doi = {10.1016/j.lingua.2007.05.004},
  urldate = {2025-09-06},
  abstract = {This paper claims that lexical categories (V, N) and functional categories (D, I, C) behave differently in bilingual code-switching: whereas functional heads always determine the order of their code-switched complements, lexical heads may not do so. This proposal thus deviates from many recent studies which suggest that all heads determine the order of their complements (e.g. Mahootian, 1993; MacSwan, 1999; Nishimura, 1997; Nishimura and Yoon, 1998). Assuming a ``Null Theory'' perspective (Mahootian, 1993; MacSwan, 1999), code-switching data are explained here in terms of existing syntactic apparatus which also governs monolingual syntax. It is proposed that word order between lexical categories and their complements is determined by head parameter instead of feature strength as an intrinsic property of the lexical heads. Nonetheless, head-complement order is inherently specified in functional categories. On this account, prepositions are functional heads instead of lexical heads.},
  keywords = {Code-switching,Functional/lexical distinction,Head-complement order,Null Theory,Prepositions,Word order},
  file = {/Users/coleman/Zotero/storage/LYGEIU3G/Chan - 2008 - Code-switching, word order and the lexicalfunctional category distinction.pdf;/Users/coleman/Zotero/storage/V6WNEWDW/S0024384107000903.html}
}

@inproceedings{chen-et-al-2023-pali,
  title = {{{PaLI}}: {{A}} Jointly-Scaled Multilingual Language-Image Model},
  booktitle = {The Eleventh International Conference on Learning Representations, {{ICLR}} 2023, Kigali, Rwanda, May 1-5, 2023},
  author = {Chen, Xi and Wang, Xiao and Changpinyo, Soravit and Piergiovanni, A. J. and Padlewski, Piotr and Salz, Daniel and Goodman, Sebastian and Grycner, Adam and Mustafa, Basil and Beyer, Lucas and Kolesnikov, Alexander and Puigcerver, Joan and Ding, Nan and Rong, Keran and Akbari, Hassan and Mishra, Gaurav and Xue, Linting and Thapliyal, Ashish V. and Bradbury, James and Kuo, Weicheng},
  year = {2023},
  publisher = {OpenReview.net},
  url = {https://openreview.net/forum?id=mWVoBz4W0u},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  timestamp = {Wed, 24 Jul 2024 16:50:33 +0200}
}

@article{chiarello-et-al-1999-imageability,
  title = {Imageability and Distributional Typicality Measures of Nouns and Verbs in Contemporary {{English}}},
  author = {Chiarello, Christine and Shears, Connie and Lund, Kevin},
  year = {1999},
  month = dec,
  journal = {Behavior Research Methods, Instruments, \& Computers},
  volume = {31},
  number = {4},
  pages = {603--637},
  issn = {1532-5970},
  doi = {10.3758/BF03200739},
  abstract = {Dissociations between noun and verb processing are not uncommon after brain injury; yet, precise psycholinguistic comparisons of nouns and verbs are hampered by the underrepresentation of verbs in published semantic word norms and by the absence of contemporary estimates for part-of-speech usage. We report herein imageability ratings and rating response times (RTs) for 1,197 words previously categorized as pure nouns, pure verbs, or words of balanced noun-verb usage on the basis of the Francis and Ku{\v c}era (1982) norms. Nouns and verbs differed in rated imageability, and there was a stronger correspondence between imageability rating and RT for nouns than for verbs. For all word types, the image-rating-RT function implied that subjects employed an image generation process to assign ratings. We also report a new measure of noun-verbtypicality that used the Hyperspace Analog to Language (HAL; Lund \& Burgess, 1996) context vectors (derived from a large sample of Usenet text) to compute the mean context distance between each word and all of thepure nouns andpure verbs. For a subset of the items, the resulting HAL noun-verb difference score was compared with part-of-speech usage in a representative sample of the Usenet corpus. It is concluded that this score can be used to estimate the extent to which a given word occurs in typical noun or verb sentence contexts in informal contemporary English discourse. The item statistics given in Appendix B will enable experimenters to select representative examples of nouns and verbs or to compare typical with atypical nouns (or verbs), while holding constant or covarying rated imageability.},
  keywords = {Balance Form,Context Vector,Difference Score,Grammatical Class,Inflected Form},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Chiarello et al_1999_Imageability and distributional typicality measures of nouns and verbs in.pdf}
}

@book{chomsky-1957-syntactic,
  title = {Syntactic {{Structures}}},
  author = {Chomsky, Noam},
  year = {1957},
  publisher = {De Gruyter Mouton},
  doi = {10.1515/9783112316009},
  urldate = {2024-05-14},
  isbn = {978-3-11-231600-9}
}

@book{colarusso-1988-northwest,
  title = {The {{Northwest Caucasian Languages}}: {{A Phonological Survey}}},
  author = {Colarusso, John},
  year = {1988},
  edition = {0},
  publisher = {Garland},
  address = {New York},
  doi = {10.4324/9781315852263},
  urldate = {2024-10-07},
  isbn = {978-1-317-91817-2},
  langid = {english}
}

@book{comrie-1988-language,
  title = {Language Universals and Linguistic Typology},
  shorttitle = {Language Universals and Linguistic Typology},
  author = {Comrie, Bernard},
  year = {1988},
  edition = {2nd},
  publisher = {The University of Chicago Press},
  address = {Chicago},
  isbn = {978-0-226-11433-0},
  langid = {english}
}

@incollection{comrie-et-al-1998-great,
  title = {The Great Dagestanian Case Hoax},
  booktitle = {Case, Typology, and Grammar},
  author = {Comrie, Bernard and Polinsky, Maria},
  year = {1998},
  pages = {95--114},
  publisher = {John Benjamins / John Benjamins},
  address = {Amsterdam}
}

@inproceedings{conneau-et-al-2020-unsupervised,
  title = {Unsupervised Cross-Lingual Representation Learning at Scale},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  author = {Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2020},
  month = jul,
  pages = {8440--8451},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.747},
  abstract = {This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6\% average accuracy on XNLI, +13\% average F1 score on MLQA, and +2.4\% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7\% in XNLI accuracy for Swahili and 11.4\% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.}
}

@article{connell-et-al-2012-strength,
  title = {Strength of Perceptual Experience Predicts Word Processing Performance Better than Concreteness or Imageability},
  author = {Connell, Louise and Lynott, Dermot},
  year = {2012},
  month = dec,
  journal = {Cognition},
  volume = {125},
  number = {3},
  pages = {452--465},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2012.07.010},
  abstract = {Abstract concepts are traditionally thought to differ from concrete concepts by their lack of perceptual information, which causes them to be processed more slowly and less accurately than perceptually-based concrete concepts. In two studies, we examined this assumption by comparing concreteness and imageability ratings to a set of perceptual strength norms in five separate modalities: sound, taste, touch, smell and vision. Results showed that concreteness and imageability do not reflect the perceptual basis of concepts: concreteness ratings appear to be based on two different intersecting decision criteria, while imageability ratings are visually biased. Analysis of lexical decision and word naming performance showed that maximum perceptual strength (i.e., strength in the dominant perceptual modality) consistently outperformed both concreteness and imageability ratings in accounting for variance in response latency and accuracy. We conclude that so-called concreteness effects in word processing emerge from the perceptual strength of a concept's representation and discuss the implications for theories of conceptual representation.},
  keywords = {Abstract and concrete concepts,Concreteness effects,Context availability,Dual coding,Imageability,Lexical decision,Perceptual strength,Situated simulation,Word naming}
}

@article{copot-et-al-2022-idiosyncratic,
  title = {Idiosyncratic Frequency as a Measure of Derivation vs. Inflection},
  author = {Copot, Maria and Mickus, Timothee and Bonami, Olivier},
  year = {2022},
  month = dec,
  journal = {Journal of Language Modelling},
  volume = {10},
  number = {2},
  pages = {193--240},
  doi = {10.15398/jlm.v10i2.301},
  annotation = {Abstract note: \&amp;lt;p\&amp;gt;There is ongoing discussion about how to conceptualize the nature of the distinction between inflection and derivation. A common approach relies on qualitative differences in the semantic relationship between inflectionally versus derivationally related words: inflection yields ways to discuss the same concept in different syntactic contexts, while derivation gives rise to words for related concepts. This differential can be expected to manifest in the predictability of word frequency between words that are related derivationally or inflectionally: predicting the token frequency of a word based on information about its base form or about related words should be easier when the two words are in an inflectional relationship, rather than a derivational one. We compare prediction error magnitude for statistical models of token frequency based on distributional and frequency information of inflectionally or derivationally related words in French. The results conform to expectations: it is easier to predict the frequency of a word from properties of an inflectionally related word than from those of a derivationally related word. Prediction error provides a quantitative, continuous method to explore differences between individual processes and differences yielded by employing different predicting information, which in turn can be used to draw conclusions about the nature and manifestation of the inflection--derivation distinction.\&amp;lt;/p\&amp;gt;}
}

@article{corbett-2010-canonical,
  title = {Canonical Derivational Morphology},
  author = {Corbett, Greville G},
  year = {2010},
  journal = {Word structure},
  volume = {3},
  number = {2},
  pages = {141--155},
  publisher = {Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK}
}

@incollection{corver-et-al-2001-semilexical,
  title = {Semi-Lexical Categories},
  booktitle = {Semi-Lexical {{Categories}}},
  author = {Corver, Norbert and Riemsdijk, Henk Van},
  editor = {Corver, Norbert and Riemsdijk, Henk Van},
  year = {2001},
  month = dec,
  pages = {1--20},
  publisher = {de Gruyter},
  doi = {10.1515/9783110874006.1},
  urldate = {2024-10-07},
  isbn = {978-3-11-016685-9}
}

@article{cotterell-et-al-2019-complexity,
  title = {On the {{Complexity}} and {{Typology}} of {{Inflectional Morphological Systems}}},
  author = {Cotterell, Ryan and Kirov, Christo and Hulden, Mans and Eisner, Jason},
  editor = {Lee, Lillian and Johnson, Mark and Roark, Brian and Nenkova, Ani},
  year = {2019},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {7},
  pages = {327--342},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  doi = {10.1162/tacl_a_00271},
  urldate = {2025-05-14},
  abstract = {We quantify the linguistic complexity of different languages' morphological systems. We verify that there is a statistically significant empirical trade-off between paradigm size and irregularity: A language`s inflectional paradigms may be either large in size or highly irregular, but never both. We define a new measure of paradigm irregularity based on the conditional entropy of the surface realization of a paradigm--- how hard it is to jointly predict all the word forms in a paradigm from the lemma. We estimate irregularity by training a predictive model. Our measurements are taken on large morphological paradigms from 36 typologically diverse languages.},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Cotterell et al_2019_On the Complexity and Typology of Inflectional Morphological Systems.pdf;/Users/coleman/Zotero/storage/A29KY7MS/Cotterell et al_2019_On the Complexity and Typology of Inflectional Morphological Systems.pdf}
}

@book{croft-1991-syntactic,
  title = {Syntactic Categories and Grammatical Relations: {{The}} Cognitive Organization of Information},
  author = {Croft, W.},
  year = {1991},
  series = {Emersion: {{Emergent}} Village Resources for Communities of Faith Series},
  publisher = {University of Chicago Press},
  url = {https://books.google.co.uk/books?id=h1gLdOkH1GgC},
  isbn = {978-0-226-12090-4},
  lccn = {90038349}
}

@incollection{croft-2001-parts,
  title = {Parts of {{Speech}}},
  booktitle = {Radical {{Construction Grammar}}: {{Syntactic Theory}} in {{Typological Perspective}}},
  author = {Croft, William},
  editor = {Croft, William},
  year = {2001},
  month = oct,
  pages = {0},
  publisher = {Oxford University Press},
  doi = {10.1093/acprof:oso/9780198299554.003.0002},
  urldate = {2025-08-09},
  abstract = {Most grammatical theories assume that the parts of speech --- noun, verb, adjective --- are categories of particular languages, but may be absent in some languages. But standard analyses are arbitrary and inconsistent about the constructions used to define syntactic categories, which leads some theorists to lump words into fewer categories and others to split words into more categories. One can be rigorous and consistent in analysis by using the same constructions across languages, namely the constructions denoting the propositional acts of reference, predication and modification, and comparing the structural coding and behavioral potential of semantic classes of lexical roots. This rigorous approach leads to universal prototypes for noun (reference to an object), verb (predication of an action), and adjective (modification by a property). Language-specific categories are represented as semantic maps on a universal conceptual space, constrained by the part of speech prototypes.},
  isbn = {978-0-19-829955-4},
  file = {/Users/coleman/Zotero/storage/5BPQ8K6V/Croft - 2001 - Parts of Speech.pdf;/Users/coleman/Zotero/storage/BIX9IKTP/9780198299554.003.html}
}

@book{croft-2001-radical,
  title = {Radical {{Construction Grammar}}: {{Syntactic Theory}} in {{Typological Perspective}}},
  author = {Croft, William},
  year = {2001},
  month = oct,
  publisher = {Oxford University Press},
  doi = {10.1093/acprof:oso/9780198299554.001.0001},
  urldate = {2024-05-14},
  abstract = {This book presents a profound critique of syntactic theory and syntactic argumentation. Recent syntactic theories are essentially formal models for the representation of grammatical knowledge. These theories posit complex syntactic structures in the analysis of sentences, consisting of atomic primitive syntactic categories and relations. The result of this approach to syntax has been an endless cycle of new and revised theories of syntactic representation. The book argues that these types of syntactic theories are incompatible with the grammatical variation found within and across languages. The extent of grammatical variation demonstrates that no scheme of atomic primitive syntactic categories and relations can form the basis of an empirically adequate syntactic theory. This book defends three theses: (i) constructions are the primitive units of syntactic representation, and grammatical categories are derivative; (ii) the only syntactic structures are the relations between a construction and the elements that make it up (that is, there is no need to posit syntactic relations); and (iii) constructions are language-specific. Constructions are complex units pairing form and meaning. Grammatical categories within and across languages are mapped onto a universal conceptual space, following the semantic map model in typology. The structure of conceptual space constrains how meaning is encoded in linguistic form, and reflects the structure of the human mind.},
  isbn = {978-0-19-829955-4}
}

@book{croft-2002-typology,
  title = {Typology and {{Universals}}},
  author = {Croft, William},
  year = {2002},
  month = nov,
  edition = {2},
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9780511840579},
  urldate = {2024-10-07},
  abstract = {Comparison of the grammars of human languages reveals systematic patterns of variation. Typology and universals research uncovers those patterns to formulate universal constraints on language and seek their exploration. In this essential textbook, William Croft presents a comprehensive introduction to the method and theory used in studying typology and universals. The theoretical issues discussed range from the most fundamental to the most abstract. The book provides students and researchers with extensive examples of language universals in phonology, morphology, syntax and semantics. This second edition has been thoroughly rewritten and updated to reflect advances in typology and universals in the past decade, including: new methodologies such as the semantic map model and questions of syntactic argumentation; discussion of current debates over deeper explanations for specific classes of universals; and comparison of the typological and generative approaches to language.},
  copyright = {https://www.cambridge.org/core/terms},
  isbn = {978-0-521-00499-2 978-0-521-80884-2 978-0-511-84057-9}
}

@article{croft-2016-comparative,
  title = {Comparative Concepts and Language-Specific Categories: {{Theory}} and Practice},
  author = {Croft, William},
  year = {2016},
  journal = {Linguistic Typology},
  volume = {20},
  number = {2},
  pages = {377--393},
  doi = {doi:10.1515/lingty-2016-0012},
  urldate = {2025-04-24}
}

@article{croft-coordination,
  title = {The {{Coordination}}---{{Subordination Continuum}}},
  author = {Croft, William},
  url = {https://academic.oup.com/book/32815/chapter/274877316},
  urldate = {2025-09-06},
  abstract = {Abstract. The distinction between coordination and subordination is claimed to be a structural universal. However, the structural criteria used to distingu},
  langid = {english}
}

@article{croft-et-al-2008-inferring,
  title = {Inferring Universals from Grammatical Variation: {{Multidimensional}} Scaling for Typological Analysis},
  shorttitle = {Inferring Universals from Grammatical Variation},
  author = {Croft, William and Poole, Keith T.},
  year = {2008},
  month = jul,
  journal = {Theoretical Linguistics},
  volume = {34},
  number = {1},
  pages = {1--37},
  publisher = {De Gruyter Mouton},
  issn = {1613-4060},
  doi = {10.1515/THLI.2008.001},
  urldate = {2025-09-05},
  abstract = {A fundamental fact about grammatical structure is that it is highly variable both across languages and within languages. Typological analysis has drawn language universals from grammatical variation, in particular by using the semantic map model. But the semantic map model, while theoretically well-motivated in typology, is not mathematically well-defined or computationally tractable, making it impossible to use with large and highly variable crosslinguistic datasets. Multidimensional scaling (MDS), in particular the Optimal Classification nonparametric unfolding algorithm, offers a powerful, formalized tool that allows linguists to infer language universals from highly complex and large-scale datasets. We compare our approach to Haspelmath's semantic map analysis of indefinite pronouns, and reanalyze Dahl's (1985) large tense-aspect dataset. MDS works best with large datasets, demonstrating the centrality of grammatical variation in inferring language universals and the importance of examining as wide a range of grammatical behavior as possible both within and across languages.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/2DH6H9X4/Croft and Poole - 2008 - Inferring universals from grammatical variation Multidimensional scaling for typological analysis.pdf}
}

@article{cutler-1981-degrees,
  title = {Degrees of Transparency in Word Formation},
  author = {Cutler, Anne},
  year = {1981},
  journal = {Canadian Journal of Linguistics/Revue canadienne de linguistique},
  volume = {26},
  number = {1},
  pages = {73--77},
  publisher = {Cambridge University Press}
}

@article{demarneffe-et-al-2021-universal,
  title = {Universal {{Dependencies}}},
  author = {{de Marneffe}, Marie-Catherine and Manning, Christopher D. and Nivre, Joakim and Zeman, Daniel},
  year = {2021},
  month = jun,
  journal = {Computational Linguistics},
  volume = {47},
  number = {2},
  pages = {255--308},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  doi = {10.1162/coli_a_00402},
  urldate = {2025-05-14},
  abstract = {Universal dependencies (UD) is a framework for morphosyntactic annotation of human language, which to date has been used to create treebanks for more than 100 languages. In this article, we outline the linguistic theory of the UD framework, which draws on a long tradition of typologically oriented grammatical theories. Grammatical relations between words are centrally used to explain how predicate--argument structures are encoded morphosyntactically in different languages while morphological features and part-of-speech classes give the properties of words. We argue that this theory is a good basis for crosslinguistically consistent annotation of typologically diverse languages in a way that supports computational natural language understanding as well as broader linguistic studies.},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/De Marneffe et al_2021_Universal Dependencies.pdf;/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/de Marneffe et al_2021_Universal Dependencies2.pdf;/Users/coleman/Zotero/storage/HQJ83U9U/de Marneffe et al_2021_Universal Dependencies2.pdf;/Users/coleman/Zotero/storage/TZHE64JV/De Marneffe et al_2021_Universal Dependencies.pdf}
}

@inproceedings{devlin-et-al-2019-bert,
  title = {{{BERT}}: {{Pre-training}} of Deep Bidirectional Transformers for Language Understanding},
  booktitle = {Proceedings of the 2019 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long and Short Papers)},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = jun,
  pages = {4171--4186},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1423},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}

@article{dixon-1977-where,
  title = {Where Have All the Adjectives Gone?},
  author = {Dixon, R. M. W.},
  year = {1977},
  journal = {Studies in Language},
  volume = {1},
  number = {1},
  pages = {19--80},
  publisher = {John Benjamins Publishing Company},
  address = {Amsterdam/Philadephia},
  issn = {0378-4177},
  doi = {10.1075/sl.1.1.04dix},
  aiatsis_callnumber = {MS 163 1 Manuscript and pamphlet manuscripts Closed Access Stacks},
  aiatsis_code = {Y123},
  aiatsis_reference_language = {DYIRBAL},
  citekeys = {langsci120:dixon1977 langsci223:dixon1977 langsci321:dixon1977 langsci336:dixon1979 langsci67:dixon1977 langsci73:dixon1977 langsci80:dixon1977},
  hhtype = {specific\_feature},
  inlg = {English [eng]},
  isreferencedby = {langsci120 langsci223 langsci321 langsci336 langsci67 langsci73 langsci80},
  lgcode = {Dyirbal [dbl]},
  macro_area = {Australia},
  ozbib_id = {1524},
  ozbibnote = {[See also Dixon 1982]},
  ozbibreftype = {17},
  src = {benjamins, hh, langsci, ozbib},
  file = {/Users/coleman/Zotero/storage/GDKFESWQ/sl.1.1.html}
}

@article{dressler-1989-prototypical,
  title = {Prototypical Differences between Inflection and Derivation},
  author = {Dressler, Wolfgang U},
  year = {1989},
  journal = {STUF-Language Typology and Universals},
  volume = {42},
  number = {1},
  pages = {3--10},
  publisher = {De Gruyter (A)}
}

@article{dryer-1989-large,
  title = {Large Linguistic Areas and Language Sampling},
  author = {Dryer, Matthew S},
  year = {1989},
  journal = {Studies in Language. International Journal sponsored by the Foundation ``Foundations of Language''},
  volume = {13},
  number = {2},
  pages = {257--292},
  publisher = {John Benjamins}
}

@incollection{dryer-1997-are,
  title = {Are {{Grammatical Relations Universal}}?},
  booktitle = {Essays on {{Language Function}} and {{Language Type}}},
  author = {Dryer, Matthew S.},
  editor = {Bybee, Joan L. and Haiman, John and Thompson, Sandra A.},
  year = {1997},
  pages = {115},
  publisher = {John Benjamins Publishing Company},
  address = {Amsterdam},
  doi = {10.1075/z.82.09dry},
  urldate = {2024-05-15},
  isbn = {978-90-272-2168-1 978-1-55619-522-8 978-90-272-7421-2},
  langid = {english}
}

@article{dube-et-al-2014-independent,
  title = {Independent Effects of Imageability and Grammatical Class in Synonym Judgement in Aphasia.},
  author = {Dub{\'e}, Catherine and Monetta, Laura and {Mart{\'i}nez-Cuiti{\~n}o}, Mar{\'i}a Macarena and Wilson, Maximiliano A.},
  year = {2014},
  journal = {Psicothema},
  volume = {26},
  number = {4},
  pages = {449--456},
  address = {Spain},
  issn = {1886-144X 0214-9915},
  doi = {10.7334/psicothema2014.31},
  abstract = {BACKGROUND: The grammatical class effect in aphasia, i.e. dissociated processing of words according to their respective grammatical class, has been attributed to either grammatical, lexical or semantic (i.e., imageability) deficits. This study explores the hypotheses of impaired semantic treatment as the source of the grammatical class effect in aphasia. METHOD: A synonym judgement task that includes nouns and verbs of high and low imageability has been administered to 30 Spanish-speaking patients suffering from receptive or productive aphasia and 30 controls. RESULTS: Normal controls performed significantly better than aphasic patients. Although globally the productive aphasics performed significantly better than the receptive aphasics, grammatical class (nouns better than verbs) and imageability (high imageability better than low imageability) affected performance in both subgroups. No significant interaction emerged between these two factors. CONCLUSION: The results suggest that the grammatical class effect may emerge from semantic impairment and that it is -at least partially- independent of the imageability of words.},
  langid = {english},
  pmid = {25340890},
  keywords = {*Imagination,*Linguistics,*Vocabulary,Aphasia/*psychology,Argentina,Female,Humans,Male,Middle Aged}
}

@article{floyd-2011-rediscovering,
  title = {Re-Discovering the {{Quechua}} Adjective},
  author = {Floyd, Simeon},
  year = {2011},
  month = jan,
  journal = {Linguistic Typology},
  volume = {15},
  number = {1},
  pages = {25--63},
  issn = {1430-0532, 1613-415X},
  doi = {10.1515/lity.2011.003},
  urldate = {2024-10-07},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Floyd_2011_Re-discovering the Quechua adjective.pdf;/Users/coleman/Zotero/storage/7V6W29UL/Floyd_2011_Re-discovering the Quechua adjective.pdf}
}

@article{futrell-et-al-2015-largescale,
  title = {Large-Scale Evidence of Dependency Length Minimization in 37 Languages},
  author = {Futrell, Richard and Mahowald, Kyle and Gibson, Edward},
  year = {2015},
  month = aug,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {112},
  number = {33},
  pages = {10336--10341},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1502134112},
  urldate = {2025-05-14},
  abstract = {Explaining the variation between human languages and the constraints on that variation is a core goal of linguistics. In the last 20 y, it has been claimed that many striking universals of cross-linguistic variation follow from a hypothetical principle that dependency length---the distance between syntactically related words in a sentence---is minimized. Various models of human sentence production and comprehension predict that long dependencies are difficult or inefficient to process; minimizing dependency length thus enables effective communication without incurring processing difficulty. However, despite widespread application of this idea in theoretical, empirical, and practical work, there is not yet large-scale evidence that dependency length is actually minimized in real utterances across many languages; previous work has focused either on a small number of languages or on limited kinds of data about each language. Here, using parsed corpora of 37 diverse languages, we show that overall dependency lengths for all languages are shorter than conservative random baselines. The results strongly suggest that dependency length minimization is a universal quantitative property of human languages and support explanations of linguistic variation in terms of general properties of human information processing.},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Futrell et al_2015_Large-scale evidence of dependency length minimization in 37 languages.pdf;/Users/coleman/Zotero/storage/BCEM4J4K/Futrell et al_2015_Large-scale evidence of dependency length minimization in 37 languages.pdf}
}

@inproceedings{futrell-et-al-2015-quantifying,
  title = {Quantifying {{Word Order Freedom}} in {{Dependency Corpora}}},
  booktitle = {Proceedings of the {{Third International Conference}} on {{Dependency Linguistics}} ({{Depling}} 2015)},
  author = {Futrell, Richard and Mahowald, Kyle and Gibson, Edward},
  editor = {Nivre, Joakim and Haji{\v c}ov{\'a}, Eva},
  year = {2015},
  month = aug,
  pages = {91--100},
  publisher = {Uppsala University, Uppsala, Sweden},
  address = {Uppsala, Sweden},
  url = {https://aclanthology.org/W15-2112/},
  urldate = {2025-05-14},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Futrell et al_2015_Quantifying Word Order Freedom in Dependency Corpora.pdf;/Users/coleman/Zotero/storage/BPGR6PRZ/Futrell et al_2015_Quantifying Word Order Freedom in Dependency Corpora.pdf}
}

@article{garrod-et-al-1999-investigating,
  title = {In and on: Investigating the Functional Geometry of Spatial Prepositions},
  shorttitle = {In and On},
  author = {Garrod, S. and Ferrier, G. and Campbell, S.},
  year = {1999},
  month = sep,
  journal = {Cognition},
  volume = {72},
  number = {2},
  pages = {167--189},
  issn = {0010-0277},
  doi = {10.1016/s0010-0277(99)00038-4},
  abstract = {Spatial prepositions such as in and on seem to denote semantically indeterminate spatial relations. This reflects, in part, the physical relationships between the objects in the scenes that they are used to portray. We argue that such physical relationships are best represented in terms of an inherently dynamic functional geometry which incorporates notions of location control. Two experiments are reported. They investigate the degree to which independent judgements of location control predict choice of description across a range of scenes. The results show that judgements of location control predict viewer's choice of description under certain circumstances. In the absence of prototypical geometric relations, control information has a strong influence on choice of description. However, when the scenes portray prototypical geometric relations, control information has less of an effect. The results support a hybrid account of the semantic representation underlying the prepositions with both a geometric and a functional component to it.},
  langid = {english},
  pmid = {10553670},
  keywords = {Adult,Female,Humans,Language,Male,Mental Processes,Semantics,Space Perception}
}

@misc{gemma-2024-gemma,
  title = {Gemma: {{Open}} Models Based on {{Gemini}} Research and Technology},
  author = {Gemma, Team},
  year = {2024},
  eprint = {2403.08295},
  primaryclass = {cs.CL},
  url = {https://arxiv.org/abs/2403.08295},
  archiveprefix = {arXiv}
}

@article{gerdes-et-al-2021-typometrics,
  title = {Typometrics: {{From Implicational}} to {{Quantitative Universals}} in {{Word Order Typology}}},
  shorttitle = {Typometrics},
  author = {Gerdes, Kim and Kahane, Sylvain and Chen, Xinying},
  year = {2021},
  month = feb,
  journal = {Glossa: a journal of general linguistics},
  volume = {6},
  number = {1},
  publisher = {Open Library of Humanities},
  issn = {2397-1835},
  doi = {10.5334/gjgl.764},
  urldate = {2025-05-14},
  abstract = {This paper develops the concept of word order universals based on a data analysis of the~Universal Dependencies project, which proposes treebanks of more than 90 languages~encoded with the same annotation scheme. The nature of the data we work on allows~us to extract rich details for testing well-known typological implicational universals~and, further, explore new kinds of universals that we call quantitative universals. We~show how such quantitative universals are in essence different from implicational~universals, including statistical universals, by the fact that they no longer lay down~any claims on categorical statements, but rather on continuous parameters, opening~a new field of research we propose to call typometrics.},
  copyright = {Copyright: {\copyright} 2021 The Author(s).                     This is an open-access article distributed under the terms of the                        Creative Commons Attribution 4.0 International License (CC-BY 4.0), which                        permits unrestricted use, distribution, and reproduction in any medium,                        provided the original author and source are credited. See http://creativecommons.org/licenses/by/4.0/.},
  langid = {english},
  keywords = {thesislit},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Gerdes et al_2021_Typometrics.pdf;/Users/coleman/Zotero/storage/QUTSSYSW/Gerdes et al_2021_Typometrics.pdf}
}

@book{givon-1979-understanding,
  title = {On {{Understanding Grammar}}},
  author = {Giv{\'o}n, T.},
  year = {1979},
  series = {Perspectives in Neurolinguistics and Psycholinguistics},
  publisher = {Academic Press},
  isbn = {978-0-12-285450-7},
  lccn = {78067876}
}

@book{givon-2018-understanding,
  title = {On {{Understanding Grammar}}: {{Revised}} Edition},
  shorttitle = {On {{Understanding Grammar}}},
  author = {Giv{\'o}n, T.},
  year = {2018},
  month = mar,
  publisher = {John Benjamins Publishing Company},
  address = {Amsterdam},
  doi = {10.1075/z.213},
  urldate = {2025-09-06},
  abstract = {In his foreword to the original edition of this classic of functionalism, typology and diachrony, Dwight Bolinger wrote: "I foresee it as one of the truly prizes statements of our current knowledge{\dots}a book about understanding done with deep understanding -- of language and its place in Nature and in the nature of humankind{\dots} The book is rich in insights, even for those who have been with linguistics for a long time. And beginners could be thankful for having it as a starting point, from which so many past mistakes have been shed". Thoroughly revised, corrected and updated, On Understanding Grammar remains, as its author intended it in 1979, a book about trying to make sense of human language and of doing linguistics. Language is considered here from multiple perspectives, intersecting with cognition and communication, typology and universals, grammaticalization, development and evolution. Within such a broad cross-disciplinary context, grammar is viewed as an automated, structured language-processing device, assembled through evolution, diachrony and use. Cross-language diversity is not arbitrary, but rather is tightly constrained and adaptively motivated, with the balance between universality and diversity mediated through development, be it evolutionary or diachronic. The book's take on language harkens back to the works of illustrious antecedents such as F. Bopp, W. von Humbold, H. Paul, A. Meillet, O. Jespersen and G. Zipf, offering a coherent alternative to the methodological and theoretical strictures of Saussure, Bloomfield and Chomsky.},
  isbn = {978-90-272-1252-8 978-90-272-6471-8},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/9FB3G6WS/Givn - 2018 - On Understanding Grammar Revised edition.pdf}
}

@book{gordon-2016-phonological,
  title = {Phonological {{Typology}}},
  author = {Gordon, Matthew K.},
  year = {2016},
  month = apr,
  publisher = {Oxford University Press},
  doi = {10.1093/acprof:oso/9780199669004.001.0001},
  urldate = {2024-10-07},
  isbn = {978-0-19-966900-4}
}

@book{hacken-1994-defining,
  title = {Defining Morphology: A Principled Approach to Determining the Boundaries of Compounding, Derivation, and Inflection},
  author = {Hacken, P.},
  year = {1994},
  series = {Altertumswissenschaftliche Texte Und Studien},
  publisher = {G. Olms Verlag},
  url = {https://books.google.co.uk/books?id=E8mWh_6mRAcC},
  isbn = {978-3-487-09891-3},
  lccn = {lc95155890}
}

@article{haiman-1980-iconicity,
  title = {The {{Iconicity}} of {{Grammar}}: {{Isomorphism}} and {{Motivation}}},
  shorttitle = {The {{Iconicity}} of {{Grammar}}},
  author = {Haiman, John},
  year = {1980},
  journal = {Language},
  volume = {56},
  number = {3},
  eprint = {414448},
  eprinttype = {jstor},
  pages = {515--540},
  publisher = {Linguistic Society of America},
  issn = {0097-8507},
  doi = {10.2307/414448},
  urldate = {2025-01-28},
  abstract = {Although linguistic signs in isolation are symbolic, the system or grammar which relates them may be diagrammatically iconic in two ways: (a) by isomorphism, a bi-unique correspondence tends to be established between signans and signatum; (b) by motivation, the structure of language directly reflects some aspect of the structure of reality. Isomorphism is so nearly universal that deviations from it require explanation. Motivation, although widespread, establishes a typology of languages, as indicated in Saussure's Cours. The evidence of artificial taboo languages suggests that degree of motivation co-varies inversely with the number of 'prima onomata' in the lexicon.},
  keywords = {thesislit},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Haiman_1980_The Iconicity of Grammar.pdf;/Users/coleman/Zotero/storage/WZ4YGLSY/Haiman_1980_The Iconicity of Grammar.pdf}
}

@article{haley-et-al-2023-corpusbased,
  title = {{Corpus-based measures discriminate inflection and derivation cross-linguistically}},
  author = {Haley, Coleman and Ponti, Edoardo M. and Goldwater, Sharon},
  year = {2023},
  month = jun,
  journal = {Society for Computation in Linguistics},
  volume = {6},
  number = {1},
  pages = {403--407},
  publisher = {University of Massachusetts Amherst Libraries},
  issn = {2834-1007},
  doi = {10.7275/z5z0-xx64},
  urldate = {2024-10-15},
  abstract = {Japanese passives are traditionally considered to have two types: direct and indirect passives. However, more recent studies, such as Ishizuka (2012), suggest the two types can be unified un- der the same syntactic movement analysis. Uti- lizing the Balanced Corpus of Contemporary Written Japanese (BCCWJ; Maekawa, 2008; Maekawa et al., 2014), this study aims to in- vestigate how likely different types of passives appear in the naturally occurring texts, espe- cially in relation to markedness-based hierar- chy called Noun Phrase Accessibility Hierar- chy (NPAH; Keenan and Comrie, 1977), and to investigate if true indirect passives occur in contemporary written Japanese.},
  langid = {None},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Haley et al_2023_Corpus-based measures discriminate inflection and derivation.pdf;/Users/coleman/Zotero/storage/8Y44T6GX/Haley et al_2023_Corpus-based measures discriminate inflection and derivation.pdf}
}

@article{harris-1954-distributional,
  title = {Distributional Structure},
  author = {Harris, Zellig},
  year = {1954},
  journal = {Word-journal of The International Linguistic Association},
  volume = {10},
  number = {23},
  pages = {146--162}
}

@article{haspelmath-1996-wordclasschanging,
  title = {Word-Class-Changing Inflection and Morphological Theory},
  author = {Haspelmath, Martin},
  year = {1996},
  journal = {Yearbook of morphology 1995},
  pages = {43--66},
  publisher = {Springer}
}

@incollection{haspelmath-2003-geometry,
  title = {The Geometry of Grammatical Meaning: {{Semantic}} Maps and Cross-Linguistic Comparison},
  booktitle = {The New Psychology of Language},
  author = {Haspelmath, Martin},
  editor = {Tomasello, Michael},
  year = {2003},
  volume = {2},
  pages = {211--242},
  publisher = {Lawrence Erlbaum Associates},
  address = {Mahwah, NJ, USA}
}

@article{haspelmath-2007-preestablished,
  title = {Pre-Established Categories Don't Exist: {{Consequences}} for Language Description and Typology},
  author = {Haspelmath, Martin},
  year = {2007},
  journal = {Linguistic Typology},
  volume = {11},
  number = {1},
  pages = {119--132},
  doi = {10.1515/LINGTY.2007.011},
  urldate = {2024-05-14}
}

@article{haspelmath-2010-comparative,
  title = {Comparative Concepts and Descriptive Categories in Crosslinguistic Studies},
  author = {Haspelmath, Martin},
  year = {2010},
  journal = {Language},
  volume = {86},
  number = {3},
  eprint = {40961695},
  eprinttype = {jstor},
  pages = {663--687},
  publisher = {Linguistic Society of America},
  issn = {0097-8507},
  url = {https://www.jstor.org/stable/40961695},
  urldate = {2024-10-07},
  abstract = {In this discussion note, I argue that we need to distinguish carefully between descriptive categories, that is, categories of particular languages, and comparative concepts, which are used for crosslinguistic comparison and are specifically created by typologists for the purposes of comparison. Descriptive formal categories cannot be equated across languages because the criteria for category assignment are different from language to language. This old structuralist insight (called CATEGORIAL PARTICULARISM) has recently been emphasized again by several linguists, but the idea that linguists need to identify 'crosslinguistic categories' before they can compare languages is still widespread, especially (but not only) in generative linguistics. Instead, what we have to do (and normally do in practice) is to create comparative concepts that allow us to identify comparable phenomena across languages and to formulate crosslinguistic generalizations. Comparative concepts have to be universally applicable, so they can only be based on other universally applicable concepts: conceptual-semantic concepts, general formal concepts, and other comparative concepts. Comparative concepts are not always purely semantically based concepts, but outside of phonology they usually contain a semantic component. The fact that typologists compare languages in terms of a separate set of concepts that is not taxonomically superordinate to descriptive linguistic categories means that typology and language-particular analysis are more independent of each other than is often thought.},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Haspelmath_2010_Comparative concepts and descriptive categories in crosslinguistic studies.pdf;/Users/coleman/Zotero/storage/BR9D7WY5/Haspelmath_2010_Comparative concepts and descriptive categories in crosslinguistic studies.pdf}
}

@article{haspelmath-2012-how,
  title = {How to Compare Major Word-Classes across the World's Languages},
  author = {Haspelmath, Martin},
  year = {2012},
  month = feb,
  journal = {UCLA Working Papers in Linguistics},
  volume = {17},
  pages = {109--130},
  doi = {10.5281/ZENODO.3678496},
  urldate = {2024-05-15},
  abstract = {In this paper, I argue that major word-classes, such as nouns, verbs and adjectives, cannot be compared across languages by asking questions such as "Does language X have a noun-verb distinction?". Such questions are routinely asked by linguists (functionalists and generativists alike), but these are the wrong questions (cf. Croft 2000), because they make presuppositions which are not valid.},
  copyright = {Creative Commons Attribution 4.0 International, Open Access}
}

@article{haspelmath-2024-inflection,
  title = {Inflection and Derivation as Traditional Comparative Concepts},
  author = {Haspelmath, Martin},
  year = {2024},
  journal = {Linguistics},
  volume = {62},
  number = {1},
  pages = {43--77},
  doi = {doi:10.1515/ling-2022-0086},
  urldate = {2024-04-24}
}

@inproceedings{hathout-et-al-2014-glaff,
  title = {{{GL{\`A}FF}}, a Large Versatile {{French}} Lexicon},
  booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation ({{LREC}}'14)},
  author = {Hathout, Nabil and Sajous, Franck and Calderone, Basilio},
  year = {2014},
  month = may,
  pages = {1007--1012},
  publisher = {European Language Resources Association (ELRA)},
  address = {Reykjavik, Iceland},
  url = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/58_Paper.pdf},
  abstract = {This paper introduces GLAFF, a large-scale versatile French lexicon extracted from Wiktionary, the collaborative online dictionary. GLAFF contains, for each entry, inflectional features and phonemic transcriptions. It distinguishes itself from the other available French lexicons by its size, its potential for constant updating and its copylefted license. We explain how we have built GLAFF and compare it to other known resources in terms of coverage and quality of the phonemic transcriptions. We show that its size and quality are strong assets that could allow GLAFF to become a reference lexicon for French NLP and linguistics. Moreover, other derived lexicons can easily be based on GLAFF to satisfy specific needs of various fields such as psycholinguistics.}
}

@inproceedings{he-et-al-10-unsupervised,
  title = {Unsupervised Learning of Syntactic Structure with Invertible Neural Projections},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  author = {He, Junxian and Neubig, Graham and {Berg-Kirkpatrick}, Taylor},
  year = {10-11 2018},
  pages = {1292--1302},
  publisher = {Association for Computational Linguistics},
  address = {Brussels, Belgium},
  doi = {10.18653/v1/D18-1160},
  abstract = {Unsupervised learning of syntactic structure is typically performed using generative models with discrete latent variables and multinomial parameters. In most cases, these models have not leveraged continuous word representations. In this work, we propose a novel generative model that jointly learns discrete syntactic structure and continuous word representations in an unsupervised fashion by cascading an invertible neural network with a structured generative prior. We show that the invertibility condition allows for efficient exact inference and marginal likelihood computation in our model so long as the prior is well-behaved. In experiments we instantiate our approach with both Markov and tree-structured priors, evaluating on two tasks: part-of-speech (POS) induction, and unsupervised dependency parsing without gold POS annotation. On the Penn Treebank, our Markov-structured model surpasses state-of-the-art results on POS induction. Similarly, we find that our tree-structured model achieves state-of-the-art performance on unsupervised dependency parsing for the difficult training condition where neither gold POS annotation nor punctuation-based constraints are available.}
}

@article{hengeveld-et-al-2010-implicational,
  title = {An Implicational Map of Parts of Speech},
  author = {Hengeveld, Kees and Van Lier, Eva},
  year = {2010},
  journal = {Linguistic Discovery},
  volume = {8},
  number = {1},
  issn = {1537-0852},
  doi = {10.1349/PS1.1537-0852.A.348},
  urldate = {2025-09-05},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/ZGFZSBJV/Hengeveld and Van Lier - 2010 - An implicational map of parts of speech.pdf}
}

@inproceedings{hessel-et-al-2018-quantifying,
  title = {Quantifying the {{Visual Concreteness}} of {{Words}} and {{Topics}} in {{Multimodal Datasets}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  author = {Hessel, Jack and Mimno, David and Lee, Lillian},
  editor = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
  year = {2018},
  month = jun,
  pages = {2194--2205},
  publisher = {Association for Computational Linguistics},
  address = {New Orleans, Louisiana},
  doi = {10.18653/v1/N18-1199},
  urldate = {2024-07-23},
  abstract = {Multimodal machine learning algorithms aim to learn visual-textual correspondences. Previous work suggests that concepts with concrete visual manifestations may be easier to learn than concepts with abstract ones. We give an algorithm for automatically computing the visual concreteness of words and topics within multimodal datasets. We apply the approach in four settings, ranging from image captions to images/text scraped from historical books. In addition to enabling explorations of concepts in multimodal datasets, our concreteness scores predict the capacity of machine learning algorithms to learn textual/visual relationships. We find that 1) concrete concepts are indeed easier to learn; 2) the large number of algorithms we consider have similar failure cases; 3) the precise positive relationship between concreteness and performance varies between datasets. We conclude with recommendations for using concreteness scores to facilitate future multimodal research.},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Hessel et al_2018_Quantifying the Visual Concreteness of Words and Topics in Multimodal Datasets.pdf;/Users/coleman/Zotero/storage/AJ36HMTH/Hessel et al_2018_Quantifying the Visual Concreteness of Words and Topics in Multimodal Datasets.pdf}
}

@inproceedings{hewitt-et-al-2019-structural,
  title = {A Structural Probe for Finding Syntax in Word Representations},
  booktitle = {Proceedings of the 2019 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long and Short Papers)},
  author = {Hewitt, John and Manning, Christopher D.},
  year = {2019},
  month = jun,
  pages = {4129--4138},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1419},
  abstract = {Recent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network's word representation space. The probe identifies a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models' vector geometry.}
}

@article{hollmann-2021-nouniness,
  title = {The `Nouniness' of Attributive Adjectives and `Verbiness' of Predicative Adjectives: Evidence from Phonology},
  shorttitle = {The `Nouniness' of Attributive Adjectives and `Verbiness' of Predicative Adjectives},
  author = {Hollmann, Willem B.},
  year = {2021},
  month = jun,
  journal = {English Language \& Linguistics},
  volume = {25},
  number = {2},
  pages = {257--279},
  issn = {1360-6743, 1469-4379},
  doi = {10.1017/S1360674320000015},
  urldate = {2025-08-09},
  abstract = {This article investigates prototypically attributive versus predicative adjectives in English in terms of the phonological properties that have been associated especially with nouns versus verbs in a substantial body of psycholinguistic research (e.g. Kelly 1992) -- often ignored in theoretical linguistic work on word classes. Inspired by Berg's (2000, 2009) `cross-level harmony constraint', the hypothesis I test is that prototypically attributive adjectives not only align more with nouns than with verbs syntactically, semantically and pragmatically, but also phonologically -- and likewise for prototypically predicative adjectives and verbs. I analyse the phonological structure of frequent adjectives from the Corpus of Contemporary American English (COCA), and show that the data do indeed support the hypothesis. Berg's `cross-level harmony constraint' may thus apply not only to the entire word classes noun, verb and adjective, but also to these two adjectival subclasses. I discuss several theoretical issues that emerge. The facts are most readily accommodated in a usage-based model, such as Radical Construction Grammar (Croft 2001), where these adjectives are seen as forming two distinct but overlapping classes. Drawing also on recent research by Boyd \& Goldberg (2011) and Hao (2015), I explore the possible nature and emergence of these classes in some detail.},
  langid = {english},
  keywords = {acquisition,adjectives,cognitive,convergent,phonology,Radical Construction Grammar,word classes},
  file = {/Users/coleman/Zotero/storage/A9S9MNBS/Hollmann - 2021 - The nouniness of attributive adjectives and verbiness of predicative adjectives evidence from p.pdf}
}

@article{hsieh-2019-distinguishing,
  title = {Distinguishing Nouns and Verbs: {{A Tagalog}} Case Study},
  shorttitle = {Distinguishing Nouns and Verbs},
  author = {Hsieh, Henrison},
  year = {2019},
  month = may,
  journal = {Natural Language \& Linguistic Theory},
  volume = {37},
  number = {2},
  pages = {523--569},
  issn = {0167-806X, 1573-0859},
  doi = {10.1007/s11049-018-9422-3},
  urldate = {2024-10-07},
  langid = {english}
}

@article{hu-et-al-2017-comparison,
  title = {A Comparison of Methods for Estimating the Determinant of High-Dimensional Covariance Matrix},
  author = {Hu, Zongliang and Dong, Kai and Dai, Wenlin and Tong, Tiejun},
  year = {2017},
  journal = {The International Journal of Biostatistics},
  volume = {13},
  number = {2},
  pages = {20170013},
  doi = {doi:10.1515/ijb-2017-0013},
  urldate = {2023-02-15}
}

@inproceedings{hwang-et-al-2017-double,
  title = {Double {{Trouble}}: {{The Problem}} of {{Construal}} in {{Semantic Annotation}} of {{Adpositions}}},
  shorttitle = {Double {{Trouble}}},
  booktitle = {Proceedings of the 6th {{Joint Conference}} on {{Lexical}} and {{Computational Semantics}} (*{{SEM}} 2017)},
  author = {Hwang, Jena D. and Bhatia, Archna and Han, Na-Rae and O'Gorman, Tim and Srikumar, Vivek and Schneider, Nathan},
  editor = {Ide, Nancy and Herbelot, Aur{\'e}lie and M{\`a}rquez, Llu{\'i}s},
  year = {2017},
  month = aug,
  pages = {178--188},
  publisher = {Association for Computational Linguistics},
  address = {Vancouver, Canada},
  doi = {10.18653/v1/S17-1022},
  urldate = {2025-08-09},
  abstract = {We consider the semantics of prepositions, revisiting a broad-coverage annotation scheme used for annotating all 4,250 preposition tokens in a 55,000 word corpus of English. Attempts to apply the scheme to adpositions and case markers in other languages, as well as some problematic cases in English, have led us to reconsider the assumption that an adposition's lexical contribution is equivalent to the role/relation that it mediates. Our proposal is to embrace the potential for construal in adposition use, expressing such phenomena directly at the token level to manage complexity and avoid sense proliferation. We suggest a framework to represent both the scene role and the adposition's lexical function so they can be annotated at scale---supporting automatic, statistical processing of domain-general language---and discuss how this representation would allow for a simpler inventory of labels.},
  file = {/Users/coleman/Zotero/storage/74W83VEJ/Hwang et al. - 2017 - Double Trouble The Problem of Construal in Semantic Annotation of Adpositions.pdf}
}

@article{jr.-1951-kolmogorovsmirnov,
  title = {The {{Kolmogorov-Smirnov}} Test for Goodness of Fit},
  author = {Jr., Frank J. Massey},
  year = {1951},
  journal = {Journal of the American Statistical Association},
  volume = {46},
  number = {253},
  eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1951.10500769},
  pages = {68--78},
  publisher = {Taylor \& Francis},
  doi = {10.1080/01621459.1951.10500769}
}

@inproceedings{kasthuri-et-al-2017-plis,
  title = {{{PLIS}}: {{Proposed}} Language Independent Stemmer for Information Retrieval Systems Using Dynamic Programming},
  booktitle = {2017 World Congress on Computing and Communication Technologies ({{WCCCT}})},
  author = {Kasthuri, M. and Kumar, S. Britto Ramesh and Khaddaj, Souheil},
  year = {2017},
  pages = {132--135},
  doi = {10.1109/WCCCT.2016.39}
}

@article{kaufman-2009-austronesian,
  title = {Austronesian {{Nominalism}} and Its Consequences: {{A Tagalog}} Case Study},
  author = {Kaufman, Daniel},
  year = {2009},
  journal = {Theoretical Linguistics},
  volume = {35},
  number = {1},
  pages = {1--49},
  doi = {10.1515/THLI.2009.001},
  urldate = {2024-05-15}
}

@article{kermes-et-al-average,
  title = {Average Surprisal of Parts-of-Speech},
  author = {Kermes, Hannah and Teich, Elke},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/WA5GC5DM/Kermes and Teich - Average surprisal of parts-of-speech.pdf}
}

@article{kim-et-al-2019-predicting,
  title = {Predicting the {{Argumenthood}} of {{English Prepositional Phrases}}},
  author = {Kim, Najoung and Rawlins, Kyle and Durme, Benjamin Van and Smolensky, Paul},
  year = {2019},
  month = jul,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  number = {01},
  pages = {6578--6585},
  issn = {2374-3468},
  doi = {10.1609/aaai.v33i01.33016578},
  urldate = {2025-09-09},
  abstract = {Distinguishing between arguments and adjuncts of a verb is a longstanding, nontrivial problem. In natural language processing, argumenthood information is important in tasks such as semantic role labeling (SRL) and prepositional phrase (PP) attachment disambiguation. In theoretical linguistics, many diagnostic tests for argumenthood exist but they often yield conflicting and potentially gradient results. This is especially the case for syntactically oblique items such as PPs. We propose two PP argumenthood prediction tasks branching from these two motivations: (1) binary argumentadjunct classification of PPs in VerbNet, and (2) gradient argumenthood prediction using human judgments as gold standard, and report results from prediction models that use pretrained word embeddings and other linguistically informed features. Our best results on each task are (1) acc. = 0.955, F1 = 0.954 (ELMo+BiLSTM) and (2) Pearson's r = 0.624 (word2vec+MLP). Furthermore, we demonstrate the utility of argumenthood prediction in improving sentence representations via performance gains on SRL when a sentence encoder is pretrained with our tasks.},
  copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/PN5UZPK6/Kim et al. - 2019 - Predicting the Argumenthood of English Prepositional Phrases.pdf}
}

@inproceedings{kingma-et-al-2015-adam,
  title = {Adam: {{A}} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {{ICLR}} 2015, San Diego, {{CA}}, {{USA}}, May 7-9, 2015, Conference Track Proceedings},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  editor = {Bengio, Yoshua and LeCun, Yann},
  year = {2015},
  url = {http://arxiv.org/abs/1412.6980},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200}
}

@article{kirkici-et-al-2013-inflection,
  title = {Inflection and Derivation in Native and Non-Native Language Processing: {{Masked}} Priming Experiments on {{Turkish}}},
  author = {Kirkici, Bilal and Clahsen, Harald},
  year = {2013},
  journal = {Bilingualism: Language and Cognition},
  volume = {16},
  number = {4},
  pages = {776--791},
  publisher = {Cambridge University Press},
  doi = {10.1017/S1366728912000648}
}

@article{konig-2006-marked,
  title = {Marked Nominative in Africa},
  author = {K{\"o}nig, Christa},
  year = {2006},
  journal = {Studies in Language. International Journal sponsored by the Foundation ``Foundations of Language''},
  volume = {30},
  number = {4},
  pages = {655--732},
  publisher = {John Benjamins}
}

@inproceedings{koper-et-al-2016-automatically,
  title = {Automatically {{Generated Affective Norms}} of {{Abstractness}}, {{Arousal}}, {{Imageability}} and {{Valence}} for 350 000 {{German Lemmas}}},
  booktitle = {Proceedings of the {{Tenth International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}}'16)},
  author = {K{\"o}per, Maximilian and {Schulte im Walde}, Sabine},
  editor = {Calzolari, Nicoletta and Choukri, Khalid and Declerck, Thierry and Goggi, Sara and Grobelnik, Marko and Maegaard, Bente and Mariani, Joseph and Mazo, Helene and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
  year = {2016},
  month = may,
  pages = {2595--2598},
  publisher = {European Language Resources Association (ELRA)},
  address = {Portoro{\v z}, Slovenia},
  url = {https://aclanthology.org/L16-1413},
  urldate = {2024-10-07},
  abstract = {This paper presents a collection of 350,000 German lemmatised words, rated on four psycholinguistic affective attributes. All ratings were obtained via a supervised learning algorithm that can automatically calculate a numerical rating of a word. We applied this algorithm to abstractness, arousal, imageability and valence. Comparison with human ratings reveals high correlation across all rating types. The full resource is publically available at: http://www.ims.uni-stuttgart.de/data/affective\_norms/},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Kper_Schulte im Walde_2016_Automatically Generated Affective Norms of Abstractness, Arousal, Imageability.pdf;/Users/coleman/Zotero/storage/YVKSNREP/Kper_Schulte im Walde_2016_Automatically Generated Affective Norms of Abstractness, Arousal, Imageability.pdf}
}

@article{kratzer-stagelevel,
  title = {Stage-{{Level}} and {{Individual-Level Predicates}}},
  author = {Kratzer, Angelika},
  file = {/Users/coleman/Zotero/storage/3X5MW2PS/Kratzer - Stage-Level and Individual-Level Predicates.pdf}
}

@article{kyjanek-et-al-2020-universal,
  title = {Universal {{Derivations}} 1.0, a Growing Collection of Harmonised Word-Formation Resources},
  author = {Kyj{\'a}nek, Luk{\'a}{\v s} and {\v Z}abokrtsk{\'y}, Zden{\v e}k and {\v S}ev{\v c}{\'i}kov{\'a}, Magda and Vidra, Jon{\'a}{\v s}},
  year = {2020},
  journal = {The Prague Bulletin of Mathematical Linguistics},
  volume = {2},
  number = {115},
  pages = {333--348},
  publisher = {Karolinum Press},
  address = {Prague, Czech Republic}
}

@article{laks-et-al-2022-hebrewnette,
  title = {Hebrewnette--a New Derivational Resource for Non-Concatenative Morphology: {{Principles}}, Design and Implementation},
  author = {Laks, Lior and Namer, Fiammetta},
  year = {2022},
  journal = {The Prague Bulletin of Mathematical Linguistics},
  volume = {118},
  pages = {25--53}
}

@incollection{larasati-et-al-2011-indonesian,
  title = {Indonesian Morphology Tool ({{MorphInd}}): {{Towards}} an Indonesian Corpus},
  booktitle = {Systems and Frameworks for Computational Morphology},
  author = {Larasati, Septina Dian and Kubo{\v n}, Vladislav and Zeman, Daniel},
  editor = {Mahlow, Cerstin and Piotrowski, Michael},
  year = {2011},
  pages = {119--129},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-23138-4_8}
}

@article{laudanna-et-al-1992-processing,
  title = {Processing Inflectional and Derivational Morphology},
  author = {Laudanna, Alessandro and Badecker, William and Caramazza, Alfonso},
  year = {1992},
  journal = {Journal of Memory and Language},
  volume = {31},
  number = {3},
  pages = {333--348},
  publisher = {Elsevier}
}

@article{levenshtein-1966-binary,
  title = {Binary Codes Capable of Correcting Deletions, Insertions and Reversals},
  author = {Levenshtein, Vladimir},
  year = {1966},
  journal = {Soviet Physics Doklady},
  volume = {10},
  pages = {707}
}

@inproceedings{levshina-2020-how,
  title = {How Tight Is Your Language? {{A}} Semantic Typology Based on {{Mutual Information}}},
  shorttitle = {How Tight Is Your Language?},
  booktitle = {Proceedings of the 19th {{International Workshop}} on {{Treebanks}} and {{Linguistic Theories}}},
  author = {Levshina, Natalia},
  editor = {Evang, Kilian and Kallmeyer, Laura and Ehren, Rafael and Petitjean, Simon and Seyffarth, Esther and Seddah, Djam{\'e}},
  year = {2020},
  month = oct,
  pages = {70--78},
  publisher = {Association for Computational Linguistics},
  address = {D{\"u}sseldorf, Germany},
  doi = {10.18653/v1/2020.tlt-1.7},
  urldate = {2025-05-14},
  keywords = {sivan},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Levshina_2020_How tight is your language.pdf;/Users/coleman/Zotero/storage/WRF88I4U/Levshina_2020_How tight is your language.pdf}
}

@article{lier-2017-typology,
  title = {The Typology of Property Words in {{Oceanic}} Languages},
  author = {van Lier, Eva},
  year = {2017},
  month = nov,
  journal = {Linguistics},
  volume = {55},
  number = {6},
  pages = {1237--1280},
  publisher = {De Gruyter Mouton},
  issn = {1613-396X},
  doi = {10.1515/ling-2017-0027},
  urldate = {2025-09-05},
  abstract = {This paper describes the morphosyntactic behavior of different semantic types of property words in a balanced sample of 36 Oceanic languages. After a brief general introduction to the functional typology of property words, I first discuss diversity in Oceanic property word classes from a family-internal perspective. In the second part of the paper, Oceanic property words are placed in a world-wide typological perspective. Specifically, I test their behavior with regard to two implicational universals proposed in the literature, concerning the relation between the encoding of predicative property words, the presence of grammatical tense, and locus of marking at the clause level. In typological studies, the Oceanic language family has been claimed to display verbal predicative property words, to lack tense, and to be head- or zero-marking, with marginal exceptions. This paper shows that, even though such an overall profile can be discerned, Oceanic property words exhibit more variation than is acknowledged in crosslinguistic research. Moreover, my findings for property word classes are fitted into a larger picture of lexical categorization in Oceanic languages.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  keywords = {adjectives,locus of marking,Oceanic languages,property words,tense},
  file = {/Users/coleman/Zotero/storage/B8NGB7ES/Lier - 2017 - The typology of property words in Oceanic languages.pdf}
}

@article{liljencrants-et-al-1972-numerical,
  title = {Numerical {{Simulation}} of {{Vowel Quality Systems}}: {{The Role}} of {{Perceptual Contrast}}},
  shorttitle = {Numerical {{Simulation}} of {{Vowel Quality Systems}}},
  author = {Liljencrants, Johan and Lindblom, Bj{\"o}rn and Lindblom, Bjorn},
  year = {1972},
  month = dec,
  journal = {Language},
  volume = {48},
  number = {4},
  pages = {839--862},
  issn = {00978507},
  doi = {10.2307/411991},
  urldate = {2024-10-07},
  jstor = {411991}
}

@inproceedings{lin-et-al-2014-microsoft,
  title = {Microsoft {{COCO}}: {{Common}} Objects in Context},
  booktitle = {Computer Vision -- {{ECCV}} 2014},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C. Lawrence},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  year = {2014},
  pages = {740--755},
  publisher = {Springer International Publishing},
  address = {Cham},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  isbn = {978-3-319-10602-1}
}

@article{lin-et-al-2022-word,
  title = {Word Imageability Is Associated with Expressive Vocabulary in Children with Autism Spectrum Disorder},
  author = {Lin, Kimberly R and Wisman Weil, Lisa and Thurm, Audrey and Lord, Catherine and Luyster, Rhiannon J},
  year = {2022},
  month = mar,
  journal = {Autism \& Developmental Language Impairments},
  volume = {7},
  pages = {23969415221085827},
  issn = {2396-9415},
  doi = {10.1177/23969415221085827},
  urldate = {2024-07-23},
  abstract = {Background \& aims Throughout typical development, children prioritize different perceptual, social, and linguistic cues to learn words. The earliest acquired words are often those that are perceptually salient and highly imageable. Imageability, the ease in which a word evokes a mental image, is a strong predictor for word age of acquisition in typically developing (TD) children, independent of other lexicosemantic features such as word frequency. However, little is known about the effects of imageability in children with autism spectrum disorder (ASD), who tend to have differences in linguistic processing and delayed language acquisition compared to their TD peers. This study explores the extent to which imageability and word frequency are associated with early noun and verb acquisition in children with ASD. Methods Secondary analyses were conducted on previously collected data of 156 children (78 TD, 78 ASD) matched on sex and parent-reported language level. Total expressive vocabulary, as measured by the MacArthur Bates Communicative Development Inventory (MB-CDI), included 123 words (78 nouns, 45 verbs) that overlapped with previously published imageability ratings and word input frequencies. A two-step hierarchical linear regression was used to examine the relationship between word input frequency, imageability, and total expressive vocabulary. An F-test was then used to assess the unique contribution of imageability on total expressive vocabulary when controlling for word input frequency. Results In both the TD and ASD groups, imageability uniquely explained a portion of the variance in total expressive vocabulary size, independent of word input frequency. Notably, imageability was significantly associated with noun vocabulary and verb vocabulary size alone, with imageability explaining a greater portion of the variance in total nouns produced than in total verbs produced. Conclusions Imageability was identified as a significant lexicosemantic feature for describing expressive vocabulary size in children with ASD. Consistent with literature on TD children, children with ASD who have small vocabularies primarily produce words that are highly imageable. Children who are more proficient word learners with larger vocabularies produce words that are less imageable, indicating a potential shift away from reliance on perceptual-based language processing. This was consistent across both noun and verb vocabularies. Implications Our findings contribute to a growing body of literature describing early word learning in children with ASD and provide a basis for exploring the use of multisensory language learning strategies.},
  pmcid = {PMC9620684},
  pmid = {36382067},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Lin et al_2022_Word imageability is associated with expressive vocabulary in children with.pdf;/Users/coleman/Zotero/storage/B5RHJET4/Lin et al_2022_Word imageability is associated with expressive vocabulary in children with.pdf}
}

@inproceedings{liu-et-al-2021-lexical,
  title = {Lexical {{Semantic Recognition}}},
  booktitle = {Proceedings of the 17th {{Workshop}} on {{Multiword Expressions}} ({{MWE}} 2021)},
  author = {Liu, Nelson F. and Hershcovich, Daniel and Kranzlein, Michael and Schneider, Nathan},
  editor = {Cook, Paul and Mitrovi{\'c}, Jelena and Escart{\'i}n, Carla Parra and Vaidya, Ashwini and Osenova, Petya and Taslimipoor, Shiva and Ramisch, Carlos},
  year = {2021},
  month = aug,
  pages = {49--56},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.mwe-1.6},
  urldate = {2025-08-14},
  abstract = {In lexical semantics, full-sentence segmentation and segment labeling of various phenomena are generally treated separately, despite their interdependence. We hypothesize that a unified lexical semantic recognition task is an effective way to encapsulate previously disparate styles of annotation, including multiword expression identification / classification and supersense tagging. Using the STREUSLE corpus, we train a neural CRF sequence tagger and evaluate its performance along various axes of annotation. As the label set generalizes that of previous tasks (PARSEME, DiMSUM), we additionally evaluate how well the model generalizes to those test sets, finding that it approaches or surpasses existing models despite training only on STREUSLE. Our work also establishes baseline models and evaluation metrics for integrated and accurate modeling of lexical semantics, facilitating future work in this area.},
  file = {/Users/coleman/Zotero/storage/TNVANPJ2/Liu et al. - 2021 - Lexical Semantic Recognition.pdf}
}

@inproceedings{ljubesic-et-al-2018-predicting,
  title = {Predicting {{Concreteness}} and {{Imageability}} of {{Words Within}} and {{Across Languages}} via {{Word Embeddings}}},
  booktitle = {Proceedings of the {{Third Workshop}} on {{Representation Learning}} for {{NLP}}},
  author = {Ljube{\v s}i{\'c}, Nikola and Fi{\v s}er, Darja and {Peti-Stanti{\'c}}, Anita},
  editor = {Augenstein, Isabelle and Cao, Kris and He, He and Hill, Felix and Gella, Spandana and Kiros, Jamie and Mei, Hongyuan and Misra, Dipendra},
  year = {2018},
  month = jul,
  pages = {217--222},
  publisher = {Association for Computational Linguistics},
  address = {Melbourne, Australia},
  doi = {10.18653/v1/W18-3028},
  urldate = {2024-10-07},
  abstract = {The notions of concreteness and imageability, traditionally important in psycholinguistics, are gaining significance in semantic-oriented natural language processing tasks. In this paper we investigate the predictability of these two concepts via supervised learning, using word embeddings as explanatory variables. We perform predictions both within and across languages by exploiting collections of cross-lingual embeddings aligned to a single vector space. We show that the notions of concreteness and imageability are highly predictable both within and across languages, with a moderate loss of up to 20\% in correlation when predicting across languages. We further show that the cross-lingual transfer via word embeddings is more efficient than the simple transfer via bilingual dictionaries.},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Ljubei et al_2018_Predicting Concreteness and Imageability of Words Within and Across Languages.pdf;/Users/coleman/Zotero/storage/R29EBYB9/Ljubei et al_2018_Predicting Concreteness and Imageability of Words Within and Across Languages.pdf}
}

@article{lynott-et-al-2020-lancaster,
  title = {The {{Lancaster Sensorimotor Norms}}: Multidimensional Measures of Perceptual and Action Strength for 40,000 {{English}} Words},
  shorttitle = {The {{Lancaster Sensorimotor Norms}}},
  author = {Lynott, Dermot and Connell, Louise and Brysbaert, Marc and Brand, James and Carney, James},
  year = {2020},
  month = jun,
  journal = {Behavior Research Methods},
  volume = {52},
  number = {3},
  pages = {1271--1291},
  issn = {1554-3528},
  doi = {10.3758/s13428-019-01316-z},
  urldate = {2024-10-15},
  abstract = {Sensorimotor information plays a fundamental role in cognition. However, the existing materials that measure the sensorimotor basis of word meanings and concepts have been restricted in terms of their sample size and breadth of sensorimotor experience. Here we present norms of sensorimotor strength for 39,707 concepts across six perceptual modalities (touch, hearing, smell, taste, vision, and interoception) and five action effectors (mouth/throat, hand/arm, foot/leg, head excluding mouth/throat, and torso), gathered from a total of 3,500 individual participants using Amazon's Mechanical Turk platform. The Lancaster Sensorimotor Norms are unique and innovative in a number of respects: They represent the largest-ever set of semantic norms for English, at 40,000 words {\texttimes} 11 dimensions (plus several informative cross-dimensional variables), they extend perceptual strength norming to the new modality of interoception, and they include the first norming of action strength across separate bodily effectors. In the first study, we describe the data collection procedures, provide summary descriptives of the dataset, and interpret the relations observed between sensorimotor dimensions. We then report two further studies, in which we (1) extracted an optimal single-variable composite of the 11-dimension sensorimotor profile (Minkowski 3 strength) and (2) demonstrated the utility of both perceptual and action strength in facilitating lexical decision times and accuracy in two separate datasets. These norms provide a valuable resource to researchers in diverse areas, including psycholinguistics, grounded cognition, cognitive semantics, knowledge representation, machine learning, and big-data approaches to the analysis of language and conceptual representations. The data are accessible via the Open Science Framework (http://osf.io/7emr6/) and an interactive web application (https://www.lancaster.ac.uk/psychology/lsnorms/).},
  langid = {english},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Lynott et al_2020_The Lancaster Sensorimotor Norms.pdf;/Users/coleman/Zotero/storage/M847Z364/Lynott et al_2020_The Lancaster Sensorimotor Norms.pdf}
}

@article{mackay-1978-derivational,
  title = {Derivational Rules and the Internal Lexicon},
  author = {MacKay, Donald G},
  year = {1978},
  journal = {Journal of verbal learning and verbal behavior},
  volume = {17},
  number = {1},
  pages = {61--71},
  publisher = {Elsevier}
}

@inproceedings{malouf-et-al-2020-lexical,
  title = {Lexical Databases for Computational Analyses: {{A}} Linguistic Perspective},
  booktitle = {Proceedings of the Society for Computation in Linguistics 2020},
  author = {Malouf, Robert and Ackerman, Farrell and Semenuks, Arturs},
  editor = {Ettinger, Allyson and Jarosz, Gaja and Pater, Joe},
  year = {2020},
  month = jan,
  pages = {446--456},
  publisher = {Association for Computational Linguistics},
  address = {New York, New York},
  url = {https://aclanthology.org/2020.scil-1.52}
}

@article{manzini-et-al-lexical,
  title = {On the {{Lexical}}/{{Functional Divide}}: {{The Case}} of {{Negation}}},
  shorttitle = {On the {{Lexical}}/{{Functional Divide}}},
  author = {Manzini, M. Rita and Savoia, Leonardo M.},
  url = {https://academic.oup.com/book/32617/chapter/270490642},
  urldate = {2025-09-06},
  abstract = {Abstract. This chapter challenges the distinction between functional and lexical items, and argues that what is usually claimed to fill the head or the spe},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/96AB4HW2/270490642.html}
}

@book{martinheidegger-1916-kategorien,
  title = {{Die Kategorien- und Bedeutungslehre des Duns Scotus}},
  author = {{Martin Heidegger}},
  year = {1916},
  url = {http://archive.org/details/Bedeutungslehre},
  urldate = {2025-09-06},
  abstract = {T{\"u}bingen : Mohr, 1916 Teilw. zugl.: Freiburg i.Br., Univ., Habil.-Schr., 1915},
  copyright = {http://creativecommons.org/publicdomain/mark/1.0/},
  langid = {german},
  keywords = {Philosophy}
}

@article{matzig-spared,
  title = {Spared Syntax and Impaired Spell-out: The Case of Prepositions in {{Broca}}'s and Anomic Aphasia},
  author = {M{\"a}tzig, S},
  abstract = {The present study deals with the impairment of prepositions, a somewhat neglected topic in aphasia research. It is the first to investigate the availability of all types of prepositions (i.e., spatial, temporal, other meaningful, subcategorized, syntactic prepositions, and particles) in a variety of comprehension and production tasks in one anomic aphasic and four Broca's aphasic patients and healthy speakers. While the availability of spatial, temporal, or subcategorized prepositions has been investigated, other preposition types have never been studied before.},
  langid = {english},
  keywords = {adpositions},
  file = {/Users/coleman/Zotero/storage/VS7L2GAM/Mtzig - Spared syntax and impaired spell-out the case of prepositions in Broca's and anomic aphasia.pdf}
}

@inproceedings{mccarthy-et-al-2020-unimorph,
  title = {{{UniMorph}} 3.0: {{Universal Morphology}}},
  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
  author = {McCarthy, Arya D. and Kirov, Christo and Grella, Matteo and Nidhi, Amrit and Xia, Patrick and Gorman, Kyle and Vylomova, Ekaterina and Mielke, Sabrina J. and Nicolai, Garrett and Silfverberg, Miikka and Arkhangelskiy, Timofey and Krizhanovsky, Nataly and Krizhanovsky, Andrew and Klyachko, Elena and Sorokin, Alexey and Mansfield, John and Ern{\v s}treits, Valts and Pinter, Yuval and Jacobs, Cassandra L. and Cotterell, Ryan and Hulden, Mans and Yarowsky, David},
  year = {2020},
  month = may,
  pages = {3922--3931},
  publisher = {European Language Resources Association},
  address = {Marseille, France},
  url = {https://aclanthology.org/2020.lrec-1.483},
  abstract = {The Universal Morphology (UniMorph) project is a collaborative effort providing broad-coverage instantiated normalized morphological paradigms for hundreds of diverse world languages. The project comprises two major thrusts: a language-independent feature schema for rich morphological annotation and a type-level resource of annotated data in diverse languages realizing that schema. We have implemented several improvements to the extraction pipeline which creates most of our data, so that it is both more complete and more correct. We have added 66 new languages, as well as new parts of speech for 12 languages. We have also amended the schema in several ways. Finally, we present three new community tools: two to validate data for resource creators, and one to make morphological data available from the command line. UniMorph is based at the Center for Language and Speech Processing (CLSP) at Johns Hopkins University in Baltimore, Maryland. This paper details advances made to the schema, tooling, and dissemination of project resources since the UniMorph 2.0 release described at LREC 2018.},
  isbn = {979-10-95546-34-4},
  langid = {english}
}

@inproceedings{mikolov-et-al-2013-distributed,
  title = {Distributed Representations of Words and Phrases and Their Compositionality},
  booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  series = {{{NIPS}}'13},
  pages = {3111--3119},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.}
}

@article{narasimhan-et-al-2015-unsupervised,
  title = {An Unsupervised Method for Uncovering Morphological Chains},
  author = {Narasimhan, Karthik and Barzilay, Regina and Jaakkola, Tommi},
  year = {2015},
  month = dec,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {3},
  pages = {157--167}
}

@article{newmeyer-2007-linguistic,
  title = {Linguistic Typology Requires Crosslinguistic Formal Categories},
  author = {Newmeyer, Frederick J},
  year = {2007},
  series = {Linguistic {{Typology}}},
  volume = {11},
  number = {1},
  pages = {133--157},
  doi = {10.1515/LINGTY.2007.012},
  urldate = {2024-05-14},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Newmeyer_2007_Linguistic typology requires crosslinguistic formal categories.pdf}
}

@incollection{newmeyer-1999-discrete,
  title = {The {{Discrete Nature Of Syntactic Categories}}: {{Against A Prototype-Based Account}}},
  shorttitle = {The {{Discrete Nature Of Syntactic Categories}}},
  author = {Newmeyer, Frederick J.},
  year = {1999},
  month = jan,
  publisher = {Brill},
  doi = {10.1163/9781849500098_009},
  urldate = {2025-09-12},
  chapter = {The Nature and Function of Syntactic Categories},
  langid = {english},
  keywords = {Languages and Linguistics,Morphology & Syntax}
}

@inproceedings{oh-et-al-2024-leading,
  title = {Leading {{Whitespaces}} of {{Language Models}}' {{Subword Vocabulary Pose}} a {{Confound}} for {{Calculating Word Probabilities}}},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Oh, Byung-Doh and Schuler, William},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = {2024},
  month = nov,
  pages = {3464--3472},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.202},
  urldate = {2025-09-12},
  file = {/Users/coleman/Zotero/storage/P5SK7MBN/Oh and Schuler - 2024 - Leading Whitespaces of Language Models' Subword Vocabulary Pose a Confound for Calculating Word Prob.pdf}
}

@inproceedings{ostling-2015-word,
  title = {Word {{Order Typology}} through {{Multilingual Word Alignment}}},
  booktitle = {Proceedings of the 53rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 7th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 2: {{Short Papers}})},
  author = {{\"O}stling, Robert},
  editor = {Zong, Chengqing and Strube, Michael},
  year = {2015},
  month = jul,
  pages = {205--211},
  publisher = {Association for Computational Linguistics},
  address = {Beijing, China},
  doi = {10.3115/v1/P15-2034},
  urldate = {2025-05-14},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/stling_2015_Word Order Typology through Multilingual Word Alignment.pdf;/Users/coleman/Zotero/storage/UBETLAMZ/stling_2015_Word Order Typology through Multilingual Word Alignment.pdf}
}

@inproceedings{pawley-2006-where,
  title = {Where Have All the Verbs Gone? {{Remarks}} on the Organisation of Languages with Small, Closed Verb Classes},
  booktitle = {11th Biennial Rice University Linguistics Symposium},
  author = {Pawley, Andrew K.},
  year = {2006},
  url = {http://www.ruf.rice.edu/~lingsymp/Pawley_paper.pdf}
}

@inproceedings{pennington-et-al-2014-glove,
  title = {{{GloVe}}: {{Global}} Vectors for Word Representation},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  year = {2014},
  month = oct,
  pages = {1532--1543},
  publisher = {Association for Computational Linguistics},
  address = {Doha, Qatar},
  doi = {10.3115/v1/D14-1162}
}

@article{perlmutter-1988-split,
  title = {The Split Morphology Hypothesis: {{Evidence}} from {{Yiddish}}},
  author = {Perlmutter, David},
  year = {1988},
  journal = {Theoretical morphology},
  pages = {79--100},
  publisher = {Academic Press San Diego, CA}
}

@misc{piantadosi-et-al-2022-meaning,
  title = {Meaning without Reference in Large Language Models},
  author = {Piantadosi, Steven T. and Hill, Felix},
  year = {2022},
  month = aug,
  number = {arXiv:2208.02957},
  eprint = {2208.02957},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.02957},
  urldate = {2025-09-06},
  abstract = {The widespread success of large language models (LLMs) has been met with skepticism that they possess anything like human concepts or meanings. Contrary to claims that LLMs possess no meaning whatsoever, we argue that they likely capture important aspects of meaning, and moreover work in a way that approximates a compelling account of human cognition in which meaning arises from conceptual role. Because conceptual role is defined by the relationships between internal representational states, meaning cannot be determined from a model's architecture, training data, or objective function, but only by examination of how its internal states relate to each other. This approach may clarify why and how LLMs are so successful and suggest how they can be made more human-like.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/coleman/Zotero/storage/PCWNLGYL/Piantadosi and Hill - 2022 - Meaning without reference in large language models.pdf;/Users/coleman/Zotero/storage/ULKYXVPX/2208.html}
}

@inproceedings{pimentel-et-al-2024-how,
  title = {How to {{Compute}} the {{Probability}} of a {{Word}}},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Pimentel, Tiago and Meister, Clara},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = {2024},
  month = nov,
  pages = {18358--18375},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.1020},
  urldate = {2025-09-12},
  abstract = {Language models (LMs) estimate a probability distribution over strings in a natural language; these distributions are crucial for computing perplexity and surprisal in linguistics research. While we are usually concerned with measuring these values for words, most LMs operate over subwords. Despite seemingly straightforward, accurately computing probabilities over one unit given probabilities over the other requires care. Indeed, we show here that many recent linguistic studies have been incorrectly computing these values. This paper derives the correct methods for computing word probabilities, highlighting issues when relying on language models that use beginning-of-word (bow)-marking tokenisers, e.g., the GPT family. Empirically, we show that correcting the widespread bug in probability computations affects measured outcomes in sentence comprehension and lexical optimisation analyses.},
  file = {/Users/coleman/Zotero/storage/TPJAMMLP/Pimentel and Meister - 2024 - How to Compute the Probability of a Word.pdf}
}

@incollection{plank-1994-inflection,
  title = {Inflection and Derivation},
  booktitle = {The Encyclopedia of Language and Linguistics},
  author = {Plank, Frans},
  year = {1994},
  pages = {1671--1679},
  publisher = {{Elsevier Science and Technology}},
  address = {Amsterdam}
}

@article{plank-2017-extent,
  title = {Extent and Limits of Linguistic Diversity as the Remit of Typology -- but through Constraints on What Is Diversity Limited?},
  author = {Plank, Frans},
  year = {2017},
  journal = {Linguistic Typology},
  volume = {21},
  number = {2017},
  pages = {43--68},
  doi = {doi:10.1515/lingty-2017-1004},
  urldate = {2024-05-14}
}

@inproceedings{qi-et-al-2020-stanza,
  title = {Stanza: {{A Python Natural Language Processing Toolkit}} for {{Many Human Languages}}},
  shorttitle = {Stanza},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{System Demonstrations}}},
  author = {Qi, Peng and Zhang, Yuhao and Zhang, Yuhui and Bolton, Jason and Manning, Christopher D.},
  editor = {Celikyilmaz, Asli and Wen, Tsung-Hsien},
  year = {2020},
  month = jul,
  pages = {101--108},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-demos.14},
  urldate = {2024-10-07},
  abstract = {We introduce Stanza, an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, and named entity recognition. We have trained Stanza on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested. Additionally, Stanza includes a native Python interface to the widely used Java Stanford CoreNLP software, which further extends its functionality to cover other tasks such as coreference resolution and relation extraction. Source code, documentation, and pretrained models for 66 languages are available at https://stanfordnlp.github.io/stanza/.},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Qi et al_2020_Stanza.pdf;/Users/coleman/Zotero/storage/D8FTDVGI/Qi et al_2020_Stanza.pdf}
}

@article{richards-2009-nouns,
  title = {Nouns, Verbs, and Hidden Structure in {{Tagalog}}},
  author = {Richards, Norvin},
  year = {2009},
  month = jul,
  journal = {Theoretical Linguistics},
  volume = {35},
  number = {1},
  pages = {139--152},
  issn = {0301-4428, 1613-4060},
  doi = {10.1515/THLI.2009.008},
  urldate = {2024-10-07},
  langid = {english},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Richards_2009_Nouns, verbs, and hidden structure in Tagalog.pdf;/Users/coleman/Zotero/storage/RJDDMYPC/Richards_2009_Nouns, verbs, and hidden structure in Tagalog.pdf}
}

@article{rofes-et-al-2018-imageability,
  title = {Imageability Ratings across Languages},
  author = {Rofes, Adri{\`a} and Zakari{\'a}s, Lilla and Ceder, Klaudia and Lind, Marianne and Johansson, Monica Blom and {de Aguiar}, V{\^a}nia and Bjeki{\'c}, Jovana and Fyndanis, Valantis and Gavarr{\'o}, Anna and Simonsen, Hanne Gram and Sacrist{\'a}n, Carlos Hern{\'a}ndez and Kambanaros, Maria and Kraljevi{\'c}, Jelena Kuva{\v c} and {Mart{\'i}nez-Ferreiro}, Silvia and Mavis, {\.I}lknur and Orellana, Carolina M{\'e}ndez and S{\"o}r, Ingrid and Luk{\'a}cs, {\'A}gnes and Tun{\c c}er, M{\"u}ge and Vuksanovi{\'c}, Jasmina and Ibarrola, Amaia Munarriz and Pourquie, Marie and Varlokosta, Spyridoula and Howard, David},
  year = {2018},
  month = jun,
  journal = {Behavior Research Methods},
  volume = {50},
  number = {3},
  pages = {1187--1197},
  issn = {1554-3528},
  doi = {10.3758/s13428-017-0936-0},
  abstract = {Imageability is a psycholinguistic variable that indicates how well a word gives rise to a mental image or sensory experience. Imageability ratings are used extensively in psycholinguistic, neuropsychological, and aphasiological studies. However, little formal knowledge exists about whether and how these ratings are associated between and within languages. Fifteen imageability databases were cross-correlated using nonparametric statistics. Some of these corresponded to unpublished data collected within a European research network---the Collaboration of Aphasia Trialists (COST IS1208). All but four of the correlations were significant. The average strength of the correlations (rho = .68) and the variance explained (R2 = 46\%) were moderate. This implies that factors other than imageability may explain 54\% of the results. Imageability ratings often correlate across languages. Different possibly interacting factors may explain the moderate strength and variance explained in the correlations: (1) linguistic and cultural factors; (2) intrinsic differences between the databases; (3) range effects; (4) small numbers of words in each database, equivalent words, and participants; and (5) mean age of the participants. The results suggest that imageability ratings may be used cross-linguistically. However, further understanding of the factors explaining the variance in the correlations will be needed before research and practical recommendations can be made.}
}

@article{rogers-illustrating,
  title = {Illustrating the Prototype Structures of Parts of Speech: {{A}} Multidimensional Scaling Analysis},
  author = {Rogers, Phillip},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/CVBYQ6HK/Rogers - Illustrating the prototype structures of parts of speech A multidimensional scaling analysis.pdf}
}

@inproceedings{rosa-et-al-2019-attempting,
  title = {Attempting to Separate Inflection and Derivation Using Vector Space Representations},
  booktitle = {Proceedings of the Second International Workshop on Resources and Tools for Derivational Morphology},
  author = {Rosa, Rudolf and {\v Z}abokrtsk{\'y}, Zden{\v e}k},
  year = {2019},
  month = sep,
  pages = {61--70},
  publisher = {{Charles University, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics}},
  address = {Prague, Czechia},
  url = {https://aclanthology.org/W19-8508}
}

@article{rosa-et-al-2019-unsupervised,
  title = {Unsupervised Lemmatization as Embeddings-Based Word Clustering},
  author = {Rosa, Rudolf and Zabokrtsk{\'y}, Zdenek},
  year = {2019},
  journal = {CoRR},
  volume = {abs/1908.08528},
  eprint = {1908.08528},
  url = {http://arxiv.org/abs/1908.08528},
  archiveprefix = {arXiv},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  timestamp = {Sat, 23 Jan 2021 01:11:13 +0100}
}

@inproceedings{ross-1972-category,
  title = {The Category Squish: {{Endstation Hauptwort}}},
  booktitle = {Proceedings of the Eighth Regional Meeting of the Chicago Linguistic Society},
  author = {Ross, John R.},
  editor = {Peranteau, Paul M. and Levi, Judith N. and Phares, Gloria C.},
  year = {1972},
  pages = {316--328},
  publisher = {Chicago Linguistic Society, University of Chicago},
  address = {Chicago, Illinois}
}

@article{saffran-et-al-1996-statistical,
  title = {Statistical Learning by 8-Month-Old Infants},
  author = {Saffran, Jenny R and Aslin, Richard N and Newport, Elissa L},
  year = {1996},
  journal = {Science},
  volume = {274},
  number = {5294},
  pages = {1926--1928},
  publisher = {American Association for the Advancement of Science}
}

@article{schakel-et-al-2015-measuring,
  title = {Measuring Word Significance Using Distributed Representations of Words},
  author = {Schakel, Adriaan M. J. and Wilson, Benjamin J.},
  year = {2015},
  journal = {Computing Research Repository},
  volume = {arXiv:1508.02297},
  url = {http://arxiv.org/abs/1508.02297}
}

@inproceedings{schneider-et-al-2015-hierarchy,
  title = {A {{Hierarchy}} with, of, and for {{Preposition Supersenses}}},
  booktitle = {Proceedings of the 9th {{Linguistic Annotation Workshop}}},
  author = {Schneider, Nathan and Srikumar, Vivek and Hwang, Jena D. and Palmer, Martha},
  editor = {Meyers, Adam and Rehbein, Ines and Zinsmeister, Heike},
  year = {2015},
  month = jun,
  pages = {112--123},
  publisher = {Association for Computational Linguistics},
  address = {Denver, Colorado, USA},
  doi = {10.3115/v1/W15-1612},
  urldate = {2025-09-06},
  file = {/Users/coleman/Zotero/storage/QZKINCSK/Schneider et al. - 2015 - A Hierarchy with, of, and for Preposition Supersenses.pdf}
}

@inproceedings{schneider-et-al-2018-comprehensive,
  title = {Comprehensive {{Supersense Disambiguation}} of {{English Prepositions}} and {{Possessives}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Schneider, Nathan and Hwang, Jena D. and Srikumar, Vivek and Prange, Jakob and Blodgett, Austin and Moeller, Sarah R. and Stern, Aviram and Bitan, Adi and Abend, Omri},
  editor = {Gurevych, Iryna and Miyao, Yusuke},
  year = {2018},
  month = jul,
  pages = {185--196},
  publisher = {Association for Computational Linguistics},
  address = {Melbourne, Australia},
  doi = {10.18653/v1/P18-1018},
  urldate = {2025-09-06},
  abstract = {Semantic relations are often signaled with prepositional or possessive marking---but extreme polysemy bedevils their analysis and automatic interpretation. We introduce a new annotation scheme, corpus, and task for the disambiguation of prepositions and possessives in English. Unlike previous approaches, our annotations are comprehensive with respect to types and tokens of these markers; use broadly applicable supersense classes rather than fine-grained dictionary definitions; unite prepositions and possessives under the same class inventory; and distinguish between a marker's lexical contribution and the role it marks in the context of a predicate or scene. Strong interannotator agreement rates, as well as encouraging disambiguation results with established supervised methods, speak to the viability of the scheme and task.},
  file = {/Users/coleman/Zotero/storage/DDC5TNX2/Schneider et al. - 2018 - Comprehensive Supersense Disambiguation of English Prepositions and Possessives.pdf}
}

@misc{schneider-et-al-2022-adposition,
  title = {Adposition and {{Case Supersenses}} v2.6: {{Guidelines}} for {{English}}},
  shorttitle = {Adposition and {{Case Supersenses}} v2.6},
  author = {Schneider, Nathan and Hwang, Jena D. and Srikumar, Vivek and Bhatia, Archna and Han, Na-Rae and O'Gorman, Tim and Moeller, Sarah R. and Abend, Omri and Shalev, Adi and Blodgett, Austin and Prange, Jakob},
  year = {2022},
  month = jul,
  number = {arXiv:1704.02134},
  eprint = {1704.02134},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1704.02134},
  urldate = {2025-08-09},
  abstract = {This document offers a detailed linguistic description of SNACS (Semantic Network of Adposition and Case Supersenses; Schneider et al., 2018), an inventory of 52 semantic labels ("supersenses") that characterize the use of adpositions and case markers at a somewhat coarse level of granularity, as demonstrated in the STREUSLE corpus (https://github.com/nert-nlp/streusle/ ; version 4.5 tracks guidelines version 2.6). Though the SNACS inventory aspires to be universal, this document is specific to English; documentation for other languages will be published separately. Version 2 is a revision of the supersense inventory proposed for English by Schneider et al. (2015, 2016) (henceforth "v1"), which in turn was based on previous schemes. The present inventory was developed after extensive review of the v1 corpus annotations for English, plus previously unanalyzed genitive case possessives (Blodgett and Schneider, 2018), as well as consideration of adposition and case phenomena in Hebrew, Hindi, Korean, and German. Hwang et al. (2017) present the theoretical underpinnings of the v2 scheme. Schneider et al. (2018) summarize the scheme, its application to English corpus data, and an automatic disambiguation task. Liu et al. (2021) offer an English Lexical Semantic Recognition tagger that includes SNACS labels in its output. This documentation can also be browsed alongside corpus data on the Xposition website (Gessler et al., 2022): http://www.xposition.org/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/coleman/Zotero/storage/87WXYVUM/Schneider et al. - 2022 - Adposition and Case Supersenses v2.6 Guidelines for English.pdf;/Users/coleman/Zotero/storage/EYUE4ITM/1704.html}
}

@phdthesis{schultze-berndt-2000-simple,
  title = {Simple and Complex Verbs in {{Jaminjung}}: {{A}} Study of Event Categorisation in an {{Australian}} Language},
  shorttitle = {Simple and Complex Verbs in {{Jaminjung}}},
  author = {{Schultze-Berndt}, Eva},
  year = {2000},
  address = {Nijmegen},
  url = {http://pubman.mpdl.mpg.de/pubman/item/escidoc:2057716},
  urldate = {2024-10-07},
  langid = {english},
  school = {Radboud University}
}

@article{schwartz-et-al-1997-dispersionfocalization,
  title = {The {{Dispersion-Focalization Theory}} of Vowel Systems},
  author = {Schwartz, Jean-Luc and Bo{\"e}, Louis-Jean and Vall{\'e}e, Nathalie and Abry, Christian},
  year = {1997},
  month = jul,
  journal = {Journal of Phonetics},
  volume = {25},
  number = {3},
  pages = {255--286},
  issn = {00954470},
  doi = {10.1006/jpho.1997.0043},
  urldate = {2024-10-07},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english}
}

@inproceedings{scivetti-et-al-2025-multilingual,
  title = {Multilingual {{Supervision Improves Semantic Disambiguation}} of {{Adpositions}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Computational Linguistics}}},
  author = {Scivetti, Wesley and Levine, Lauren and Schneider, Nathan},
  editor = {Rambow, Owen and Wanner, Leo and Apidianaki, Marianna and {Al-Khalifa}, Hend and Eugenio, Barbara Di and Schockaert, Steven},
  year = {2025},
  month = jan,
  pages = {3655--3669},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, UAE},
  url = {https://aclanthology.org/2025.coling-main.247/},
  urldate = {2025-08-09},
  abstract = {Adpositions display a remarkable amount of ambiguity and flexibility in their meanings, and are used in different ways across languages. We conduct a systematic corpus-based cross-linguistic investigation into the lexical semantics of adpositions, utilizing SNACS (Schneider et al., 2018), an annotation framework with data available in several languages. Our investigation encompasses 5 of these languages: Chinese, English, Gujarati, Hindi, and Japanese. We find substantial distributional differences in adposition semantics, even in comparable corpora. We further train classifiers to disambiguate adpositions in each of our languages. Despite the cross-linguistic differences in adpositional usage, sharing annotated data across languages boosts overall disambiguation performance, leading to the highest published scores on this task for all 5 languages.},
  file = {/Users/coleman/Zotero/storage/4EQGCMBU/Scivetti et al. - 2025 - Multilingual Supervision Improves Semantic Disambiguation of Adpositions.pdf}
}

@article{scott-et-al-2019-glasgow,
  title = {The {{Glasgow Norms}}: {{Ratings}} of 5,500 Words on Nine Scales},
  shorttitle = {The {{Glasgow Norms}}},
  author = {Scott, Graham G. and Keitel, Anne and Becirspahic, Marc and Yao, Bo and Sereno, Sara C.},
  year = {2019},
  month = jun,
  journal = {Behavior Research Methods},
  volume = {51},
  number = {3},
  pages = {1258--1270},
  issn = {1554-3528},
  doi = {10.3758/s13428-018-1099-3},
  urldate = {2024-10-15},
  abstract = {The Glasgow Norms are a set of normative ratings for 5,553 English words on nine psycholinguistic dimensions: arousal, valence, dominance, concreteness, imageability, familiarity, age of acquisition, semantic size, and gender association. The Glasgow Norms are unique in several respects. First, the corpus itself is relatively large, while simultaneously providing norms across a substantial number of lexical dimensions. Second, for any given subset of words, the same participants provided ratings across all nine dimensions (33 participants/word, on average). Third, two novel dimensions---semantic size and gender association---are included. Finally, the corpus contains a set of 379 ambiguous words that are presented either alone (e.g., toast) or with information that selects an alternative sense (e.g., toast (bread), toast (speech)). The relationships between the dimensions of the Glasgow Norms were initially investigated by assessing their correlations. In addition, a principal component analysis revealed four main factors, accounting for 82\% of the variance (Visualization, Emotion, Salience, and Exposure). The validity of the Glasgow Norms was established via comparisons of our ratings to 18 different sets of current psycholinguistic norms. The dimension of size was tested with megastudy data, confirming findings from past studies that have explicitly examined this variable. Alternative senses of ambiguous words (i.e., disambiguated forms), when discordant on a given dimension, seemingly led to appropriately distinct ratings. Informal comparisons between the ratings of ambiguous words and of their alternative senses showed different patterns that likely depended on several factors (the number of senses, their relative strengths, and the rating scales themselves). Overall, the Glasgow Norms provide a valuable resource---in particular, for researchers investigating the role of word recognition in language comprehension.},
  langid = {english},
  keywords = {Age of acquisition,Arousal,Concreteness,Dominance,Familiarity,Gender association,Imageability,Psycholinguistic norms,Semantic size,Valence},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Scott et al_2019_The Glasgow Norms.pdf;/Users/coleman/Zotero/storage/NS4MX3L7/Scott et al_2019_The Glasgow Norms.pdf}
}

@incollection{silverstein-1986-7,
  title = {7. {{Hierarchy}} of Features and Ergativity},
  booktitle = {Features and Projections},
  author = {Silverstein, Michael},
  year = {1986},
  pages = {163--232},
  publisher = {De Gruyter Mouton},
  address = {Berlin, Boston},
  doi = {doi:10.1515/9783110871661-008},
  urldate = {2023-10-13},
  isbn = {978-3-11-087166-1}
}

@book{simone-et-al-2014-word,
  title = {Word {{Classes}}: {{Nature}}, {{Typology}} and {{Representations}}},
  shorttitle = {Word {{Classes}}},
  author = {Simone, Raffaele and Masini, Francesca},
  year = {2014},
  publisher = {John Benjamins Publishing Company},
  address = {Amsterdam/Philadelphia, NETHERLANDS, THE},
  url = {http://ebookcentral.proquest.com/lib/ed/detail.action?docID=1784085},
  urldate = {2025-09-09},
  isbn = {978-90-272-6976-8},
  keywords = {Grammar Comparative and general -- Grammatical catagories.,Parts of speech.,Typology (Linguistics),Word (Linguistics)},
  file = {/Users/coleman/Zotero/storage/EVZ5QKBH/detail.html}
}

@book{spencer-2013-lexical,
  title = {Lexical Relatedness},
  author = {Spencer, Andrew},
  year = {2013},
  publisher = {Oxford University Press},
  address = {Oxford}
}

@article{spreen-et-al-1966-parameters,
  title = {Parameters of Abstraction, Meaningfulness, and Pronunciability for 329 Nouns},
  author = {Spreen, Otfried and Schulz, Rudolph W.},
  year = {1966},
  month = oct,
  journal = {Journal of Verbal Learning and Verbal Behavior},
  volume = {5},
  number = {5},
  pages = {459--468},
  issn = {00225371},
  doi = {10.1016/S0022-5371(66)80061-0},
  urldate = {2024-10-07},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english}
}

@book{stassen-1997-intransitive,
  title = {Intransitive {{Predication}}},
  author = {Stassen, Leon},
  year = {1997},
  month = sep,
  publisher = {Oxford University Press},
  doi = {10.1093/oso/9780198236931.001.0001},
  abstract = {Intransitive Predication constitutes a major contribution to the study of typological linguistics and theoretical linguistics in general. Basing his analysis on a sample of 410 languages, Leon Stassen investigates cross-linguistic variation in one of the core domains of all natural languages. The author views this domain as a `cognitive space', the topography of which is the same for all languages. It is assumed to consist of four subdomains, which correspond to a four-way distinction between the semantic classes of event predicates, property predicates, class predicates, and locational predicates. Leon Stassen offers a typology of the structural manifestations of this domain, in terms of the nature and number of the formal strategies used in its encoding. He discusses a number of abstract principles which can be employed in explaining the cross-linguistic variation embodied by the typology. In the final chapter, he brings together the research results in a universally applicable model, which can be read as a `flow chart' for the encoding of intransitive predications in different language types.},
  isbn = {978-0-19-823693-1}
}

@incollection{stassen-2021-black,
  title = {Black and White {{Languages}}},
  booktitle = {Meaning and {{Grammar}} of {{Nouns}} and {{Verbs}}},
  author = {Stassen, Leon},
  editor = {Gerland, Doris and Horn, Christian and Latrouite, Anja and Ortmann, Albert},
  year = {2021},
  month = dec,
  pages = {315--338},
  publisher = {De Gruyter},
  doi = {10.1515/9783110720075-012},
  urldate = {2025-09-05},
  copyright = {http://creativecommons.org/licenses/by-nc-nd/4.0},
  isbn = {978-3-11-072007-5},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/M24GTGTH/Stassen - 2021 - Black and white Languages.pdf}
}

@article{staub-Forthcoming-predictability,
  title = {Predictability in {{Language Comprehension}}: {{Prospects}} and {{Problems}} for {{Surprisal}}},
  shorttitle = {Predictability in {{Language Comprehension}}},
  author = {Staub, Adrian},
  year = {Forthcoming},
  journal = {Annual Review of Linguistics},
  publisher = {Annual Reviews},
  doi = {10.1146/annurev-linguistics-011724-121517},
  urldate = {2024-10-14},
  abstract = {Surprisal theory proposes that a word\&apos;s predictability influences processing difficulty because each word requires the comprehender to update a probability distribution over possible sentences. This article first considers the theory\&apos;s detailed predictions regarding the effects of predictability on reading time and N400 amplitude. Two rather unintuitive predictions appear to be correct based on the current evidence: There is no specific cost when an unpredictable word is encountered in a context where another word is predictable, and the function relating predictability to processing difficulty is logarithmic, not linear. Next, the article addresses the viability of the claim, also associated with Surprisal, that conditional probability is the ``causal bottleneck'' mediating all effects on incremental processing difficulty. This claim fares less well as conditional probability does not account for the difficulty associated with encountering a low-frequency word or the difficulty associated with garden path disambiguation. Surprisal provides a compelling account of predictability effects but does not provide a complete account of incremental processing difficulty.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/C79Y8GYK/annurev-linguistics-011724-121517.html}
}

@incollection{stekauer-2015-14,
  title = {14. {{The}} Delimitation of Derivation and Inflection},
  booktitle = {Volume 1 Word-Formation},
  author = {{\v S}tekauer, Pavol},
  editor = {M{\"u}ller, Peter O. and Ohnheiser, Ingeborg and Olsen, Susan and Rainer, Franz},
  year = {2015},
  pages = {218--235},
  publisher = {de Gruyter Mouton}
}

@article{striklievers-et-al-2021-linguistic,
  title = {The Linguistic Dimensions of Concrete and Abstract Concepts: Lexical Category, Morphological Structure, Countability, and Etymology},
  author = {Strik Lievers, Francesca and Bolognesi, Marianna and Winter, Bodo},
  year = {2021},
  journal = {Cognitive Linguistics},
  volume = {32},
  number = {4},
  pages = {641--670},
  doi = {10.1515/cog-2021-0007},
  urldate = {2024-05-15}
}

@book{strunk-2020-finitestate,
  title = {A Finite-State Morphological Analyzer for Central Alaskan {{Yup}}'{{Ik}}},
  author = {Strunk, Lonny Alaskuk},
  year = {2020},
  publisher = {University of Washington}
}

@article{swingley-2005-statistical,
  title = {Statistical Clustering and the Contents of the Infant Vocabulary},
  author = {Swingley, Daniel},
  year = {2005},
  journal = {Cognitive psychology},
  volume = {50},
  number = {1},
  pages = {86--132},
  publisher = {Elsevier}
}

@misc{sylak-glassman-2016-composition,
  title = {The Composition and Use of the Universal Morphological Feature Schema ({{UniMorph}} Schema)},
  author = {{Sylak-Glassman}, John},
  year = {2016},
  url = {https://unimorph.github.io/doc/unimorph-schema.pdf}
}

@inproceedings{takaoka-et-al-2018-sudachi,
  title = {Sudachi: A {{Japanese Tokenizer}} for {{Business}}},
  shorttitle = {Sudachi},
  booktitle = {Proceedings of the {{Eleventh International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}} 2018)},
  author = {Takaoka, Kazuma and Hisamoto, Sorami and Kawahara, Noriko and Sakamoto, Miho and Uchida, Yoshitaka and Matsumoto, Yuji},
  editor = {Calzolari, Nicoletta and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Hasida, Koiti and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, H{\'e}l{\`e}ne and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios and Tokunaga, Takenobu},
  year = {2018},
  month = may,
  publisher = {European Language Resources Association (ELRA)},
  address = {Miyazaki, Japan},
  url = {https://aclanthology.org/L18-1355/},
  urldate = {2025-09-12},
  file = {/Users/coleman/Zotero/storage/BHTSQ8ND/Takaoka et al. - 2018 - Sudachi a Japanese Tokenizer for Business.pdf}
}


@article{tariq-fuzzy,
  title = {The `{{Fuzzy}}' {{Boundary Between Two Types}} of {{Japanese Adjectives}}},
  author = {Tariq, Ana Kanza},
  abstract = {The two kinds of Japanese adjectives, i-adjectives and na-adjectives, along with nouns, employ different forms (-i, -na, and no) to modify a noun. Based on such patterns, along with other grammatical characteristics identified in constructed examples, boundaries between lexical categories have traditionally been understood to be clear-cut.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/CH772RY2/Tariq - The Fuzzy Boundary Between Two Types of Japanese Adjectives.pdf}
}

@inproceedings{thapliyal-et-al-2022-crossmodal3600,
  title = {Crossmodal-3600: {{A Massively Multilingual Multimodal Evaluation Dataset}}},
  shorttitle = {Crossmodal-3600},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Thapliyal, Ashish V. and Pont Tuset, Jordi and Chen, Xi and Soricut, Radu},
  editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
  year = {2022},
  month = dec,
  pages = {715--729},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.45},
  urldate = {2024-10-07},
  abstract = {Research in massively multilingual image captioning has been severely hampered by a lack of high-quality evaluation datasets. In this paper we present the Crossmodal-3600 dataset (XM3600 in short), a geographically diverse set of 3600 images annotated with human-generated reference captions in 36 languages. The images were selected from across the world, covering regions where the 36 languages are spoken, and annotated with captions that achieve consistency in terms of style across all languages, while avoiding annotation artifacts due to direct translation. We apply this benchmark to model selection for massively multilingual image captioning models, and show superior correlation results with human evaluations when using XM3600 as golden references for automatic metrics.},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Thapliyal et al_2022_Crossmodal-3600.pdf;/Users/coleman/Zotero/storage/J924XLCK/Thapliyal et al_2022_Crossmodal-3600.pdf}
}

@article{theil-1970-estimation,
  title = {On the {{Estimation}} of {{Relationships Involving Qualitative Variables}}},
  author = {Theil, Henri},
  year = {1970},
  journal = {American Journal of Sociology},
  volume = {76},
  number = {1},
  pages = {103--154},
  publisher = {The University of Chicago Press},
  issn = {0002-9602},
  urldate = {2024-10-15},
  abstract = {This article is concerned with the specification and estimation of relationships whose dependent variable is qualitative in nature (such as "yes" or "no"). It discusses logit equations with and without interaction, and the estimation procedure is generalized least squares. Part I deals with dependent variables that take only two values, Par II with variables taking more than two values, and part III describes informational measures for the explanatory power of the determining factors. The discussion of more advanced technical matters is contained in various appendixes.},
  jstor = {2775440},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Theil_1970_On the Estimation of Relationships Involving Qualitative Variables.pdf;/Users/coleman/Zotero/storage/F3CXAY3U/Theil_1970_On the Estimation of Relationships Involving Qualitative Variables.pdf}
}

@article{thiessen-et-al-2003-when,
  title = {When Cues Collide: Use of Stress and Statistical Cues to Word Boundaries by 7-to 9-Month-Old Infants.},
  author = {Thiessen, Erik D and Saffran, Jenny R},
  year = {2003},
  journal = {Developmental psychology},
  volume = {39},
  number = {4},
  pages = {706},
  publisher = {American Psychological Association}
}

@article{thiessen-et-al-2013-extraction,
  title = {The Extraction and Integration Framework: A Two-Process Account of Statistical Learning.},
  author = {Thiessen, Erik D and Kronstein, Alexandra T and Hufnagle, Daniel G},
  year = {2013},
  journal = {Psychological bulletin},
  volume = {139},
  number = {4},
  pages = {792},
  publisher = {American Psychological Association}
}

@incollection{thompson-1988-discourse,
  title = {A Discourse Approach to the Cross-Linguistic Category `{{Adjective}}'},
  booktitle = {Linguistic {{Categorization}}: {{Proceedings}} of an {{International Symposium}} in {{Milwaukee}}, {{Wisconsin}}, {{April}} 10--11, 1987},
  author = {Thompson, S.},
  editor = {Corrigan, Roberta and Eckman, Fred and Noonan, Michael},
  year = {1988},
  pages = {245--265},
  publisher = {John Benjamins Publishing Company},
  url = {https://www.degruyterbrill.com/document/doi/10.1075/cilt.61.16tho/pdf?srsltid=AfmBOooKCoLbtL8RY53xPdJuFnDSLYasdYxnQoubBYSt3mN1y-4FT-Rd},
  urldate = {2025-09-06},
  isbn = {978-90-272-7852-4},
  langid = {english}
}
@article{morita-2010-internal,
  title = {The {{Internal Structures}} of {{Adjectives}} in {{Japanese}}},
  author = {Morita, Chigusa},
  year = {2010},
  journal = {Linguistic research: working papers in English linguistics},
  volume = {26},
  pages = {105--117}
}
@article{oshima-et-al-2019-gradability,
  title = {Gradability, Scale Structure, and the Division of Labor between Nouns and Adjectives: {{The}} Case of {{Japanese}}},
  shorttitle = {Gradability, Scale Structure, and the Division of Labor between Nouns and Adjectives},
  author = {Oshima, David Y. and Akita, Kimi and Sano, Shin-ichiro},
  year = {2019},
  month = mar,
  journal = {Glossa: a journal of general linguistics},
  volume = {4},
  number = {1},
  publisher = {Open Library of Humanities},
  issn = {2397-1835},
  doi = {10.5334/gjgl.737},
  urldate = {2025-09-12},
  abstract = {Japanese has three major ``adjective-like'' word classes, which roughly correspond to ``adjectives'', ``adjectival nouns'', and ``precopular nouns'' in Martin's (1975) A Reference Grammar of Japanese. This work explores how the three classes contrast semantically, paying special attention to the notion of gradability. Their scale-structural characteristics, in comparison with the English adjective class, will be examined, aiming to contribute to a better understanding of how languages may contrast in terms of (i) how different kinds of stative predicates divide the labor in encoding different kinds of state concepts, and (ii) how the niche of their noun class (as a major part-of-speech) is delimited. The major findings include (i) that ``adjectives'' and ``adjectival nouns'' have a strong tendency to encode relative gradable concepts, (ii) that ``precopular nouns'' tend to be nongradable, and (iii) none of the three Japanese classes is closely tied to the feature of absolute gradability.},
  copyright = {Copyright: {\copyright} 2019 The Author(s). This is an open-access article distributed under the terms of the Creative Commons Attribution 4.0 International License (CC-BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. See http://creativecommons.org/licenses/by/4.0/.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/4ZL7PDEA/Oshima et al. - 2019 - Gradability, scale structure, and the division of labor between nouns and adjectives The case of Ja.pdf}
}



@article{thompson-et-al-2007-statistical,
  title = {Statistical Learning of Syntax: {{The}} Role of Transitional Probability},
  author = {Thompson, Susan P and Newport, Elissa L},
  year = {2007},
  journal = {Language learning and development},
  volume = {3},
  number = {1},
  pages = {1--42},
  publisher = {Taylor \& Francis}
}

@phdthesis{uehara-1995-syntactic,
  title = {Syntactic Categories in {{Japanese}}: {{A}} Typological and Cognitive Introduction},
  shorttitle = {Syntactic Categories in {{Japanese}}},
  author = {Uehara, Satoshi},
  year = {1995},
  address = {United States -- Michigan},
  url = {https://www.proquest.com/docview/304224225/abstract/8E9037B9963C4950PQ/1},
  urldate = {2025-08-09},
  abstract = {This dissertation examines the organization of syntactic categories in Japanese and provides a typologically valid basis for cognitive linguistic analyses of the language. Three fundamental problems in syntactic category analyses of Japanese in the past are identified in Chapter 1. The chapter then briefly outlines recent discoveries on the nature of "category" in cognitive linguistics; specifically "prototype" semantic theory (Lakoff 1987, Taylor 1990), discusses central notions of Cognitive Grammar developed by Langacker (1987, 1991), and summarizes Croft's (1991) theory for the universal definition of major syntactic categories. Chap. 2 examines structural criteria used in previous analyses for identification of the five so-called "major" categories in Japanese, clarifies points of disagreement, re-evaluates each criterion in light of Croft's model, and argues that the crucial notion characterizing the structural aspects of the categories in Japanese is a language-specific property of morphological boundness. This boundness criterion not only serves as a basis for the definition of inflection in Japanese, but divides syntactic categories in Japanese into two major classes, each of which is unmarked, or "designed" for one of the two primary pragmatic functions of predication and reference. Two "unique" categories of Japanese, Nominal Adjectives and Verbal Nouns are examined in Chap. 3 and 4 respectively, and cognitive semantic analyses of the two problematic categories are presented. Specifically, it is argued that the grammatical behavior that the two categories exhibit is well motivated by their meaning--meaning as "conceptualization." Evidence is provided for the fundamental assumptions necessarily underlying syntactic category analyses of the language: prototype organization, the syntax-lexicon continuum, and the conceptualization process. Chap. 5 examines a functional motivation of the cardinal structural feature of boundness in Japanese, presents the results of a lexical semantic survey, and argues that the boundness distinction is well motivated by the semantic distinction of relationality, although this correlation has been obscured to some extent in contemporary Japanese. A diachronic account for the form/meaning mismatch is offered in which a semantic shift that occurred to Nouns, which led to the birth of Nominal Adjectives, was not fully accompanied by the shift in morphological boundness.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9798209363859},
  langid = {english},
  school = {University of Michigan},
  keywords = {Education,Language,Language arts,Language literature and linguistics,Linguistics,Modern language},
  file = {/Users/coleman/Zotero/storage/USIE2SAM/Uehara - 1995 - Syntactic categories in Japanese A typological and cognitive introduction.pdf}
}

@incollection{uehara-2009-diachronic,
  title = {A Diachronic Perspective on Prototypicality: {{The}} Case of Nominal Adjectives in {{Japanese}}},
  shorttitle = {A Diachronic Perspective on Prototypicality},
  booktitle = {Cognitive {{Approaches}} to {{Lexical Semantics}}},
  author = {Uehara, Satoshi},
  editor = {Cuyckens, Hubert and Dirven, Ren{\'e} and Taylor, John R.},
  year = {2009},
  month = jun,
  pages = {363--392},
  publisher = {De Gruyter Mouton},
  url = {https://www.degruyterbrill.com/document/doi/10.1515/9783110219074.363/html?srsltid=AfmBOooPHTfSqJNvd9V3dJusey2KXqMnw4-gfuh6FT2FbhWxlCDuBLFR},
  urldate = {2025-08-09},
  isbn = {978-3-11-021907-4},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/TPMZMN62/Uehara - 2009 - A diachronic perspective on prototypicality The case of nominal adjectives in Japanese.pdf}
}


@article{verkerk-et-al-2008-encoding,
  title = {The Encoding of Adjectives},
  author = {Verkerk, Annemarie and Lestrade, Sander},
  year = {2008},
  month = jan,
  journal = {Linguistics in the Netherlands},
  volume = {25},
  number = {1},
  pages = {157--168},
  publisher = {John Benjamins},
  issn = {0929-7332, 1569-9919},
  doi = {10.1075/avt.25.17ver},
  urldate = {2025-09-05},
  abstract = {Welcome to e-content platform of John Benjamins Publishing Company. Here you can find all of our electronic books and journals, for purchase and download or subscriber access.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/JRBUXZZY/Verkerk and Lestrade - 2008 - The encoding of adjectives.pdf;/Users/coleman/Zotero/storage/BTD6PGFJ/avt.25.html}
}

@misc{vilca-et-al-2012-analizador,
  title = {Analizador Morf{\'o}logico de La Lengua {{Quechua}} Basado En Software Libre {{Helsinkifinite-statetransducer}} ({{HFST}})},
  author = {Vilca, Hugo David Calderon and Mari{\~n}{\'o}, Flor Cagniy C{\'a}rdenas and Calderon, Edwin Fredy Mamani},
  year = {2012},
  publisher = {COMTEL}
}

@book{vogel-et-al-2011-approaches,
  title = {Approaches to the Typology of Word Classes},
  author = {Vogel, Petra M and Comrie, Bernard},
  year = {2011},
  volume = {23},
  publisher = {Walter de Gruyter}
}

@article{vulic-et-al-2020-multisimlex,
  title = {Multi-{{SimLex}}: A Large-Scale Evaluation of Multilingual and Crosslingual Lexical Semantic Similarity},
  author = {Vuli{\'c}, Ivan and Baker, Simon and Ponti, Edoardo Maria and Petti, Ulla and Leviant, Ira and Wing, Kelly and Majewska, Olga and Bar, Eden and Malone, Matt and Poibeau, Thierry and Reichart, Roi and Korhonen, Anna},
  year = {2020},
  month = dec,
  journal = {Computational Linguistics},
  volume = {46},
  number = {4},
  pages = {847--897},
  doi = {10.1162/coli_a_00391},
  abstract = {We introduce Multi-SimLex, a large-scale lexical resource and evaluation benchmark covering data sets for 12 typologically diverse languages, including major languages (e.g., Mandarin Chinese, Spanish, Russian) as well as less-resourced ones (e.g., Welsh, Kiswahili). Each language data set is annotated for the lexical relation of semantic similarity and contains 1,888 semantically aligned concept pairs, providing a representative coverage of word classes (nouns, verbs, adjectives, adverbs), frequency ranks, similarity intervals, lexical fields, and concreteness levels. Additionally, owing to the alignment of concepts across languages, we provide a suite of 66 crosslingual semantic similarity data sets. Because of its extensive size and language coverage, Multi-SimLex provides entirely novel opportunities for experimental evaluation and analysis. On its monolingual and crosslingual benchmarks, we evaluate and analyze a wide array of recent state-of-the-art monolingual and crosslingual representation models, including static and contextualized word embeddings (such as fastText, monolingual and multilingual BERT, XLM), externally informed lexical representations, as well as fully unsupervised and (weakly) supervised crosslingual word embeddings. We also present a step-by-step data set creation protocol for creating consistent, Multi-Simlex--style resources for additional languages. We make these contributions---the public release of Multi-SimLex data sets, their creation protocol, strong baseline results, and in-depth analyses which can be helpful in guiding future developments in multilingual lexical semantics and representation learning---available via a Web site that will encourage community effort in further expansion of Multi-Simlex to many more languages. Such a large-scale semantic resource could inspire significant further advances in NLP across languages.}
}

@inproceedings{wartena-2013-distributional,
  title = {Distributional Similarity of Words with Different Frequencies},
  booktitle = {Proceedings of the 13th Edition of the {{Dutch-Belgian}} Information Retrieval {{Workshop}} ({{DIR}} 2013)},
  author = {Wartena, Christian},
  year = {2013},
  pages = {8--11},
  publisher = {Hochschule Hannover}
}

@phdthesis{weber-1983-grammar,
  title = {A {{Grammar}} of {{Huallaga}} (Huanuco) {{Quechua}}.},
  author = {Weber, David John},
  year = {1983},
  address = {United States -- California},
  url = {https://www.proquest.com/docview/303135208/abstract/B0302B76EE6B4E87PQ/1},
  urldate = {2024-10-07},
  abstract = {This is a reference grammar of Huallaga (Huanuco) Quechua, an American Indian language spoken in central Peru. After (1) a general introduction and (2) an introduction to HgQ syntax, it contains chapters of the following topics: on word and suffix classes for (3) verbs, (4) substantives, (5) adverbs, and (6) other classes; on morphology: (7) word formation generally, (8) the "transitions," i.e., the complex which indicates the person of the subject and object, and (9) the suffixes which occur between the root and the transition; on grammatical relations: (10) case markers (11) and passives; (12) on substantive phrases; (13) on relative clauses and complements; (14) on adverbial clauses; (15) on reduplication; (16) on question formation; (17) on negation; (18) on conjunction; on the post-transition suffixes: (19) the "shading" suffixes (-lla, -pis, -na, and -raq), (20) the (so-called) "topic" marker -qa, and (21) the evidential suffixes (-mi, -shi and -chi); (22) on idiomatic and formulaic expressions; and (23) on phonology and loan processes.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9798403416092},
  langid = {english},
  school = {University of California, Los Angeles},
  keywords = {Language literature and linguistics,Linguistics},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/WEBER_A Grammar of Huallaga (huanuco) Quechua.pdf;/Users/coleman/Zotero/storage/Y2NPCQLC/WEBER_A Grammar of Huallaga (huanuco) Quechua.pdf}
}

@incollection{wetzer-2013-nouny,
  title = {``{{Nouny}}'' and ``Verby'' Adjectivale: A Typology of Predicative Adjectival Constructions},
  shorttitle = {``{{Nouny}}'' and ``Verby'' Adjectivale},
  booktitle = {Meaning and {{Grammar}}: {{Cross-Linguistic Perspectives}}},
  author = {Wetzer, Harrie},
  editor = {Kefer, Michel and van der Auwera, Johan},
  year = {2013},
  month = jul,
  pages = {223--262},
  publisher = {De Gruyter Mouton},
  url = {https://www.degruyterbrill.com/document/doi/10.1515/9783110851656.223/html},
  urldate = {2025-08-09},
  isbn = {978-3-11-085165-6},
  langid = {english},
  keywords = {adjectives,cognitive,continuum,thesis},
  file = {/Users/coleman/Zotero/storage/CEJCW2N5/Wetzer - 2013 - Nouny and verby adjectivale a typology of predicative adjectival constructions.pdf}
}

@book{wetzer-2013-typology,
  title = {The {{Typology}} of {{Adjectival Predication}}},
  author = {Wetzer, Harrie},
  year = {2013},
  month = mar,
  publisher = {De Gruyter Mouton},
  doi = {10.1515/9783110813586},
  urldate = {2025-08-09},
  abstract = {The Typology of Adjectival Predication by Harrie Wetzer was published on March 1, 2013 by De Gruyter Mouton.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  isbn = {978-3-11-081358-6},
  langid = {english},
  keywords = {adjectives,Adjektiv,cognitive,Kontrastive Grammatik},
  file = {/Users/coleman/Zotero/storage/CI6JG93N/Wetzer - 2013 - The Typology of Adjectival Predication.pdf}
}

@book{wiltschko-2014-universal,
  title = {The {{Universal Structure}} of {{Categories}}},
  author = {Wiltschko, Martina},
  year = {2014},
  series = {Cambridge {{Studies}} in {{Linguistics}}},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9781139833899},
  abstract = {Using data from a variety of languages such as Blackfoot, Halkomelem, and Upper Austrian German, this book explores a range of grammatical categories and constructions, including tense, aspect, subjunctive, case and demonstratives. It presents a new theory of grammatical categories - the Universal Spine Hypothesis - and reinforces generative notions of Universal Grammar while accommodating insights from linguistic typology. In essence, this new theory shows that language-specific categories are built from a small set of universal categories and language-specific units of language. Throughout the book the Universal Spine Hypothesis is compared to two alternative theories - the Universal Base Hypothesis and the No Base Hypothesis. This valuable addition to the field will be welcomed by graduate students and researchers in linguistics.},
  isbn = {978-1-107-03851-6}
}

@inproceedings{wu-et-al-2023-composition,
  title = {Composition and {{Deformance}}: {{Measuring Imageability}} with a {{Text-to-Image Model}}},
  shorttitle = {Composition and {{Deformance}}},
  booktitle = {Proceedings of the 5th {{Workshop}} on {{Narrative Understanding}}},
  author = {Wu, Si and Smith, David},
  editor = {Akoury, Nader and Clark, Elizabeth and Iyyer, Mohit and Chaturvedi, Snigdha and Brahman, Faeze and Chandu, Khyathi},
  year = {2023},
  month = jul,
  pages = {106--117},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.wnu-1.16},
  urldate = {2024-07-23},
  abstract = {Although psycholinguists and psychologists have long studied the tendency of linguistic strings to evoke mental images in hearers or readers, most computational studies have applied this concept of imageability only to isolated words. Using recent developments in text-to-image generation models, such as DALLE mini, we propose computational methods that use generated images to measure the imageability of both single English words and connected text. We sample text prompts for image generation from three corpora: human-generated image captions, news article sentences, and poem lines. We subject these prompts to different deformances to examine the model's ability to detect changes in imageability caused by compositional change. We find high correlation between the proposed computational measures of imageability and human judgments of individual words. We also find the proposed measures more consistently respond to changes in compositionality than baseline approaches. We discuss possible effects of model training and implications for the study of compositionality in text-to-image models.},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Wu_Smith_2023_Composition and Deformance.pdf;/Users/coleman/Zotero/storage/CI3UWIWZ/Wu_Smith_2023_Composition and Deformance.pdf}
}

@misc{ye-et-al-2024-computer,
  title = {Computer Vision Datasets and Models Exhibit Cultural and Linguistic Diversity in Perception},
  author = {Ye, Andre and Santy, Sebastin and Hwang, Jena D. and Zhang, Amy X. and Krishna, Ranjay},
  year = {2024},
  eprint = {2310.14356},
  primaryclass = {cs.CV},
  url = {https://arxiv.org/abs/2310.14356},
  archiveprefix = {arXiv}
}

@inproceedings{zhai-et-al-2023-sigmoid,
  title = {Sigmoid Loss for Language Image Pre-Training},
  booktitle = {2023 {{IEEE}}/{{CVF}} International Conference on Computer Vision ({{ICCV}})},
  author = {Zhai, X. and Mustafa, B. and Kolesnikov, A. and Beyer, L.},
  year = {2023},
  month = oct,
  pages = {11941--11952},
  publisher = {IEEE Computer Society},
  address = {Los Alamitos, CA, USA},
  doi = {10.1109/ICCV51070.2023.01100},
  abstract = {We propose a simple pairwise sigmoid loss for imagetext pre-training. Unlike standard contrastive learning with softmax normalization, the sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. With only four TPUv4 chips, we can train a Base CLIP model at 4k batch size and a Large LiT model at 20k batch size, the latter achieves 84.5},
  keywords = {computer vision,memory management,robustness,self-supervised learning,standards}
}

%%% OLD, TRY TO DELETE %%%
% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@inbook{ackemaneeleman2019,
  title     = {Default person versus default number in agreement},
  abstract  = {In this paper, we compare the behaviour of the default in the person system (third person) withthe default in the number system (singular). We argue, following Nevins (2007; 2011), thatthird person pronouns have person features, while singular DPs lack number features. Theevidence for these claims comes from situations in which a single head agrees with multiple DPs that have contrasting person and number specifications. In case the number of morphological slots in which agreement can be realized is lower than the number of agreement relations established in syntax, such contrasting specification may prove problematic. As it turns out, conflicts between singular and plural do not result in ungrammaticality, but conflicts between third person and first or second person do. Such person clashes can be avoided if the morphological realization of the relevant person features is syncretic. Alternatively, languages may make use of a person hierarchy that regulates the morphological realization of conflicting specifications for person. The argument we present is rooted in, and supports, the theory of person developed in Ackema & Neeleman (2013; to appear).},
  keywords  = {person, number, default, agreement, person hierarchy},
  author    = {Peter Ackema and Ad Neeleman},
  year      = {2019},
  doi       = {10.5281/zenodo.3458062},
  language  = {English},
  isbn      = {9783961102013},
  series    = {Open Generative Syntax},
  publisher = {Language Science Press},
  pages     = {21--54},
  booktitle = {Agreement, Case and Locality in the Nominal and Verbal Domains}
}
@article{anderson-1982-wheres,
  author  = {Anderson, Stephen R.},
  journal = {Linguistic Inquiry},
  pages   = {571612},
  title   = {Wheres morphology?},
  volume  = {13},
  year    = {1982}
}
@misc{arppe2019finite,
  author  = {Arppe, Antti and Harrigan, Atticus and Schmirler, Katherine and Antonsen, Lene and Trosterud, Trond and N{\o}rsteb{\o} Moshagen, Sjur and Silfverberg, Miikka and Wolvengrey, Arok and Snoek, Conor and Lachler, Jordan and Santos, Eddie Antonio and Okim{\=a}sis, Jean and Thunder, Dorothy},
  url     = {https://giellalt.uit.no/lang/crk/PlainsCreeDocumentation.html},
  title   = {Finite-State Transducer-Based Computational Model of {Plains Cree} Morphology},
  year    = {2014--2019},
  urldate = {2020-11-02}
}
@inproceedings{babazhanova,
  title     = {Geometric Probing of Word Vectors},
  author    = {Babazhanova, Madina and Tezekbayev, Maxat and Assylbekov, Zhenisbek},
  booktitle = {ESANN 2021 Proceedings - 29th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
  month     = oct # {{--}} # jun,
  year      = {2021},
  address   = {Virtual, Online, Belgium},
  publisher = {i6doc.com publication},
  doi       = {10.14428/esann/2021.ES2021-105},
  pages     = {587--592}
}

@incollection{backhouse-inflected-2004,
  title     = {Inflected and {{Uninflected Adjectives}} in {{Japanese}}},
  booktitle = {Adjective Classes: {A} Cross-Linguistic Typology},
  author    = {Backhouse, Anthony E},
  editor    = {Dixon, R M W and Aikhenvald, Alexandra Y},
  year      = {2004},
  month     = sep,
  pages     = {50--73},
  publisher = {Oxford University PressOxford},
  doi       = {10.1093/oso/9780199270934.003.0002},
  urldate   = {2024-10-07},
  abstract  = {Abstract             This chapter deals with adjectives in Japanese. Typologically, Japanese is a dependent-marking language; typical constituent order in the clause is predicate-final, and modifiers precede heads. Nouns function as the head of NPs commonly followed by case markers such as ga (NOM) and o (Ace), as modifier of nouns in NPs followed by the adnominal marker no, as complement of the copula da, and as complement of other copular verbs (such as naru `become') followed by the marker ni. Verbs function as the head of intransitive and transitive predicates, and directly precede NPs in modifying structures. Unlike nouns, verbs and the copula da arc inflected, largely on an agglutinating pattern. Lexically, Japanese has dearly delineated strata. The Sino and foreign strata arc the result of borrowing from classical Chinese and (chiefly) European languages respectively; Sino words are, at least diachronically, typically bimorphemic. In addition, mimetic items form a distinct stratum within the native vocabulary.},
  isbn      = {978-0-19-927093-4 978-1-383-04146-0},
  langid    = {english}
}
@article{bauer2004,
  title   = {The function of word-formation and the inflection-derivation distinction},
  author  = {Bauer, Laurie},
  journal = {Words and their Places. A Festschrift for J. Lachlan Mackenzie. Amsterdam: Vrije Universiteit},
  pages   = {283--292},
  year    = {2004}
}
@article{Beard1982,
  title   = {The plural as a lexical derivation},
  author  = {Beard, Robert},
  journal = {Glossa},
  volume  = {16},
  number  = {2},
  pages   = {133--148},
  year    = {1982}
}
@article{benjamini-control-2001,
  title      = {The {{Control}} of the {{False Discovery Rate}} in {{Multiple Testing}} under {{Dependency}}},
  author     = {Benjamini, Yoav and Yekutieli, Daniel},
  year       = {2001},
  journal    = {The Annals of Statistics},
  volume     = {29},
  number     = {4},
  eprint     = {2674075},
  eprinttype = {jstor},
  pages      = {1165--1188},
  publisher  = {Institute of Mathematical Statistics},
  issn       = {0090-5364},
  urldate    = {2024-10-14},
  abstract   = {Benjamini and Hochberg suggest that the false discovery rate may be the appropriate error rate to control in many applied multiple testing problems. A simple procedure was given there as an FDR controlling procedure for independent test statistics and was shown to be much more powerful than comparable procedures which control the traditional familywise error rate. We prove that this same procedure also controls the false discovery rate when the test statistics have positive regression dependency on each of the test statistics corresponding to the true null hypotheses. This condition for positive dependency is general enough to cover many problems of practical interest, including the comparisons of many treatments with a single control, multivariate normal test statistics with positive correlation matrix and multivariate t. Furthermore, the test statistics may be discrete, and the tested hypotheses composite without posing special difficulties. For all other forms of dependency, a simple conservative modification of the procedure controls the false discovery rate. Thus the range of problems for which a procedure with proven FDR control can be offered is greatly increased.},
  file       = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Benjamini_Yekutieli_2001_The Control of the False Discovery Rate in Multiple Testing under Dependency.pdf}
}
@misc{berger2024crosslingualcrossculturalvariationimage,
  title         = {Cross-Lingual and Cross-Cultural Variation in Image Descriptions},
  author        = {Uri Berger and Edoardo M. Ponti},
  year          = {2024},
  eprint        = {2409.16646},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2409.16646}
}

@inproceedings{bergmanis_segmentation_2017,
  author    = {Bergmanis, Toms and Goldwater, Sharon},
  title     = {From segmentation to analyses: a probabilistic model for unsupervised morphology induction},
  booktitle = {Proceedings of EACL},
  year      = 2017,
  address   = {Valencia, Spain}
}

@inproceedings{bert,
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author    = {Devlin, Jacob  and
               Chang, Ming-Wei  and
               Lee, Kenton  and
               Toutanova, Kristina},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = jun,
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N19-1423},
  doi       = {10.18653/v1/N19-1423},
  pages     = {4171--4186},
  abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}
@inproceedings{bertsyntax1,
  title     = {{A} Structural Probe for Finding Syntax in Word Representations},
  author    = {Hewitt, John  and
               Manning, Christopher D.},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = jun,
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N19-1419},
  doi       = {10.18653/v1/N19-1419},
  pages     = {4129--4138},
  abstract  = {Recent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network{'}s word representation space. The probe identifies a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models{'} vector geometry.}
}
@misc{beyer2024paligemmaversatile3bvlm,
  title  = {{PaliGemma}: A versatile 3B {VLM} for transfer},
  author = {Lucas Beyer and Andreas Steiner and Andr Susano Pinto and Alexander Kolesnikov and Xiao Wang and Daniel Salz and Maxim Neumann and Ibrahim Alabdulmohsin and Michael Tschannen and Emanuele Bugliarello and Thomas Unterthiner and Daniel Keysers and Skanda Koppula and Fangyu Liu and Adam Grycner and Alexey Gritsenko and Neil Houlsby and Manoj Kumar and Keran Rong and Julian Eisenschlos and Rishabh Kabra and Matthias Bauer and Matko Bonjak and Xi Chen and Matthias Minderer and Paul Voigtlaender and Ioana Bica and Ivana Balazevic and Joan Puigcerver and Pinelopi Papalampidi and Olivier Henaff and Xi Xiong and Radu Soricut and Jeremiah Harmsen and Xiaohua Zhai},
  year   = {2024}
}
@article{bird-verbs-2003,
  title    = {Verbs and Nouns: The Importance of Being Imageable},
  author   = {Bird, Helen and Howard, David and Franklin, Sue},
  year     = {2003},
  month    = mar,
  journal  = {Journal of Neurolinguistics},
  volume   = {16},
  number   = {2},
  pages    = {113--149},
  issn     = {0911-6044},
  doi      = {10.1016/S0911-6044(02)00016-7},
  abstract = {There are many differences between verbs and nouns---semantic, syntactic and phonological. We focus on the semantic distinctions and examine differences in performance in both normal control subjects and individuals with aphasia. In tasks requiring production of particular semantic categories and categorisation of given verbs and nouns, control subjects produced fewer verbs than nouns and were slower and less accurate in verb categorisation. Patients who had shown a verb deficit in naming also had particular difficulties producing both verbs and nouns of relatively low imageability. In reading and writing, some patients exhibited poorer performance with verbs than nouns, even when verb/noun homonyms were used. When imageability was controlled, however, no dissociation was shown. We conclude that in simple single word tasks imageability must be controlled to eliminate this as a factor in apparent verb deficits. Other semantic factors, however, could affect performance, particularly when tasks involve the relationships between category exemplars.},
  keywords = {Aphasia,Imageability,Nouns,Reading,Semantics,Verbs,Writing}
}
@incollection{bisang-grammaticalization-2017,
  title        = {Grammaticalization},
  booktitle    = {Oxford {{Research Encyclopedia}} of {{Linguistics}}},
  author       = {Bisang, Walter},
  year         = {2017},
  month        = mar,
  publisher    = {Oxford University Press},
  doi          = {10.1093/acrefore/9780199384655.013.103},
  urldate      = {2024-10-07},
  abstract     = {Linguistic change not only affects the lexicon and the phonology of words, it also operates on the grammar of a language. In this context, grammaticalization is concerned with the development of lexical items into markers of grammatical categories or, more generally, with the development of markers used for procedural cueing of abstract relationships out of linguistic items with concrete referential meaning. A well-known example is the English verb               go               in its function of a future marker, as in               She is going to visit her friend               . Phenomena like these are very frequent across the world's languages and across many different domains of grammatical categories. In the last 50 years, research on grammaticalization has come up with a plethora of (a) generalizations, (b) models of how grammaticalization works, and (c) methodological refinements.                          On (a): Processes of grammaticalization develop gradually, step by step, and the sequence of the individual stages follows certain clines as they have been generalized from cross-linguistic comparison (unidirectionality). Even though there are counterexamples that go against the directionality of various clines, their number seems smaller than assumed in the late 1990s.             On (b): Models or scenarios of grammaticalization integrate various factors. Depending on the theoretical background, grammaticalization and its results are motivated either by the competing motivations of economy vs. iconicity/explicitness in functional typology or by a change from movement to merger in the minimalist program. Pragmatic inference is of central importance for initiating processes of grammaticalization (and maybe also at later stages), and it activates mechanisms like reanalysis and analogy, whose status is controversial in the literature. Finally, grammaticalization does not only work within individual languages/varieties, it also operates across languages. In situations of contact, the existence of a certain grammatical category may induce grammaticalization in another language.             On (c): Even though it is hard to measure degrees of grammaticalization in terms of absolute and exact figures, it is possible to determine relative degrees of grammaticalization in terms of the autonomy of linguistic signs. Moreover, more recent research has come up with criteria for distinguishing grammaticalization and lexicalization (defined as the loss of productivity, transparency, and/or compositionality of former productive, transparent, and compositional structures).             In spite of these findings, there are still quite a number of questions that need further research. Two questions to be discussed address basic issues concerning the overall properties of grammaticalization. (1) What is the relation between constructions and grammaticalization? In the more traditional view, constructions are seen as the syntactic framework within which linguistic items are grammaticalized. In more recent approaches based on construction grammar, constructions are defined as combinations of form and meaning. Thus, grammaticalization can be seen in the light of constructionalization, i.e., the creation of new combinations of form and meaning. Even though constructionalization covers many apects of grammaticalization, it does not exhaustively cover the domain of grammaticalization. (2) Is grammaticalization cross-linguistically homogeneous, or is there a certain range of variation? There is evidence from East and mainland Southeast Asia that there is cross-linguistic variation to some extent.},
  collaborator = {Bisang, Walter},
  isbn         = {978-0-19-938465-5},
  langid       = {english}
}
@incollection{bisang-word-2010,
  title     = {Word {{Classes}}},
  booktitle = {The {{Oxford Handbook}} of {{Linguistic Typology}}},
  author    = {Bisang, Walter},
  editor    = {Song, Jae Jung},
  year      = {2010},
  month     = nov,
  pages     = {0},
  publisher = {Oxford University Press},
  doi       = {10.1093/oxfordhb/9780199281251.013.0015},
  urldate   = {2024-05-15},
  abstract  = {This article introduces the four prerequisites for distinguishing word classes: semantic criteria; pragmatic criteria/criteria of discourse function; formal criteria; and distinction between lexical and syntactic levels of analysis. The most important approaches to word classes based on the first three prerequisites are addressed. The article also deals with the distinction between content words and function words. It then takes up the discussion of the universal status of the noun/verb distinction by integrating the fourth prerequisite. The languages discussed are Classical Nahuatl, Late Archaic Chinese, and Tongan. The distinction between content words and function words is not identical to the distinction between open and closed word classes. The article reviews Dixon's seminal approach to adjectives. The sub-classes of adverbs are considered. The definition of word classes integrates all the central elements that make language structure, and it integrates a whole paradigm of constructions.},
  isbn      = {978-0-19-928125-1}
}

@book{Blake2001,
  title     = {Case},
  author    = {Blake, Barry J},
  year      = {2001},
  publisher = {Cambridge University Press}
}
@article{bonami,
  author  = {Olivier Bonami and Denis Paperno},
  year    = {2018},
  title   = {Inflection vs. derivation in a distributional vector space},
  journal = {Lingue e
             linguaggio},
  volume  = {17},
  number  = {2},
  pages   = {173--196}
}
@article{bonami_paradigm_2019,
  title    = {Paradigm structure and predictability in derivational morphology},
  volume   = {29},
  issn     = {1871-5656},
  url      = {https://doi.org/10.1007/s11525-018-9322-6},
  doi      = {10.1007/s11525-018-9322-6},
  abstract = {In this paper we address the usefulness of the notion of a paradigm in the context of derivational morphology. We first define a notion of paradigmatic system that extends conservatively the notion as it is used in inflection so as to be applicable to collections of structured families of derivationally-related words. We then build on this definition in an empirical quantitative study of derivational families of verbs in French. We apply information-theoretic measures of predictability initially designed by Ackerman et al. (2009) in the context of inflection. We conclude that key quantitative properties are common to inflectional and derivational paradigmatic systems, and hence that (partial) paradigms are an important ingredient of the study of derivation.},
  number   = {2},
  journal  = {Morphology},
  author   = {Bonami, Olivier and Strnadov, Jana},
  month    = may,
  year     = {2019},
  pages    = {167--197}
}
@incollection{booij-inflection-2007,
  title      = {Inflection},
  shorttitle = {The {{Grammar}} of {{Words}}},
  booktitle  = {The {{Grammar}} of {{Words}}: {{An Introduction}} to {{Linguistic Morphology}}},
  author     = {Booij, Geert},
  editor     = {Booij, Geert},
  year       = {2007},
  month      = jul,
  pages      = {99--124},
  publisher  = {Oxford University Press},
  doi        = {10.1093/acprof:oso/9780199226245.003.0005},
  urldate    = {2024-10-15},
  abstract   = {Inflection is the expression of morphosyntactic properties on words. Examples are case and number marking on nouns, and number and person marking on verbs. These properties play a role in computing the correct form of word in a sentence. Unlike derivation, inflectional processes do not create new words but forms of a word. There are different theoretical models for inflection: Word-and-Paradigm, Item-and-Arrangement, and Item-and-Process models.},
  isbn       = {978-0-19-922624-5},
  file       = {/Users/coleman/Zotero/storage/9GZ3YNIL/337167067.html}
}
@incollection{booij1996,
  title     = {Inherent versus contextual inflection and the split
               morphology hypothesis},
  author    = {Booij, Geert},
  editors   = {Booij, Geert and van Marle, Jaap},
  booktitle = {Yearbook of Morphology 1995},
  year      = {1996},
  pages     = {1-16},
  publisher = {Springer},
  location  = {Kluwer, Dordrecht}
}
@article{boschlootest,
  title     = {Raised conditional level of significance for the 2$\times$ 2-table when testing the equality of two probabilities},
  author    = {Boschloo, RD},
  journal   = {Statistica Neerlandica},
  volume    = {24},
  number    = {1},
  pages     = {1--9},
  year      = {1970},
  publisher = {Wiley Online Library}
}
@article{brysbaert-concreteness-2014,
  title    = {Concreteness Ratings for 40 Thousand Generally Known {{English}} Word Lemmas},
  author   = {Brysbaert, Marc and Warriner, Amy Beth and Kuperman, Victor},
  year     = {2014},
  month    = sep,
  journal  = {Behavior Research Methods},
  volume   = {46},
  number   = {3},
  pages    = {904--911},
  issn     = {1554-3528},
  doi      = {10.3758/s13428-013-0403-5},
  urldate  = {2024-10-15},
  abstract = {Concreteness ratings are presented for 37,058 English words and 2,896 two-word expressions (such as zebra crossing and zoom in), obtained from over 4,000 participants by means of a norming study using Internet crowdsourcing for data collection. Although the instructions stressed that the assessment of word concreteness would be based on experiences involving all senses and motor responses, a comparison with the existing concreteness norms indicates that participants, as before, largely focused on visual and haptic experiences. The reported data set is a subset of a comprehensive list of English lemmas and contains all lemmas known by at least 85~\% of the raters. It can be used in future research as a reference list of generally known English lemmas.},
  langid   = {english},
  keywords = {Concreteness,Crowdsourcing,Ratings,Word recognition},
  file     = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Brysbaert et al_2014_Concreteness ratings for 40 thousand generally known English word lemmas.pdf}
}

@incollection{brysbaert2024concreteness,
  title     = {Concreteness and Imageability Norms},
  booktitle = {International Encyclopedia of Language and Linguistics},
  author    = {Brysbaert, Marc},
  year      = {to appear},
  edition   = {3rd},
  publisher = {Elsevier}
}

@book{bybee-1985-morphology,
  address   = {Amsterdam},
  author    = {Bybee, Joan L},
  key       = {Bybee 1985},
  publisher = {John Benjamins},
  title     = {Morphology: A study of the relation between meaning and form},
  year      = {1985}
}

@article{chiarello-imageability-1999,
  title    = {Imageability and Distributional Typicality Measures of Nouns and Verbs in Contemporary {{English}}},
  author   = {Chiarello, Christine and Shears, Connie and Lund, Kevin},
  year     = {1999},
  month    = dec,
  journal  = {Behavior Research Methods, Instruments, \& Computers},
  volume   = {31},
  number   = {4},
  pages    = {603--637},
  issn     = {1532-5970},
  doi      = {10.3758/BF03200739},
  abstract = {Dissociations between noun and verb processing are not uncommon after brain injury; yet, precise psycholinguistic comparisons of nouns and verbs are hampered by the underrepresentation of verbs in published semantic word norms and by the absence of contemporary estimates for part-of-speech usage. We report herein imageability ratings and rating response times (RTs) for 1,197 words previously categorized as pure nouns, pure verbs, or words of balanced noun-verb usage on the basis of the Francis and Ku{\v c}era (1982) norms. Nouns and verbs differed in rated imageability, and there was a stronger correspondence between imageability rating and RT for nouns than for verbs. For all word types, the image-rating-RT function implied that subjects employed an image generation process to assign ratings. We also report a new measure of noun-verbtypicality that used the Hyperspace Analog to Language (HAL; Lund \& Burgess, 1996) context vectors (derived from a large sample of Usenet text) to compute the mean context distance between each word and all of thepure nouns andpure verbs. For a subset of the items, the resulting HAL noun-verb difference score was compared with part-of-speech usage in a representative sample of the Usenet corpus. It is concluded that this score can be used to estimate the extent to which a given word occurs in typical noun or verb sentence contexts in informal contemporary English discourse. The item statistics given in Appendix B will enable experimenters to select representative examples of nouns and verbs or to compare typical with atypical nouns (or verbs), while holding constant or covarying rated imageability.}
}
@book{chomsky-1957-syntactic,
  title     = {Syntactic {{Structures}}},
  author    = {Chomsky, Noam},
  year      = {1957},
  publisher = {De Gruyter Mouton},
  doi       = {10.1515/9783112316009},
  urldate   = {2024-05-14},
  isbn      = {978-3-11-231600-9}
}
@book{colarusso-northwest-1988,
  title     = {The {{Northwest Caucasian Languages}}: {{A Phonological Survey}}},
  author    = {Colarusso, John},
  year      = {1988},
  edition   = {0},
  publisher = {Garland},
  location  = {New York},
  doi       = {10.4324/9781315852263},
  url       = {https://www.taylorfrancis.com/books/9781317918172},
  urldate   = {2024-10-07},
  isbn      = {978-1-317-91817-2},
  langid    = {english},
  pagetotal = {520}
}
@book{comrie-language-1988,
  title      = {Language Universals and Linguistic Typology},
  shorttitle = {Language Universals and Linguistic Typology},
  author     = {Comrie, Bernard},
  year       = {1988},
  edition    = {2nd},
  publisher  = {The University of Chicago Press},
  address    = {Chicago},
  isbn       = {978-0-226-11433-0},
  langid     = {english}
}
@inbook{ComriePolinsky1998,
  title        = {The Great Dagestanian Case Hoax},
  booktitle    = {Case, Typology, and Grammar},
  year         = {1998},
  pages        = {95-114},
  publisher    = {John Benjamins},
  organization = {John Benjamins},
  address      = {Amsterdam},
  author       = {Comrie, Bernard and Polinsky, Maria}
}
@article{connell-strength-2012,
  title    = {Strength of Perceptual Experience Predicts Word Processing Performance Better than Concreteness or Imageability},
  author   = {Connell, Louise and Lynott, Dermot},
  year     = {2012},
  month    = dec,
  journal  = {Cognition},
  volume   = {125},
  number   = {3},
  pages    = {452--465},
  issn     = {0010-0277},
  doi      = {10.1016/j.cognition.2012.07.010},
  abstract = {Abstract concepts are traditionally thought to differ from concrete concepts by their lack of perceptual information, which causes them to be processed more slowly and less accurately than perceptually-based concrete concepts. In two studies, we examined this assumption by comparing concreteness and imageability ratings to a set of perceptual strength norms in five separate modalities: sound, taste, touch, smell and vision. Results showed that concreteness and imageability do not reflect the perceptual basis of concepts: concreteness ratings appear to be based on two different intersecting decision criteria, while imageability ratings are visually biased. Analysis of lexical decision and word naming performance showed that maximum perceptual strength (i.e., strength in the dominant perceptual modality) consistently outperformed both concreteness and imageability ratings in accounting for variance in response latency and accuracy. We conclude that so-called concreteness effects in word processing emerge from the perceptual strength of a concept's representation and discuss the implications for theories of conceptual representation.},
  keywords = {Abstract and concrete concepts,Concreteness effects,Context availability,Dual coding,Imageability,Lexical decision,Perceptual strength,Situated simulation,Word naming}
}
@article{copot-et-al-2022-idiosyncratic
  title        = {Idiosyncratic frequency as a measure of derivation vs. inflection},
  volume       = {10},
  url          = {https://jlm.ipipan.waw.pl/index.php/JLM/article/view/301},
  doi          = {10.15398/jlm.v10i2.301},
  abstractnote = {&amp;lt;p&amp;gt;There is ongoing discussion about how to conceptualize the nature of the distinction between inflection and derivation. A common approach relies on qualitative differences in the semantic relationship between inflectionally versus derivationally related words: inflection yields ways to discuss the same concept in different syntactic contexts, while derivation gives rise to words for related concepts. This differential can be expected to manifest in the predictability of word frequency between words that are related derivationally or inflectionally: predicting the token frequency of a word based on information about its base form or about related words should be easier when the two words are in an inflectional relationship, rather than a derivational one. We compare prediction error magnitude for statistical models of token frequency based on distributional and frequency information of inflectionally or derivationally related words in French. The results conform to expectations: it is easier to predict the frequency of a word from properties of an inflectionally related word than from those of a derivationally related word. Prediction error provides a quantitative, continuous method to explore differences between individual processes and differences yielded by employing different predicting information, which in turn can be used to draw conclusions about the nature and manifestation of the inflectionderivation distinction.&amp;lt;/p&amp;gt;},
  number       = {2},
  journal      = {Journal of Language Modelling},
  author       = {Copot, Maria and Mickus, Timothee and Bonami, Olivier},
  year         = {2022},
  month        = {Dec.},
  pages        = {193240}
}
@article{corbett2010,
  title     = {Canonical derivational morphology},
  author    = {Corbett, Greville G},
  journal   = {Word structure},
  volume    = {3},
  number    = {2},
  pages     = {141--155},
  year      = {2010},
  publisher = {Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK}
}
@incollection{corver-semilexical-2001,
  title     = {Semi-Lexical Categories},
  booktitle = {Semi-Lexical {{Categories}}},
  author    = {Corver, Norbert and Riemsdijk, Henk Van},
  editor    = {Corver, Norbert and Riemsdijk, Henk Van},
  year      = {2001},
  month     = dec,
  pages     = {1--20},
  publisher = {de {G}ruyter},
  doi       = {10.1515/9783110874006.1},
  urldate   = {2024-10-07},
  isbn      = {978-3-11-016685-9}
}
@article{cotterell-et-al-2019-complexity,
  title     = {On the {{Complexity}} and {{Typology}} of {{Inflectional Morphological Systems}}},
  author    = {Cotterell, Ryan and Kirov, Christo and Hulden, Mans and Eisner, Jason},
  editor    = {Lee, Lillian and Johnson, Mark and Roark, Brian and Nenkova, Ani},
  year      = {2019},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {7},
  pages     = {327--342},
  publisher = {MIT Press},
  address   = {Cambridge, MA},
  doi       = {10.1162/tacl_a_00271},
  urldate   = {2025-05-14},
  abstract  = {We quantify the linguistic complexity of different languages' morphological systems. We verify that there is a statistically significant empirical trade-off between paradigm size and irregularity: A language`s inflectional paradigms may be either large in size or highly irregular, but never both. We define a new measure of paradigm irregularity based on the conditional entropy of the surface realization of a paradigm--- how hard it is to jointly predict all the word forms in a paradigm from the lemma. We estimate irregularity by training a predictive model. Our measurements are taken on large morphological paradigms from 36 typologically diverse languages.},
  file      = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Cotterell et al_2019_On the Complexity and Typology of Inflectional Morphological Systems.pdf}
}
@article{croft-2016,
  url         = {https://doi.org/10.1515/lingty-2016-0012},
  title       = {Comparative concepts and language-specific categories: Theory and practice},
  title       = {},
  author      = {William Croft},
  pages       = {377--393},
  volume      = {20},
  number      = {2},
  journal     = {Linguistic Typology},
  doi         = {doi:10.1515/lingty-2016-0012},
  year        = {2016},
  lastchecked = {2025-04-24}
}

@book{croft-2001-radical,
  title     = {Radical {{Construction Grammar}}: {{Syntactic Theory}} in {{Typological Perspective}}},
  author    = {Croft, William},
  year      = {2001},
  month     = oct,
  publisher = {Oxford University Press},
  doi       = {10.1093/acprof:oso/9780198299554.001.0001},
  urldate   = {2024-05-14},
  abstract  = {This book presents a profound critique of syntactic theory and syntactic argumentation. Recent syntactic theories are essentially formal models for the representation of grammatical knowledge. These theories posit complex syntactic structures in the analysis of sentences, consisting of atomic primitive syntactic categories and relations. The result of this approach to syntax has been an endless cycle of new and revised theories of syntactic representation. The book argues that these types of syntactic theories are incompatible with the grammatical variation found within and across languages. The extent of grammatical variation demonstrates that no scheme of atomic primitive syntactic categories and relations can form the basis of an empirically adequate syntactic theory. This book defends three theses: (i) constructions are the primitive units of syntactic representation, and grammatical categories are derivative; (ii) the only syntactic structures are the relations between a construction and the elements that make it up (that is, there is no need to posit syntactic relations); and (iii) constructions are language-specific. Constructions are complex units pairing form and meaning. Grammatical categories within and across languages are mapped onto a universal conceptual space, following the semantic map model in typology. The structure of conceptual space constrains how meaning is encoded in linguistic form, and reflects the structure of the human mind.},
  isbn      = {978-0-19-829955-4}
}
@book{croft-typology-2002,
  title     = {Typology and {{Universals}}},
  author    = {Croft, William},
  year      = {2002},
  month     = nov,
  edition   = {2nd},
  series     = {Cambridge Textbooks in Linguistics},
  publisher = {Cambridge University Press},
  doi       = {10.1017/CBO9780511840579},
  urldate   = {2024-10-07},
  abstract  = {Comparison of the grammars of human languages reveals systematic patterns of variation. Typology and universals research uncovers those patterns to formulate universal constraints on language and seek their exploration. In this essential textbook, William Croft presents a comprehensive introduction to the method and theory used in studying typology and universals. The theoretical issues discussed range from the most fundamental to the most abstract. The book provides students and researchers with extensive examples of language universals in phonology, morphology, syntax and semantics. This second edition has been thoroughly rewritten and updated to reflect advances in typology and universals in the past decade, including: new methodologies such as the semantic map model and questions of syntactic argumentation; discussion of current debates over deeper explanations for specific classes of universals; and comparison of the typological and generative approaches to language.},
  copyright = {https://www.cambridge.org/core/terms},
  isbn      = {978-0-521-00499-2 978-0-521-80884-2 978-0-511-84057-9}
}
@book{croft1991,
  title     = {Syntactic Categories and Grammatical Relations: {{The}} Cognitive Organization of Information},
  author    = {Croft, W.},
  year      = {1991},
  series    = {Emersion: {{Emergent}} Village Resources for Communities of Faith Series},
  publisher = {University of Chicago Press},
  isbn      = {978-0-226-12090-4},
  lccn      = {90038349}
}
@article{cutler-1981-degrees,
  title     = {Degrees of transparency in word formation},
  author    = {Cutler, Anne},
  journal   = {Canadian Journal of Linguistics/Revue canadienne de linguistique},
  volume    = {26},
  number    = {1},
  pages     = {73--77},
  year      = {1981},
  publisher = {Cambridge University Press}
}
@inproceedings{czech,
  title     = {Attempting to separate inflection and derivation using vector space representations},
  author    = {Rosa, Rudolf  and
               {\v{Z}}abokrtsk{\'y}, Zden{\v{e}}k},
  booktitle = {Proceedings of the Second International Workshop on Resources and Tools for Derivational Morphology},
  month     = sep,
  year      = {2019},
  address   = {Prague, Czechia},
  publisher = {Charles University, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics},
  url       = {https://aclanthology.org/W19-8508},
  pages     = {61--70}
}
@article{demarneffe-universal-2021,
  title    = {Universal {{Dependencies}}},
  author   = {{d}e Marneffe, Marie-Catherine and Manning, Christopher D. and Nivre, Joakim and Zeman, Daniel},
  year     = {2021},
  month    = may,
  journal  = {Computational Linguistics},
  volume   = {47},
  number   = {2},
  pages    = {255--308},
  issn     = {0891-2017, 1530-9312},
  doi      = {10.1162/coli_a_00402},
  urldate  = {2024-10-07},
  abstract = {Abstract             Universal dependencies (UD) is a framework for morphosyntactic annotation of human language, which to date has been used to create treebanks for more than 100 languages. In this article, we outline the linguistic theory of the UD framework, which draws on a long tradition of typologically oriented grammatical theories. Grammatical relations between words are centrally used to explain how predicate--argument structures are encoded morphosyntactically in different languages while morphological features and part-of-speech classes give the properties of words. We argue that this theory is a good basis for crosslinguistically consistent annotation of typologically diverse languages in a way that supports computational natural language understanding as well as broader linguistic studies.},
  langid   = {english},
  file     = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/De Marneffe et al_2021_Universal Dependencies.pdf;/Users/coleman/Zotero/storage/RK2MTTGY/De Marneffe et al. - 2021 - Universal Dependencies.pdf}
}
@article{haley-et-al-2024-corpusbased,
  title = {Corpus-Based Measures Discriminate Inflection and Derivation Cross-Linguistically},
  author = {Haley, Coleman and Ponti, Edoardo M. and Goldwater, Sharon},
  year = 2024,
  month = dec,
  journal = {Journal of Language Modelling},
  volume = {12},
  number = {2},
  pages = {477--529},
  issn = {2299-8470},
  doi = {10.15398/jlm.v12i2.351},
  urldate = {2025-10-23},
  abstract = {In morphology, a distinction is commonly drawn between inflection and derivation. However, a precise definition of this distinction which reflects the way it manifests across languages remains elusive within linguistic theory, typically being based on subjective tests. In this study, we present 4 quantitative measures which use the statistics of a raw text corpus in a language to estimate to what extent a given morphological construction changes the form and distribution of lexemes. In particular, we measure both the average and the variance of this change across lexemes. Crucially, distributional information captures syntactic and semantic properties and can be operationalised by word embeddings. Based on a sample of 26 languages, we find that we can reconstruct 89{\textpm}1\% of the classification of constructions into inflection and derivation in UniMorph using our 4 measures, providing large-scale cross-linguistic evidence that the concepts of inflection and derivation are associated with measurable signatures in terms of form and distribution that behave consistently across a variety of languages. We also use our measures to identify in a quantitative way whether categories of inflection which have been considered noncanonical in the linguistic literature, such as inherent inflection or transpositions, appear so in terms of properties of their form and distribution. We find that while combining multiple measures reduces the amount of overlap between inflectional and derivational constructions, there are still many constructions near the model's decision boundary between the two categories. This indicates a gradient, rather than categorical, distinction.},
  copyright = {Copyright (c) 2024 Coleman Haley, Edoardo M. Ponti, Sharon Goldwater},
  langid = {english},
  keywords = {derivation,distributional semantics,inflection,morphology,typology},
  file = {/Users/coleman/Zotero/storage/T36R5RMP/Haley et al. - 2024 - Corpus-based measures discriminate inflection and derivation cross-linguistically.pdf}
}

@article{distributionalsem,
  author  = {Zellig Harris},
  year    = {1954},
  title   = {Distributional structure},
  journal = {Word},
  volume  = {10},
  number  = {23},
  pages   = {146--162}
}
@article{dressler1989,
  title     = {Prototypical differences between inflection and derivation},
  author    = {Dressler, Wolfgang U},
  journal   = {STUF-Language Typology and Universals},
  volume    = {42},
  number    = {1},
  pages     = {3--10},
  year      = {1989},
  publisher = {De Gruyter (A)}
}
@incollection{dryer-are-1997,
  title     = {Are {{Grammatical Relations Universal}}?},
  booktitle = {Essays on {{Language Function}} and {{Language Type}}},
  author    = {Dryer, Matthew S.},
  editor    = {Bybee, Joan L. and Haiman, John and Thompson, Sandra A.},
  year      = {1997},
  pages     = {115},
  publisher = {John Benjamins Publishing Company},
  address   = {Amsterdam},
  doi       = {10.1075/z.82.09dry},
  urldate   = {2024-05-15},
  isbn      = {978-90-272-2168-1 978-1-55619-522-8 978-90-272-7421-2},
  langid    = {english}
}
@article{dryer1989,
  title     = {Large linguistic areas and language sampling},
  author    = {Dryer, Matthew S},
  journal   = {Studies in Language. International Journal sponsored by the Foundation Foundations of Language},
  volume    = {13},
  number    = {2},
  pages     = {257--292},
  year      = {1989},
  publisher = {John Benjamins}
}
@article{dube-independent-2014,
  title    = {Independent Effects of Imageability and Grammatical Class in Synonym Judgement in Aphasia.},
  author   = {Dub{\'e}, Catherine and Monetta, Laura and {Mart{\'i}nez-Cuiti{\~n}o}, Mar{\'i}a Macarena and Wilson, Maximiliano A.},
  year     = {2014},
  journal  = {Psicothema},
  volume   = {26},
  number   = {4},
  pages    = {449--456},
  address  = {Spain},
  issn     = {1886-144X 0214-9915},
  doi      = {10.7334/psicothema2014.31},
  abstract = {BACKGROUND: The grammatical class effect in aphasia, i.e. dissociated processing of words according to their respective grammatical class, has been attributed to  either grammatical, lexical or semantic (i.e., imageability) deficits. This study  explores the hypotheses of impaired semantic treatment as the source of the  grammatical class effect in aphasia. METHOD: A synonym judgement task that  includes nouns and verbs of high and low imageability has been administered to 30  Spanish-speaking patients suffering from receptive or productive aphasia and 30  controls. RESULTS: Normal controls performed significantly better than aphasic  patients. Although globally the productive aphasics performed significantly  better than the receptive aphasics, grammatical class (nouns better than verbs)  and imageability (high imageability better than low imageability) affected  performance in both subgroups. No significant interaction emerged between these  two factors. CONCLUSION: The results suggest that the grammatical class effect  may emerge from semantic impairment and that it is -at least partially-  independent of the imageability of words.},
  langid   = {english},
  pmid     = {25340890},
  keywords = {*Imagination,*Linguistics,*Vocabulary,Aphasia/*psychology,Argentina,Female,Humans,Male,Middle Aged}
}
@article{fasttext,
  title     = {Enriching Word Vectors with Subword Information},
  author    = {Bojanowski, Piotr  and
               Grave, Edouard  and
               Joulin, Armand  and
               Mikolov, Tomas},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {5},
  year      = {2017},
  address   = {Cambridge, MA},
  publisher = {MIT Press},
  url       = {https://aclanthology.org/Q17-1010},
  doi       = {10.1162/tacl\_a\_00051},
  pages     = {135--146},
  abstract  = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.}
}
@article{floyd-rediscovering-2011,
  title   = {Re-Discovering the {{Quechua}} Adjective},
  author  = {Floyd, Simeon},
  year    = {2011},
  month   = jan,
  journal = {Linguistic Typology},
  volume  = {15},
  number  = {1},
  issn    = {1430-0532, 1613-415X},
  doi     = {10.1515/lity.2011.003},
  pages   = {25--63},
  urldate = {2024-10-07},
  file    = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Floyd_2011_Re-discovering the Quechua adjective.pdf}
}
@incollection{function,
  title     = {The function of word-formation and the inflection-derivation distinction},
  author    = {Laurie Bauer},
  booktitle = {Words in their Places. A {F}estschrift for {J}. {L}achlan {M}ackenzie},
  publisher = {Vrije Universiteit},
  address   = {Amsterdam},
  year      = {2004},
  pages     = {283--292}
}

@article{futrell-2015-largescale,
  title     = {Large-Scale Evidence of Dependency Length Minimization in 37 Languages},
  author    = {Futrell, Richard and Mahowald, Kyle and Gibson, Edward},
  year      = {2015},
  month     = aug,
  journal   = {Proceedings of the National Academy of Sciences},
  volume    = {112},
  number    = {33},
  pages     = {10336--10341},
  publisher = {Proceedings of the National Academy of Sciences},
  doi       = {10.1073/pnas.1502134112},
  urldate   = {2025-05-14},
  abstract  = {Explaining the variation between human languages and the constraints on that variation is a core goal of linguistics. In the last 20 y, it has been claimed that many striking universals of cross-linguistic variation follow from a hypothetical principle that dependency length---the distance between syntactically related words in a sentence---is minimized. Various models of human sentence production and comprehension predict that long dependencies are difficult or inefficient to process; minimizing dependency length thus enables effective communication without incurring processing difficulty. However, despite widespread application of this idea in theoretical, empirical, and practical work, there is not yet large-scale evidence that dependency length is actually minimized in real utterances across many languages; previous work has focused either on a small number of languages or on limited kinds of data about each language. Here, using parsed corpora of 37 diverse languages, we show that overall dependency lengths for all languages are shorter than conservative random baselines. The results strongly suggest that dependency length minimization is a universal quantitative property of human languages and support explanations of linguistic variation in terms of general properties of human information processing.},
  file      = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Futrell et al_2015_Large-scale evidence of dependency length minimization in 37 languages.pdf}
}
@inproceedings{futrell-quantifying-2015,
  title     = {Quantifying {{Word Order Freedom}} in {{Dependency Corpora}}},
  booktitle = {Proceedings of the {{Third International Conference}} on {{Dependency Linguistics}} ({{Depling}} 2015)},
  author    = {Futrell, Richard and Mahowald, Kyle and Gibson, Edward},
  editor    = {Nivre, Joakim and Haji{\v c}ov{\'a}, Eva},
  year      = {2015},
  month     = aug,
  pages     = {91--100},
  publisher = {Uppsala University, Uppsala, Sweden},
  address   = {Uppsala, Sweden},
  urldate   = {2025-05-14},
  file      = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Futrell et al_2015_Quantifying Word Order Freedom in Dependency Corpora.pdf}
}
@misc{gemmateam2024gemmaopenmodelsbased,
  title         = {Gemma: {O}pen Models Based on {G}emini Research and Technology},
  author        = {Team Gemma},
  year          = {2024},
  eprint        = {2403.08295},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2403.08295}
}
@article{gerdes-et-al-2021-typometrics,
  title      = {Typometrics: {{From Implicational}} to {{Quantitative Universals}} in {{Word Order Typology}}},
  shorttitle = {Typometrics},
  author     = {Gerdes, Kim and Kahane, Sylvain and Chen, Xinying},
  year       = {2021},
  month      = feb,
  journal    = {Glossa: a journal of general linguistics},
  volume     = {6},
  number     = {1},
  publisher  = {Open Library of Humanities},
  issn       = {2397-1835},
  doi        = {10.5334/gjgl.764},
  urldate    = {2025-05-14},
  abstract   = {This paper develops the concept of word order universals based on a data analysis of the~Universal Dependencies project, which proposes treebanks of more than 90 languages~encoded with the same annotation scheme. The nature of the data we work on allows~us to extract rich details for testing well-known typological implicational universals~and, further, explore new kinds of universals that we call quantitative universals. We~show how such quantitative universals are in essence different from implicational~universals, including statistical universals, by the fact that they no longer lay down~any claims on categorical statements, but rather on continuous parameters, opening~a new field of research we propose to call typometrics.},
  copyright  = {Copyright: {\copyright} 2021 The Author(s).                     This is an open-access article distributed under the terms of the                        Creative Commons Attribution 4.0 International License (CC-BY 4.0), which                        permits unrestricted use, distribution, and reproduction in any medium,                        provided the original author and source are credited. See http://creativecommons.org/licenses/by/4.0/.},
  langid     = {english},
  file       = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Gerdes et al_2021_Typometrics.pdf}
}
@book{givon1979,
  title     = {On {{Understanding Grammar}}},
  author    = {Giv{\'o}n, T.},
  year      = {1979},
  series    = {Perspectives in Neurolinguistics and Psycholinguistics},
  publisher = {Academic Press},
  isbn      = {978-0-12-285450-7},
  lccn      = {78067876}
}
@book{givon1984syntax,
  author={Givon, Talmy},
  year={1984},
  publisher={Amsterdam:
Benjamins},
  title={Syntax: A Functional-Typological Introduction Vol I},
}
@phdthesis{uehara,
  title = {Syntactic Categories in {{Japanese}}: {{A}} Typological and Cognitive Introduction},
  shorttitle = {Syntactic Categories in {{Japanese}}},
  author = {Uehara, Satoshi},
  year = 1995,
  address = {United States -- Michigan},
  url = {https://www.proquest.com/docview/304224225/abstract/8E9037B9963C4950PQ/1},
  urldate = {2025-08-09},
  abstract = {This dissertation examines the organization of syntactic categories in Japanese and provides a typologically valid basis for cognitive linguistic analyses of the language. Three fundamental problems in syntactic category analyses of Japanese in the past are identified in Chapter 1. The chapter then briefly outlines recent discoveries on the nature of "category" in cognitive linguistics; specifically "prototype" semantic theory (Lakoff 1987, Taylor 1990), discusses central notions of Cognitive Grammar developed by Langacker (1987, 1991), and summarizes Croft's (1991) theory for the universal definition of major syntactic categories. Chap. 2 examines structural criteria used in previous analyses for identification of the five so-called "major" categories in Japanese, clarifies points of disagreement, re-evaluates each criterion in light of Croft's model, and argues that the crucial notion characterizing the structural aspects of the categories in Japanese is a language-specific property of morphological boundness. This boundness criterion not only serves as a basis for the definition of inflection in Japanese, but divides syntactic categories in Japanese into two major classes, each of which is unmarked, or "designed" for one of the two primary pragmatic functions of predication and reference. Two "unique" categories of Japanese, Nominal Adjectives and Verbal Nouns are examined in Chap. 3 and 4 respectively, and cognitive semantic analyses of the two problematic categories are presented. Specifically, it is argued that the grammatical behavior that the two categories exhibit is well motivated by their meaning--meaning as "conceptualization." Evidence is provided for the fundamental assumptions necessarily underlying syntactic category analyses of the language: prototype organization, the syntax-lexicon continuum, and the conceptualization process. Chap. 5 examines a functional motivation of the cardinal structural feature of boundness in Japanese, presents the results of a lexical semantic survey, and argues that the boundness distinction is well motivated by the semantic distinction of relationality, although this correlation has been obscured to some extent in contemporary Japanese. A diachronic account for the form/meaning mismatch is offered in which a semantic shift that occurred to Nouns, which led to the birth of Nominal Adjectives, was not fully accompanied by the shift in morphological boundness.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9798209363859},
  langid = {english},
  school = {University of Michigan},
  keywords = {Education,Language,Language arts,Language literature and linguistics,Linguistics,Modern language},
  file = {/Users/coleman/Zotero/storage/USIE2SAM/Uehara - 1995 - Syntactic categories in Japanese A typological and cognitive introduction.pdf}
}


@inproceedings{glaff,
  title     = {{GL{\`A}FF}, a Large Versatile {F}rench Lexicon},
  author    = {Hathout, Nabil  and
               Sajous, Franck  and
               Calderone, Basilio},
  booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)},
  month     = may,
  year      = {2014},
  address   = {Reykjavik, Iceland},
  publisher = {European Language Resources Association (ELRA)},
  url       = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/58_Paper.pdf},
  pages     = {1007--1012},
  abstract  = {This paper introduces GLAFF, a large-scale versatile French lexicon extracted from Wiktionary, the collaborative online dictionary. GLAFF contains, for each entry, inflectional features and phonemic transcriptions. It distinguishes itself from the other available French lexicons by its size, its potential for constant updating and its copylefted license. We explain how we have built GLAFF and compare it to other known resources in terms of coverage and quality of the phonemic transcriptions. We show that its size and quality are strong assets that could allow GLAFF to become a reference lexicon for French NLP and linguistics. Moreover, other derived lexicons can easily be based on GLAFF to satisfy specific needs of various fields such as psycholinguistics.}
}
@inproceedings{glove,
  title     = {{G}lo{V}e: Global Vectors for Word Representation},
  author    = {Pennington, Jeffrey  and
               Socher, Richard  and
               Manning, Christopher},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
  month     = oct,
  year      = {2014},
  address   = {Doha, Qatar},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D14-1162},
  doi       = {10.3115/v1/D14-1162},
  pages     = {1532--1543}
}
@book{gordon-phonological-2016,
  title     = {Phonological {{Typology}}},
  author    = {Gordon, Matthew K.},
  year      = {2016},
  month     = apr,
  publisher = {Oxford University Press},
  doi       = {10.1093/acprof:oso/9780199669004.001.0001},
  urldate   = {2024-10-07},
  isbn      = {978-0-19-966900-4}
}
@book{greenberg-1966-universals,
  title      = {Universals of Language},
  shorttitle = {Universals of Language},
  editor     = {Greenberg, Joseph Harold},
  year       = {1966},
  series     = {The {{M}}.{{I}}.{{T}}. {{Press}} Paperback Series},
  edition    = {2nd},
  number     = {37},
  publisher  = {M.I.T Pr},
  address    = {Cambridge, Mass.},
  isbn       = {978-0-262-57008-4},
  langid     = {english}
}
@book{greenberg1966,
  title     = {Universals of language},
  editor    = {Greenberg, Joseph H.},
  edition   = {2},
  year      = {1966},
  publisher = {M.I.T. Press}
}
@article{haiman1980,
  title      = {The {{Iconicity}} of {{Grammar}}: {{Isomorphism}} and {{Motivation}}},
  shorttitle = {The {{Iconicity}} of {{Grammar}}},
  author     = {Haiman, John},
  year       = {1980},
  journal    = {Language},
  volume     = {56},
  number     = {3},
  eprint     = {414448},
  eprinttype = {jstor},
  pages      = {515--540},
  publisher  = {Linguistic Society of America},
  issn       = {0097-8507},
  doi        = {10.2307/414448},
  urldate    = {2025-01-28},
  abstract   = {Although linguistic signs in isolation are symbolic, the system or grammar which relates them may be diagrammatically iconic in two ways: (a) by isomorphism, a bi-unique correspondence tends to be established between signans and signatum; (b) by motivation, the structure of language directly reflects some aspect of the structure of reality. Isomorphism is so nearly universal that deviations from it require explanation. Motivation, although widespread, establishes a typology of languages, as indicated in Saussure's Cours. The evidence of artificial taboo languages suggests that degree of motivation co-varies inversely with the number of 'prima onomata' in the lexicon.},
  keywords   = {thesislit},
  file       = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Haiman_1980_The Iconicity of Grammar.pdf}
}
@article{haley-corpusbased-2023,
  title     = {{Corpus-based measures discriminate inflection and derivation cross-linguistically}},
  author    = {Haley, Coleman and Ponti, Edoardo M. and Goldwater, Sharon},
  year      = {2023},
  month     = jun,
  journal   = {Society for Computation in Linguistics},
  volume    = {6},
  number    = {1},
  publisher = {University of Massachusetts Amherst Libraries},
  issn      = {2834-1007},
  pages     = {403--407},
  doi       = {10.7275/z5z0-xx64},
  urldate   = {2024-10-15},
  abstract  = {Japanese passives are traditionally considered to have two types: direct and indirect passives. However, more recent studies, such as Ishizuka (2012), suggest the two types can be unified un- der the same syntactic movement analysis. Uti- lizing the Balanced Corpus of Contemporary Written Japanese (BCCWJ; Maekawa, 2008; Maekawa et al., 2014), this study aims to in- vestigate how likely different types of passives appear in the naturally occurring texts, espe- cially in relation to markedness-based hierar- chy called Noun Phrase Accessibility Hierar- chy (NPAH; Keenan and Comrie, 1977), and to investigate if true indirect passives occur in contemporary written Japanese.},
  langid    = {None},
  file      = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Haley et al_2023_Corpus-based measures discriminate inflection and derivation.pdf}
}
@article{haspelmath-2010-comparative,
  title      = {Comparative Concepts and Descriptive Categories in Crosslinguistic Studies},
  author     = {Haspelmath, Martin},
  year       = {2010},
  journal    = {Language},
  volume     = {86},
  number     = {3},
  eprint     = {40961695},
  eprinttype = {jstor},
  pages      = {663--687},
  publisher  = {Linguistic Society of America},
  issn       = {0097-8507},
  urldate    = {2024-10-07},
  abstract   = {In this discussion note, I argue that we need to distinguish carefully between descriptive categories, that is, categories of particular languages, and comparative concepts, which are used for crosslinguistic comparison and are specifically created by typologists for the purposes of comparison. Descriptive formal categories cannot be equated across languages because the criteria for category assignment are different from language to language. This old structuralist insight (called CATEGORIAL PARTICULARISM) has recently been emphasized again by several linguists, but the idea that linguists need to identify 'crosslinguistic categories' before they can compare languages is still widespread, especially (but not only) in generative linguistics. Instead, what we have to do (and normally do in practice) is to create comparative concepts that allow us to identify comparable phenomena across languages and to formulate crosslinguistic generalizations. Comparative concepts have to be universally applicable, so they can only be based on other universally applicable concepts: conceptual-semantic concepts, general formal concepts, and other comparative concepts. Comparative concepts are not always purely semantically based concepts, but outside of phonology they usually contain a semantic component. The fact that typologists compare languages in terms of a separate set of concepts that is not taxonomically superordinate to descriptive linguistic categories means that typology and language-particular analysis are more independent of each other than is often thought.},
  file       = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Haspelmath_2010_Comparative concepts and descriptive categories in crosslinguistic studies.pdf}
}

@incollection{haspelmath-2003-geometry,
  title     = {The Geometry of Grammatical Meaning: {{Semantic}} Maps and Cross-Linguistic Comparison},
  booktitle = {The New Psychology of Language},
  author    = {Haspelmath, Martin},
  editor    = {Tomasello, Michael},
  year      = {2003},
  volume    = {2},
  pages     = {211--242},
  publisher = {Lawrence Erlbaum Associates},
  address   = {Mahwah, NJ, USA}
}
@article{haspelmath-2012-how,
  title     = {How to Compare Major Word-Classes across the World's Languages},
  author    = {Haspelmath, Martin},
  year      = {2012},
  month     = feb,
  journal   = {UCLA Working Papers in Linguistics},
  volume    = {17},
  pages     = {109--130},
  doi       = {10.5281/ZENODO.3678496},
  urldate   = {2024-05-15},
  abstract  = {In this paper, I argue that major word-classes, such as nouns, verbs and adjectives, cannot be compared across languages by asking questions such as "Does language X have a noun-verb distinction?". Such questions are routinely asked by linguists (functionalists and generativists alike), but these are the wrong questions (cf. Croft 2000), because they make presuppositions which are not valid.},
  copyright = {Creative Commons Attribution 4.0 International, Open Access}
}
@article{haspelmath-2007-preestablished,
  title   = {Pre-Established Categories Don't Exist: {{Consequences}} for Language Description and Typology},
  author  = {Haspelmath, Martin},
  year    = {2007},
  journal = {Linguistic {{Typology}}},
  volume  = {11},
  number  = {1},
  pages   = {119--132},
  doi     = {10.1515/LINGTY.2007.011},
  urldate = {2024-05-14}
}
@article{haspelmath1996,
  title     = {Word-class-changing inflection and morphological theory},
  author    = {Haspelmath, Martin},
  journal   = {Yearbook of morphology 1995},
  pages     = {43--66},
  year      = {1996},
  publisher = {Springer}
}
@article{haspelmath-2024-inflection,
  url         = {https://doi.org/10.1515/ling-2022-0086},
  title       = {Inflection and derivation as traditional comparative concepts},
  author      = {Martin Haspelmath},
  pages       = {43--77},
  volume      = {62},
  number      = {1},
  journal     = {Linguistics},
  doi         = {doi:10.1515/ling-2022-0086},
  year        = {2024},
  lastchecked = {2024-04-24}
}
@inproceedings{hessel-quantifying-2018,
  title     = {Quantifying the {{Visual Concreteness}} of {{Words}} and {{Topics}} in {{Multimodal Datasets}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  author    = {Hessel, Jack and Mimno, David and Lee, Lillian},
  editor    = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
  year      = {2018},
  month     = jun,
  pages     = {2194--2205},
  publisher = {Association for Computational Linguistics},
  address   = {New Orleans, Louisiana},
  doi       = {10.18653/v1/N18-1199},
  urldate   = {2024-07-23},
  abstract  = {Multimodal machine learning algorithms aim to learn visual-textual correspondences. Previous work suggests that concepts with concrete visual manifestations may be easier to learn than concepts with abstract ones. We give an algorithm for automatically computing the visual concreteness of words and topics within multimodal datasets. We apply the approach in four settings, ranging from image captions to images/text scraped from historical books. In addition to enabling explorations of concepts in multimodal datasets, our concreteness scores predict the capacity of machine learning algorithms to learn textual/visual relationships. We find that 1) concrete concepts are indeed easier to learn; 2) the large number of algorithms we consider have similar failure cases; 3) the precise positive relationship between concreteness and performance varies between datasets. We conclude with recommendations for using concreteness scores to facilitate future multimodal research.},
  file      = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Hessel et al_2018_Quantifying the Visual Concreteness of Words and Topics in Multimodal Datasets.pdf}
}
@article{hsieh-distinguishing-2019,
  title      = {Distinguishing Nouns and Verbs: {{A Tagalog}} Case Study},
  shorttitle = {Distinguishing Nouns and Verbs},
  author     = {Hsieh, Henrison},
  year       = {2019},
  month      = may,
  journal    = {Natural Language \& Linguistic Theory},
  volume     = {37},
  number     = {2},
  pages      = {523--569},
  issn       = {0167-806X, 1573-0859},
  doi        = {10.1007/s11049-018-9422-3},
  urldate    = {2024-10-07},
  langid     = {english}
}
@article{HuDongDaiTong+2017,
  url         = {https://doi.org/10.1515/ijb-2017-0013},
  title       = {A Comparison of Methods for Estimating the Determinant of High-Dimensional Covariance Matrix},
  author      = {Zongliang Hu and Kai Dong and Wenlin Dai and Tiejun Tong},
  pages       = {20170013},
  volume      = {13},
  number      = {2},
  journal     = {The International Journal of Biostatistics},
  doi         = {doi:10.1515/ijb-2017-0013},
  year        = {2017},
  lastchecked = {2023-02-15}
}
@article{kaufman-austronesian-2009,
  title   = {Austronesian {{Nominalism}} and Its Consequences: {{A Tagalog}} Case Study},
  author  = {Kaufman, Daniel},
  year    = {2009},
  journal = {Theoretical {{Linguistics}}},
  volume  = {35},
  number  = {1},
  pages   = {1--49},
  doi     = {10.1515/THLI.2009.001},
  urldate = {2024-05-15}
}
@article{kirkici_clahsen_2013,
  title     = {Inflection and derivation in native and non-native language processing: Masked priming experiments on Turkish},
  volume    = {16},
  doi       = {10.1017/S1366728912000648},
  number    = {4},
  journal   = {Bilingualism: Language and Cognition},
  publisher = {Cambridge University Press},
  author    = {Kirkici, Bilal and Clahsen, Harald},
  year      = {2013},
  pages     = {776791}
}
@article{kolgomorov-smirnov,
  author    = { Frank J.   Massey   Jr. },
  title     = {The {K}olmogorov-{S}mirnov Test for Goodness of Fit},
  journal   = {Journal of the American Statistical Association},
  volume    = {46},
  number    = {253},
  pages     = {68-78},
  year      = {1951},
  publisher = {Taylor & Francis},
  doi       = {10.1080/01621459.1951.10500769},
  url       = { 
               https://www.tandfonline.com/doi/abs/10.1080/01621459.1951.10500769
               
               },
  eprint    = { 
               https://www.tandfonline.com/doi/pdf/10.1080/01621459.1951.10500769
               
               }
}
@article{konig2006marked,
  title     = {Marked nominative in Africa},
  author    = {K{\"o}nig, Christa},
  journal   = {Studies in Language. International Journal sponsored by the Foundation Foundations of Language},
  volume    = {30},
  number    = {4},
  pages     = {655--732},
  year      = {2006},
  publisher = {John Benjamins}
}



@inproceedings{koper-automatically-2016,
  title     = {Automatically {{Generated Affective Norms}} of {{Abstractness}}, {{Arousal}}, {{Imageability}} and {{Valence}} for 350 000 {{German Lemmas}}},
  booktitle = {Proceedings of the {{Tenth International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}}'16)},
  author    = {K{\"o}per, Maximilian and {Schulte im Walde}, Sabine},
  editor    = {Calzolari, Nicoletta and Choukri, Khalid and Declerck, Thierry and Goggi, Sara and Grobelnik, Marko and Maegaard, Bente and Mariani, Joseph and Mazo, Helene and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
  year      = {2016},
  month     = may,
  pages     = {2595--2598},
  publisher = {European Language Resources Association (ELRA)},
  address   = {Portoro{\v z}, Slovenia},
  urldate   = {2024-10-07},
  abstract  = {This paper presents a collection of 350,000 German lemmatised words, rated on four psycholinguistic affective attributes. All ratings were obtained via a supervised learning algorithm that can automatically calculate a numerical rating of a word. We applied this algorithm to abstractness, arousal, imageability and valence. Comparison with human ratings reveals high correlation across all rating types. The full resource is publically available at: http://www.ims.uni-stuttgart.de/data/affective\_norms/},
  file      = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Kper_Schulte im Walde_2016_Automatically Generated Affective Norms of Abstractness, Arousal, Imageability.pdf}
}

@inproceedings{kumarsouheil2017,
  author    = {Kasthuri, M. and Kumar, S. Britto Ramesh and Khaddaj, Souheil},
  booktitle = {2017 World Congress on Computing and Communication Technologies (WCCCT)},
  title     = {PLIS: Proposed Language Independent Stemmer for Information Retrieval Systems Using Dynamic Programming},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {132-135},
  doi       = {10.1109/WCCCT.2016.39}
}
@article{laksnamer2022,
  title   = {Hebrewnette--A New Derivational Resource for Non-concatenative Morphology: Principles, Design and Implementation},
  author  = {Laks, Lior and Namer, Fiammetta},
  journal = {The Prague Bulletin of Mathematical Linguistics},
  volume  = {118},
  pages   = {25--53},
  year    = {2022}
}

@incollection{larasati2011indonesian,
  title     = {Indonesian Morphology Tool ({{MorphInd}}): {{Towards}} an Indonesian Corpus},
  booktitle = {Systems and Frameworks for Computational Morphology},
  author    = {Larasati, Septina Dian and Kubo{\v n}, Vladislav and Zeman, Daniel},
  editor    = {Mahlow, Cerstin and Piotrowski, Michael},
  year      = {2011},
  pages     = {119--129},
  publisher = {{Springer Berlin Heidelberg}},
  address   = {{Berlin, Heidelberg}},
  doi       = {10.1007/978-3-642-23138-4_8}
}

@article{laudanna-et-al-1992-processing,
  title     = {Processing inflectional and derivational morphology},
  author    = {Laudanna, Alessandro and Badecker, William and Caramazza, Alfonso},
  journal   = {Journal of Memory and Language},
  volume    = {31},
  number    = {3},
  pages     = {333--348},
  year      = {1992},
  publisher = {Elsevier}
}

@article{levenshtein,
  title   = {Binary Codes Capable of Correcting Deletions, Insertions and Reversals},
  author  = {Vladimir Levenshtein},
  journal = {Soviet Physics Doklady},
  volume  = {10},
  pages   = {707},
  year    = {1966}
}
@inproceedings{levshina-2020-how,
  title      = {How Tight Is Your Language? {{A}} Semantic Typology Based on {{Mutual Information}}},
  shorttitle = {How Tight Is Your Language?},
  booktitle  = {Proceedings of the 19th {{International Workshop}} on {{Treebanks}} and {{Linguistic Theories}}},
  author     = {Levshina, Natalia},
  editor     = {Evang, Kilian and Kallmeyer, Laura and Ehren, Rafael and Petitjean, Simon and Seyffarth, Esther and Seddah, Djam{\'e}},
  year       = {2020},
  month      = oct,
  pages      = {70--78},
  publisher  = {Association for Computational Linguistics},
  address    = {D{\"u}sseldorf, Germany},
  doi        = {10.18653/v1/2020.tlt-1.7},
  urldate    = {2025-05-14},
  file       = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Levshina_2020_How tight is your language.pdf}
}
@article{levy-2008-expectation,
  title = {Expectation-Based Syntactic Comprehension},
  author = {Levy, Roger},
  year = {2008},
  journal = {Cognition},
  volume = {106},
  number = {3},
  pages = {1126--1177},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2007.05.006},
  abstract = {This paper investigates the role of resource allocation as a source of processing difficulty in human sentence comprehension. The paper proposes a simple information-theoretic characterization of processing difficulty as the work incurred by resource reallocation during parallel, incremental, probabilistic disambiguation in sentence comprehension, and demonstrates its equivalence to the theory of Hale [Hale, J. (2001). A probabilistic Earley parser as a psycholinguistic model. In Proceedings of NAACL (Vol. 2, pp. 159--166)], in which the difficulty of a word is proportional to its surprisal (its negative log-probability) in the context within which it appears. This proposal subsumes and clarifies findings that high-constraint contexts can facilitate lexical processing, and connects these findings to well-known models of parallel constraint-based comprehension. In addition, the theory leads to a number of specific predictions about the role of expectation in syntactic comprehension, including the reversal of locality-based difficulty patterns in syntactically constrained contexts, and conditions under which increased ambiguity facilitates processing. The paper examines a range of established results bearing on these predictions, and shows that they are largely consistent with the surprisal theory.},
  keywords = {Frequency,Information theory,Parsing,Prediction,Sentence processing,Syntactic complexity,Syntax,Word order}
}

@article{smith-levy-2013,
  title = {The Effect of Word Predictability on Reading Time Is Logarithmic},
  author = {Smith, Nathaniel J. and Levy, Roger},
  year = {2013},
  journal = {Cognition},
  volume = {128},
  number = {3},
  pages = {302--319},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2013.02.013},
  abstract = {It is well known that real-time human language processing is highly incremental and context-driven, and that the strength of a comprehender's expectation for each word encountered is a key determinant of the difficulty of integrating that word into the preceding context. In reading, this differential difficulty is largely manifested in the amount of time taken to read each word. While numerous studies over the past thirty years have shown expectation-based effects on reading times driven by lexical, syntactic, semantic, pragmatic, and other information sources, there has been little progress in establishing the quantitative relationship between expectation (or prediction) and reading times. Here, by combining a state-of-the-art computational language model, two large behavioral data-sets, and non-parametric statistical techniques, we establish for the first time the quantitative form of this relationship, finding that it is logarithmic over six orders of magnitude in estimated predictability. This result is problematic for a number of established models of eye movement control in reading, but lends partial support to an optimal perceptual discrimination account of word recognition. We also present a novel model in which language processing is highly incremental well below the level of the individual word, and show that it predicts both the shape and time-course of this effect. At a more general level, this result provides challenges for both anticipatory processing and semantic integration accounts of lexical predictability effects. And finally, this result provides evidence that comprehenders are highly sensitive to relative differences in predictability -- even for differences between highly unpredictable words -- and thus helps bring theoretical unity to our understanding of the role of prediction at multiple levels of linguistic structure in real-time language comprehension.},
  keywords = {Expectation,Information theory,Probabilistic models of cognition,Psycholinguistics,Reading}
}

@article{liljencrants-numerical-1972,
  title      = {Numerical {{Simulation}} of {{Vowel Quality Systems}}: {{The Role}} of {{Perceptual Contrast}}},
  shorttitle = {Numerical {{Simulation}} of {{Vowel Quality Systems}}},
  author     = {Liljencrants, Johan and Lindblom, Bj{\"o}rn and Lindblom, Bjorn},
  year       = {1972},
  month      = dec,
  journal    = {Language},
  volume     = {48},
  number     = {4},
  eprint     = {411991},
  eprinttype = {jstor},
  pages      = {839--862},
  issn       = {00978507},
  doi        = {10.2307/411991},
  urldate    = {2024-10-07}
}
@article{lin-word-2022,
  title    = {Word Imageability Is Associated with Expressive Vocabulary in Children with Autism Spectrum Disorder},
  author   = {Lin, Kimberly R and Wisman Weil, Lisa and Thurm, Audrey and Lord, Catherine and Luyster, Rhiannon J},
  year     = {2022},
  month    = mar,
  journal  = {Autism \& Developmental Language Impairments},
  volume   = {7},
  issn     = {2396-9415},
  doi      = {10.1177/23969415221085827},
  urldate  = {2024-07-23},
  abstract = {Background \& aims Throughout typical development, children prioritize different perceptual, social, and linguistic cues to learn words. The earliest acquired words are often those that are perceptually salient and highly imageable. Imageability, the ease in which a word evokes a mental image, is a strong predictor for word age of acquisition in typically developing (TD) children, independent of other lexicosemantic features such as word frequency. However, little is known about the effects of imageability in children with autism spectrum disorder (ASD), who tend to have differences in linguistic processing and delayed language acquisition compared to their TD peers. This study explores the extent to which imageability and word frequency are associated with early noun and verb acquisition in children with ASD. Methods Secondary analyses were conducted on previously collected data of 156 children (78 TD, 78 ASD) matched on sex and parent-reported language level. Total expressive vocabulary, as measured by the MacArthur Bates Communicative Development Inventory (MB-CDI), included 123 words (78 nouns, 45 verbs) that overlapped with previously published imageability ratings and word input frequencies. A two-step hierarchical linear regression was used to examine the relationship between word input frequency, imageability, and total expressive vocabulary. An F-test was then used to assess the unique contribution of imageability on total expressive vocabulary when controlling for word input frequency. Results In both the TD and ASD groups, imageability uniquely explained a portion of the variance in total expressive vocabulary size, independent of word input frequency. Notably, imageability was significantly associated with noun vocabulary and verb vocabulary size alone, with imageability explaining a greater portion of the variance in total nouns produced than in total verbs produced. Conclusions Imageability was identified as a significant lexicosemantic feature for describing expressive vocabulary size in children with ASD. Consistent with literature on TD children, children with ASD who have small vocabularies primarily produce words that are highly imageable. Children who are more proficient word learners with larger vocabularies produce words that are less imageable, indicating a potential shift away from reliance on perceptual-based language processing. This was consistent across both noun and verb vocabularies. Implications Our findings contribute to a growing body of literature describing early word learning in children with ASD and provide a basis for exploring the use of multisensory language learning strategies.},
  pmcid    = {PMC9620684},
  pmid     = {36382067},
  file     = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Lin et al_2022_Word imageability is associated with expressive vocabulary in children with.pdf}
}

@inproceedings{ljubesic-predicting-2018,
  title     = {Predicting {{Concreteness}} and {{Imageability}} of {{Words Within}} and {{Across Languages}} via {{Word Embeddings}}},
  booktitle = {Proceedings of the {{Third Workshop}} on {{Representation Learning}} for {{NLP}}},
  author    = {Ljube{\v s}i{\'c}, Nikola and Fi{\v s}er, Darja and {Peti-Stanti{\'c}}, Anita},
  editor    = {Augenstein, Isabelle and Cao, Kris and He, He and Hill, Felix and Gella, Spandana and Kiros, Jamie and Mei, Hongyuan and Misra, Dipendra},
  year      = {2018},
  month     = jul,
  pages     = {217--222},
  publisher = {Association for Computational Linguistics},
  address   = {Melbourne, Australia},
  doi       = {10.18653/v1/W18-3028},
  urldate   = {2024-10-07},
  abstract  = {The notions of concreteness and imageability, traditionally important in psycholinguistics, are gaining significance in semantic-oriented natural language processing tasks. In this paper we investigate the predictability of these two concepts via supervised learning, using word embeddings as explanatory variables. We perform predictions both within and across languages by exploiting collections of cross-lingual embeddings aligned to a single vector space. We show that the notions of concreteness and imageability are highly predictable both within and across languages, with a moderate loss of up to 20\% in correlation when predicting across languages. We further show that the cross-lingual transfer via word embeddings is more efficient than the simple transfer via bilingual dictionaries.},
  file      = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Ljubei et al_2018_Predicting Concreteness and Imageability of Words Within and Across Languages.pdf}
}
@article{lynott-lancaster-2020,
  title      = {The {{Lancaster Sensorimotor Norms}}: Multidimensional Measures of Perceptual and Action Strength for 40,000 {{English}} Words},
  shorttitle = {The {{Lancaster Sensorimotor Norms}}},
  author     = {Lynott, Dermot and Connell, Louise and Brysbaert, Marc and Brand, James and Carney, James},
  year       = {2020},
  month      = jun,
  journal    = {Behavior Research Methods},
  volume     = {52},
  number     = {3},
  pages      = {1271--1291},
  issn       = {1554-3528},
  doi        = {10.3758/s13428-019-01316-z},
  urldate    = {2024-10-15},
  abstract   = {Sensorimotor information plays a fundamental role in cognition. However, the existing materials that measure the sensorimotor basis of word meanings and concepts have been restricted in terms of their sample size and breadth of sensorimotor experience. Here we present norms of sensorimotor strength for 39,707 concepts across six perceptual modalities (touch, hearing, smell, taste, vision, and interoception) and five action effectors (mouth/throat, hand/arm, foot/leg, head excluding mouth/throat, and torso), gathered from a total of 3,500 individual participants using Amazon's Mechanical Turk platform. The Lancaster Sensorimotor Norms are unique and innovative in a number of respects: They represent the largest-ever set of semantic norms for English, at 40,000 words {\texttimes} 11 dimensions (plus several informative cross-dimensional variables), they extend perceptual strength norming to the new modality of interoception, and they include the first norming of action strength across separate bodily effectors. In the first study, we describe the data collection procedures, provide summary descriptives of the dataset, and interpret the relations observed between sensorimotor dimensions. We then report two further studies, in which we (1) extracted an optimal single-variable composite of the 11-dimension sensorimotor profile (Minkowski 3 strength) and (2) demonstrated the utility of both perceptual and action strength in facilitating lexical decision times and accuracy in two separate datasets. These norms provide a valuable resource to researchers in diverse areas, including psycholinguistics, grounded cognition, cognitive semantics, knowledge representation, machine learning, and big-data approaches to the analysis of language and conceptual representations. The data are accessible via the Open Science Framework (http://osf.io/7emr6/) and an interactive web application (https://www.lancaster.ac.uk/psychology/lsnorms/).},
  langid     = {english},
  file       = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Lynott et al_2020_The Lancaster Sensorimotor Norms.pdf}
}
@article{mackay-1978-derivational,
  title     = {Derivational rules and the internal lexicon},
  author    = {MacKay, Donald G},
  journal   = {Journal of verbal learning and verbal behavior},
  volume    = {17},
  number    = {1},
  pages     = {61--71},
  year      = {1978},
  publisher = {Elsevier}
}
@inproceedings{malouf2020,
  title     = {Lexical databases for computational analyses: A linguistic perspective},
  author    = {Malouf, Robert  and
               Ackerman, Farrell  and
               Semenuks, Arturs},
  editor    = {Ettinger, Allyson  and
               Jarosz, Gaja  and
               Pater, Joe},
  booktitle = {Proceedings of the Society for Computation in Linguistics 2020},
  month     = jan,
  year      = {2020},
  address   = {New York, New York},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.scil-1.52},
  pages     = {446--456}
}
@misc{martinez-using-2024,
  title      = {Using Large Language Models to Estimate Features of Multi-Word Expressions: {{Concreteness}}, Valence, Arousal},
  shorttitle = {Using Large Language Models to Estimate Features of Multi-Word Expressions},
  author     = {Mart{\'i}nez, Gonzalo and Molero, Juan Diego and Gonz{\'a}lez, Sandra and Conde, Javier and Brysbaert, Marc and Reviriego, Pedro},
  year       = {2024}
}
@inproceedings{mccarthy-etal-2020-unimorp,
  title        = {UniMorph 3.0: Universal Morphology},
  author       = {McCarthy, Arya D and Kirov, Christo and Grella, Matteo and Nidhi, Amrit and Xia, Patrick and Gorman, Kyle and Vylomova, Ekaterina and Mielke, Sabrina J and Nicolai, Garrett and Silfverberg, Miikka and others},
  booktitle    = {Proceedings of The 12th language resources and evaluation conference},
  pages        = {3922--3931},
  year         = {2020},
  organization = {European Language Resources Association}
}
@inproceedings{mccarthy-etal-2020-unimorph,
  title     = {{U}ni{M}orph 3.0: {U}niversal {M}orphology},
  author    = {McCarthy, Arya D.  and
               Kirov, Christo  and
               Grella, Matteo  and
               Nidhi, Amrit  and
               Xia, Patrick  and
               Gorman, Kyle  and
               Vylomova, Ekaterina  and
               Mielke, Sabrina J.  and
               Nicolai, Garrett  and
               Silfverberg, Miikka  and
               Arkhangelskiy, Timofey  and
               Krizhanovsky, Nataly  and
               Krizhanovsky, Andrew  and
               Klyachko, Elena  and
               Sorokin, Alexey  and
               Mansfield, John  and
               Ern{\v{s}}treits, Valts  and
               Pinter, Yuval  and
               Jacobs, Cassandra L.  and
               Cotterell, Ryan  and
               Hulden, Mans  and
               Yarowsky, David},
  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
  month     = may,
  year      = {2020},
  address   = {Marseille, France},
  publisher = {European Language Resources Association},
  url       = {https://aclanthology.org/2020.lrec-1.483},
  pages     = {3922--3931},
  abstract  = {The Universal Morphology (UniMorph) project is a collaborative effort providing broad-coverage instantiated normalized morphological paradigms for hundreds of diverse world languages. The project comprises two major thrusts: a language-independent feature schema for rich morphological annotation and a type-level resource of annotated data in diverse languages realizing that schema. We have implemented several improvements to the extraction pipeline which creates most of our data, so that it is both more complete and more correct. We have added 66 new languages, as well as new parts of speech for 12 languages. We have also amended the schema in several ways. Finally, we present three new community tools: two to validate data for resource creators, and one to make morphological data available from the command line. UniMorph is based at the Center for Language and Speech Processing (CLSP) at Johns Hopkins University in Baltimore, Maryland. This paper details advances made to the schema, tooling, and dissemination of project resources since the UniMorph 2.0 release described at LREC 2018.},
  language  = {English},
  isbn      = {979-10-95546-34-4}
}
@misc{minixhofer-zeroshot-2024,
  title         = {Zero-{{Shot Tokenizer Transfer}}},
  author        = {Minixhofer, Benjamin and Ponti, Edoardo Maria and Vuli{\'c}, Ivan},
  year          = {2024},
  month         = may,
  number        = {arXiv:2405.07883},
  eprint        = {2405.07883},
  primaryclass  = {cs},
  publisher     = {arXiv},
  urldate       = {2024-05-15},
  abstract      = {Language models (LMs) are bound to their tokenizer, which maps raw text to a sequence of vocabulary items (tokens). This restricts their flexibility: for example, LMs trained primarily on English may still perform well in other natural and programming languages, but have vastly decreased efficiency due to their English-centric tokenizer. To mitigate this, we should be able to swap the original LM tokenizer with an arbitrary one, on the fly, without degrading performance. Hence, in this work we define a new problem: Zero-Shot Tokenizer Transfer (ZeTT). The challenge at the core of ZeTT is finding embeddings for the tokens in the vocabulary of the new tokenizer. Since prior heuristics for initializing embeddings often perform at chance level in a ZeTT setting, we propose a new solution: we train a hypernetwork taking a tokenizer as input and predicting the corresponding embeddings. We empirically demonstrate that the hypernetwork generalizes to new tokenizers both with encoder (e.g., XLM-R) and decoder LLMs (e.g., Mistral-7B). Our method comes close to the original models' performance in cross-lingual and coding tasks while markedly reducing the length of the tokenized sequence. We also find that the remaining gap can be quickly closed by continued training on less than 1B tokens. Finally, we show that a ZeTT hypernetwork trained for a base (L)LM can also be applied to fine-tuned variants without extra training. Overall, our results make substantial strides toward detaching LMs from their tokenizer.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computation and Language},
  file          = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Minixhofer et al_2024_Zero-Shot Tokenizer Transfer.pdf;/Users/coleman/Zotero/storage/63WZHVK7/2405.html}
}
  month = aug,
  journal = {arXiv.org},
  urldate = {2024-10-07},
  abstract = {This study investigates the potential of large language models (LLMs) to provide accurate estimates of concreteness, valence and arousal for multi-word expressions. Unlike previous artificial intelligence (AI) methods, LLMs can capture the nuanced meanings of multi-word expressions. We systematically evaluated ChatGPT-4o's ability to predict concreteness, valence and arousal. In Study 1, ChatGPT-4o showed strong correlations with human concreteness ratings (r = .8) for multi-word expressions. In Study 2, these findings were repeated for valence and arousal ratings of individual words, matching or outperforming previous AI models. Study 3 extended the prevalence and arousal analysis to multi-word expressions and showed promising results despite the lack of large-scale human benchmarks. These findings highlight the potential of LLMs for generating valuable psycholinguistic data related to multiword expressions. To help researchers with stimulus selection, we provide datasets with AI norms of concreteness, valence and arousal for 126,397 English single words and 63,680 multi-word expressions},
  howpublished = {https://arxiv.org/abs/2408.16012v1},
  langid = {english},
  file = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Martnez et al_2024_Using large language models to estimate features of multi-word expressions.pdf}
}
@inproceedings{morphynet,
  title     = {{M}orphy{N}et: a Large Multilingual Database of Derivational and Inflectional Morphology},
  author    = {Batsuren, Khuyagbaatar  and
               Bella, G{\'a}bor  and
               Giunchiglia, Fausto},
  booktitle = {Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology},
  month     = aug,
  year      = {2021},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.sigmorphon-1.5},
  doi       = {10.18653/v1/2021.sigmorphon-1.5},
  pages     = {39--48},
  abstract  = {Large-scale morphological databases provide essential input to a wide range of NLP applications. Inflectional data is of particular importance for morphologically rich (agglutinative and highly inflecting) languages, and derivations can be used, e.g. to infer the semantics of out-of-vocabulary words. Extending the scope of state-of-the-art multilingual morphological databases, we announce the release of MorphyNet, a high-quality resource with 15 languages, 519k derivational and 10.1M inflectional entries, and a rich set of morphological features. MorphyNet was extracted from Wiktionary using both hand-crafted and automated methods, and was manually evaluated to be of a precision higher than 98{\%}. Both the resource generation logic and the resulting database are made freely available and are reusable as stand-alone tools or in combination with existing resources.}
}
@inproceedings{mscoco,
  author    = {Lin, Tsung-Yi
               and Maire, Michael
               and Belongie, Serge
               and Hays, James
               and Perona, Pietro
               and Ramanan, Deva
               and Doll{\'a}r, Piotr
               and Zitnick, C. Lawrence},
  editor    = {Fleet, David
               and Pajdla, Tomas
               and Schiele, Bernt
               and Tuytelaars, Tinne},
  title     = {Microsoft COCO: Common Objects in Context},
  booktitle = {Computer Vision -- ECCV 2014},
  year      = {2014},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {740--755},
  abstract  = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  isbn      = {978-3-319-10602-1}
}

@article{multisimlex,
  title    = {Multi-{S}im{L}ex: A Large-Scale Evaluation of Multilingual and Crosslingual Lexical Semantic Similarity},
  author   = {Vuli{\'c}, Ivan  and
              Baker, Simon  and
              Ponti, Edoardo Maria  and
              Petti, Ulla  and
              Leviant, Ira  and
              Wing, Kelly  and
              Majewska, Olga  and
              Bar, Eden  and
              Malone, Matt  and
              Poibeau, Thierry  and
              Reichart, Roi  and
              Korhonen, Anna},
  journal  = {Computational Linguistics},
  volume   = {46},
  number   = {4},
  month    = dec,
  year     = {2020},
  url      = {https://aclanthology.org/2020.cl-4.5},
  doi      = {10.1162/coli_a_00391},
  pages    = {847--897},
  abstract = {We introduce Multi-SimLex, a large-scale lexical resource and evaluation benchmark covering data sets for 12 typologically diverse languages, including major languages (e.g., Mandarin Chinese, Spanish, Russian) as well as less-resourced ones (e.g., Welsh, Kiswahili). Each language data set is annotated for the lexical relation of semantic similarity and contains 1,888 semantically aligned concept pairs, providing a representative coverage of word classes (nouns, verbs, adjectives, adverbs), frequency ranks, similarity intervals, lexical fields, and concreteness levels. Additionally, owing to the alignment of concepts across languages, we provide a suite of 66 crosslingual semantic similarity data sets. Because of its extensive size and language coverage, Multi-SimLex provides entirely novel opportunities for experimental evaluation and analysis. On its monolingual and crosslingual benchmarks, we evaluate and analyze a wide array of recent state-of-the-art monolingual and crosslingual representation models, including static and contextualized word embeddings (such as fastText, monolingual and multilingual BERT, XLM), externally informed lexical representations, as well as fully unsupervised and (weakly) supervised crosslingual word embeddings. We also present a step-by-step data set creation protocol for creating consistent, Multi-Simlex{--}style resources for additional languages. We make these contributions{---}the public release of Multi-SimLex data sets, their creation protocol, strong baseline results, and in-depth analyses which can be helpful in guiding future developments in multilingual lexical semantics and representation learning{---}available via a Web site that will encourage community effort in further expansion of Multi-Simlex to many more languages. Such a large-scale semantic resource could inspire significant further advances in NLP across languages.}
}
@article{narasimhan_unsupervised_2015,
  title   = {An unsupervised method for uncovering morphological chains},
  volume  = {3},
  journal = {Transactions of the Association for Computational Linguistics},
  author  = {Narasimhan, Karthik and Barzilay, Regina and Jaakkola, Tommi},
  month   = {December},
  year    = {2015},
  pages   = {157--167}
}

@article{newmeyer-2007-linguistic,
  title   = {Linguistic Typology Requires Crosslinguistic Formal Categories},
  author  = {Newmeyer, Frederick J},
  year    = {2007},
  series  = {Linguistic {{Typology}}},
  volume  = {11},
  number  = {1},
  pages   = {133--157},
  doi     = {10.1515/LINGTY.2007.012},
  urldate = {2024-05-14}
}
@misc{oh2024leadingwhitespaceslanguagemodels,
  title         = {Leading Whitespaces of Language Models' Subword Vocabulary Poses a Confound for Calculating Word Probabilities},
  author        = {Byung-Doh Oh and William Schuler},
  year          = {2024},
  eprint        = {2406.10851},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2406.10851}
}
@inproceedings{oscar,
  author    = {Julien Abadji and Pedro Javier Ortiz Su{\'a}rez and Laurent Romary and Beno{\^i}t Sagot},
  title     = {Ungoliant: An optimized pipeline for the generation of a very large-scale multilingual web corpus},
  series    = {Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-9) 2021. Limerick, 12 July 2021 (Online-Event)},
  editor    = {Harald L{\"u}ngen and Marc Kupietz and Piotr Baski and Adrien Barbaresi and Simon Clematide and Ines Pisetta},
  publisher = {Leibniz-Institut f{\"u}r Deutsche Sprache},
  address   = {Mannheim},
  doi       = {10.14618/ids-pub-10468},
  url       = {https://nbn-resolving.org/urn:nbn:de:bsz:mh39-104688},
  pages     = {1 -- 9},
  year      = {2021},
  abstract  = {Since the introduction of large language models in Natural Language Processing, large raw corpora have played a crucial role in Computational Linguistics. However, most of these large raw corpora are either available only for English or not available to the general public due to copyright issues. Nevertheless, there are some examples of freely available multilingual corpora for training Deep Learning NLP models, such as the OSCAR and Paracrawl corpora. However, they have quality issues, especially for low-resource languages. Moreover, recreating or updating these corpora is very complex. In this work, we try to reproduce and improve the goclassy pipeline used to create the OSCAR corpus. We propose a new pipeline that is faster, modular, parameterizable, and well documented. We use it to create a corpus similar to OSCAR but larger and based on recent data. Also, unlike OSCAR, the metadata information is at the document level. We release our pipeline under an open source license and publish the corpus under a research-only license.},
  language  = {en}
}
@inproceedings{ostling-2015-word,
  title     = {Word {{Order Typology}} through {{Multilingual Word Alignment}}},
  booktitle = {Proceedings of the 53rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 7th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 2: {{Short Papers}})},
  author    = {{\"O}stling, Robert},
  editor    = {Zong, Chengqing and Strube, Michael},
  year      = {2015},
  month     = jul,
  pages     = {205--211},
  publisher = {Association for Computational Linguistics},
  address   = {Beijing, China},
  doi       = {10.3115/v1/P15-2034},
  urldate   = {2025-05-14},
  file      = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/stling_2015_Word Order Typology through Multilingual Word Alignment.pdf}
}
@incollection{oxford,
  author    = {Bisang, Walter},
  isbn      = {9780199281251},
  title     = {{Word Classes}},
  booktitle = {{The Oxford Handbook of Linguistic Typology}},
  publisher = {Oxford University Press},
  year      = {2010},
  month     = {11},
  abstract  = {{This article introduces the four prerequisites for distinguishing word classes: semantic criteria; pragmatic criteria/criteria of discourse function; formal criteria; and distinction between lexical and syntactic levels of analysis. The most important approaches to word classes based on the first three prerequisites are addressed. The article also deals with the distinction between content words and function words. It then takes up the discussion of the universal status of the noun/verb distinction by integrating the fourth prerequisite. The languages discussed are Classical Nahuatl, Late Archaic Chinese, and Tongan. The distinction between content words and function words is not identical to the distinction between open and closed word classes. The article reviews Dixon's seminal approach to adjectives. The sub-classes of adverbs are considered. The definition of word classes integrates all the central elements that make language structure, and it integrates a whole paradigm of constructions.}},
  doi       = {10.1093/oxfordhb/9780199281251.013.0015},
  url       = {https://doi.org/10.1093/oxfordhb/9780199281251.013.0015},
  eprint    = {https://academic.oup.com/book/0/chapter/335277661/chapter-ag-pdf/44444033/book\_38630\_section\_335277661.ag.pdf}
}
@inproceedings{pawley2006where,
  title     = {Where Have All the Verbs Gone? {{Remarks}} on the Organisation of Languages with Small, Closed Verb Classes},
  booktitle = {11th Biennial Rice University Linguistics Symposium},
  author    = {Pawley, Andrew K.},
  year      = {2006}
}
@article{perlmutter-1988-split,
  title     = {The split morphology hypothesis: Evidence from Yiddish},
  author    = {Perlmutter, David},
  journal   = {Theoretical morphology},
  pages     = {79--100},
  year      = {1988},
  publisher = {Academic Press San Diego, CA}
}


@misc{pimentel2024computeprobabilityword,
  title         = {How to Compute the Probability of a Word},
  author        = {Tiago Pimentel and Clara Meister},
  year          = {2024},
  eprint        = {2406.14561},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2406.14561}
}
@article{plank-extent-2017,
  title   = {Extent and Limits of Linguistic Diversity as the Remit of Typology -- but through Constraints on What Is Diversity Limited?},
  author  = {Plank, Frans},
  year    = {2017},
  journal = {Linguistic Typology},
  volume  = {21},
  number  = {2017},
  pages   = {43--68},
  doi     = {doi:10.1515/lingty-2017-1004},
  urldate = {2024-05-14}
}
@incollection{plank-inflection-1994,
  title     = {Inflection and Derivation},
  author    = {Frans Plank},
  booktitle = {The Encyclopedia of Language and Linguistics},
  publisher = {Elsevier Science and Technology},
  address   = {Amsterdam},
  year      = {1994},
  pages     = {1671--1679}
}

@incollection{Plank1994,
  title     = {Inflection and Derivation},
  author    = {Frans Plank},
  booktitle = {The Encyclopedia of Language and Linguistics},
  publisher = {Elsevier Science and Technology},
  address   = {Amsterdam},
  year      = {1994},
  pages     = {1671--1679}
}

@inproceedings{qi-stanza-2020,
  title      = {Stanza: {{A Python Natural Language Processing Toolkit}} for {{Many Human Languages}}},
  shorttitle = {Stanza},
  booktitle  = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{System Demonstrations}}},
  author     = {Qi, Peng and Zhang, Yuhao and Zhang, Yuhui and Bolton, Jason and Manning, Christopher D.},
  editor     = {Celikyilmaz, Asli and Wen, Tsung-Hsien},
  year       = {2020},
  month      = jul,
  pages      = {101--108},
  publisher  = {Association for Computational Linguistics},
  address    = {Online},
  doi        = {10.18653/v1/2020.acl-demos.14},
  urldate    = {2024-10-07},
  abstract   = {We introduce Stanza, an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, and named entity recognition. We have trained Stanza on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested. Additionally, Stanza includes a native Python interface to the widely used Java Stanford CoreNLP software, which further extends its functionality to cover other tasks such as coreference resolution and relation extraction. Source code, documentation, and pretrained models for 66 languages are available at https://stanfordnlp.github.io/stanza/.},
  file       = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Qi et al_2020_Stanza.pdf}
}

@misc{quz-fst,
  title     = {Analizador morf\'{o}logico de la lengua {Q}uechua basado en software libre {H}elsinkifinite-statetransducer ({HFST})},
  author    = {Vilca, Hugo David Calderon and Mari\'{o}, Flor Cagniy C\'{a}rdenas and Calderon, Edwin Fredy Mamani},
  year      = {2012},
  publisher = {COMTEL}
}

@article{richards-nouns-2009,
  title   = {Nouns, Verbs, and Hidden Structure in {{Tagalog}}},
  author  = {Richards, Norvin},
  year    = {2009},
  month   = jul,
  journal = {Theoretical Linguistics},
  volume  = {35},
  number  = {1},
  pages   = {139--152},
  issn    = {0301-4428, 1613-4060},
  doi     = {10.1515/THLI.2009.008},
  urldate = {2024-10-07},
  langid  = {english},
  file    = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Richards_2009_Nouns, verbs, and hidden structure in Tagalog.pdf}
}

@article{rofes-imageability-2018,
  title    = {Imageability Ratings across Languages},
  author   = {Rofes, Adri{\`a} and Zakari{\'a}s, Lilla and Ceder, Klaudia and Lind, Marianne and Johansson, Monica Blom and {de Aguiar}, V{\^a}nia and Bjeki{\'c}, Jovana and Fyndanis, Valantis and Gavarr{\'o}, Anna and Simonsen, Hanne Gram and Sacrist{\'a}n, Carlos Hern{\'a}ndez and Kambanaros, Maria and Kraljevi{\'c}, Jelena Kuva{\v c} and {Mart{\'i}nez-Ferreiro}, Silvia and Mavis, {\.I}lknur and Orellana, Carolina M{\'e}ndez and S{\"o}r, Ingrid and Luk{\'a}cs, {\'A}gnes and Tun{\c c}er, M{\"u}ge and Vuksanovi{\'c}, Jasmina and Ibarrola, Amaia Munarriz and Pourquie, Marie and Varlokosta, Spyridoula and Howard, David},
  year     = {2018},
  month    = jun,
  journal  = {Behavior Research Methods},
  volume   = {50},
  number   = {3},
  pages    = {1187--1197},
  issn     = {1554-3528},
  doi      = {10.3758/s13428-017-0936-0},
  abstract = {Imageability is a psycholinguistic variable that indicates how well a word gives rise to a mental image or sensory experience. Imageability ratings are used extensively in psycholinguistic, neuropsychological, and aphasiological studies. However, little formal knowledge exists about whether and how these ratings are associated between and within languages. Fifteen imageability databases were cross-correlated using nonparametric statistics. Some of these corresponded to unpublished data collected within a European research network---the Collaboration of Aphasia Trialists (COST IS1208). All but four of the correlations were significant. The average strength of the correlations (rho = .68) and the variance explained (R2 = 46\%) were moderate. This implies that factors other than imageability may explain 54\% of the results. Imageability ratings often correlate across languages. Different possibly interacting factors may explain the moderate strength and variance explained in the correlations: (1) linguistic and cultural factors; (2) intrinsic differences between the databases; (3) range effects; (4) small numbers of words in each database, equivalent words, and participants; and (5) mean age of the participants. The results suggest that imageability ratings may be used cross-linguistically. However, further understanding of the factors explaining the variance in the correlations will be needed before research and practical recommendations can be made.}
}

@inproceedings{Ross1972,
  author    = {Ross, John R.},
  title     = {The category squish: Endstation Hauptwort},
  booktitle = {Proceedings of the Eighth Regional Meeting of the Chicago Linguistic Society},
  editor    = {Paul M. Peranteau and Judith N. Levi and Gloria C. Phares},
  pages     = {316--328},
  year      = {1972},
  publisher = {Chicago Linguistic Society, University of Chicago},
  address   = {Chicago, Illinois}
}
@article{saffranetal1996,
  title     = {Statistical learning by 8-month-old infants},
  author    = {Saffran, Jenny R and Aslin, Richard N and Newport, Elissa L},
  journal   = {Science},
  volume    = {274},
  number    = {5294},
  pages     = {1926--1928},
  year      = {1996},
  publisher = {American Association for the Advancement of Science}
}
@article{schakel2015measuring,
  title   = {Measuring Word Significance using Distributed Representations of Words},
  author  = {Adriaan M. J. Schakel and Benjamin J. Wilson},
  journal = {Computing Research Repository},
  volume  = {arXiv:1508.02297},
  year    = {2015},
  url     = {http://arxiv.org/abs/1508.02297}
}
@phdthesis{schultze-berndt-simple-2000,
  title      = {Simple and Complex Verbs in {{Jaminjung}}: {{A}} Study of Event Categorisation in an {{Australian}} Language},
  shorttitle = {Simple and Complex Verbs in {{Jaminjung}}},
  author     = {{Schultze-Berndt}, Eva},
  year       = {2000},
  address    = {Nijmegen},
  urldate    = {2024-10-07},
  langid     = {english},
  school     = {Radboud University}
}

@article{schwartz-dispersionfocalization-1997,
  title     = {The {{Dispersion-Focalization Theory}} of Vowel Systems},
  author    = {Schwartz, Jean-Luc and Bo{\"e}, Louis-Jean and Vall{\'e}e, Nathalie and Abry, Christian},
  year      = {1997},
  month     = jul,
  journal   = {Journal of Phonetics},
  volume    = {25},
  number    = {3},
  pages     = {255--286},
  issn      = {00954470},
  doi       = {10.1006/jpho.1997.0043},
  urldate   = {2024-10-07},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid    = {english}
}
@article{scott-glasgow-2019,
  title      = {The {{Glasgow Norms}}: {{Ratings}} of 5,500 Words on Nine Scales},
  shorttitle = {The {{Glasgow Norms}}},
  author     = {Scott, Graham G. and Keitel, Anne and Becirspahic, Marc and Yao, Bo and Sereno, Sara C.},
  year       = {2019},
  month      = jun,
  journal    = {Behavior Research Methods},
  volume     = {51},
  number     = {3},
  pages      = {1258--1270},
  issn       = {1554-3528},
  doi        = {10.3758/s13428-018-1099-3},
  urldate    = {2024-10-15},
  abstract   = {The Glasgow Norms are a set of normative ratings for 5,553 English words on nine psycholinguistic dimensions: arousal, valence, dominance, concreteness, imageability, familiarity, age of acquisition, semantic size, and gender association. The Glasgow Norms are unique in several respects. First, the corpus itself is relatively large, while simultaneously providing norms across a substantial number of lexical dimensions. Second, for any given subset of words, the same participants provided ratings across all nine dimensions (33 participants/word, on average). Third, two novel dimensions---semantic size and gender association---are included. Finally, the corpus contains a set of 379 ambiguous words that are presented either alone (e.g., toast) or with information that selects an alternative sense (e.g., toast (bread), toast (speech)). The relationships between the dimensions of the Glasgow Norms were initially investigated by assessing their correlations. In addition, a principal component analysis revealed four main factors, accounting for 82\% of the variance (Visualization, Emotion, Salience, and Exposure). The validity of the Glasgow Norms was established via comparisons of our ratings to 18 different sets of current psycholinguistic norms. The dimension of size was tested with megastudy data, confirming findings from past studies that have explicitly examined this variable. Alternative senses of ambiguous words (i.e., disambiguated forms), when discordant on a given dimension, seemingly led to appropriately distinct ratings. Informal comparisons between the ratings of ambiguous words and of their alternative senses showed different patterns that likely depended on several factors (the number of senses, their relative strengths, and the rating scales themselves). Overall, the Glasgow Norms provide a valuable resource---in particular, for researchers investigating the role of word recognition in language comprehension.},
  langid     = {english},
  keywords   = {Age of acquisition,Arousal,Concreteness,Dominance,Familiarity,Gender association,Imageability,Psycholinguistic norms,Semantic size,Valence},
  file       = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Scott et al_2019_The Glasgow Norms.pdf}
}

@incollection{shopen_1985,
  place     = {Cambridge},
  title     = {Inflectional Morphology},
  author    = {Anderson, Stephen R.},
  editors   = {Timothy Shopen},
  edition   = {1},
  booktitle = {Language Typology and Syntactic Description},
  volume    = {3},
  publisher = {Cambridge University Press},
  year      = {1985},
  pages     = {150--201}
}
@inbook{silverstein1986,
  url         = {https://doi.org/10.1515/9783110871661-008},
  title       = {7. Hierarchy of Features and Ergativity},
  booktitle   = {Features and Projections},
  author      = {Michael Silverstein},
  publisher   = {De Gruyter Mouton},
  address     = {Berlin, Boston},
  pages       = {163--232},
  doi         = {doi:10.1515/9783110871661-008},
  isbn        = {9783110871661},
  year        = {1986},
  lastchecked = {2023-10-13}
}
@book{spencer,
  title     = {Lexical Relatedness},
  author    = {Andrew Spencer},
  publisher = {Oxford University Press},
  address   = {Oxford},
  year      = {2013}
}

@article{spreen-parameters-1966,
  title     = {Parameters of Abstraction, Meaningfulness, and Pronunciability for 329 Nouns},
  author    = {Spreen, Otfried and Schulz, Rudolph W.},
  year      = {1966},
  month     = oct,
  journal   = {Journal of Verbal Learning and Verbal Behavior},
  volume    = {5},
  number    = {5},
  pages     = {459--468},
  issn      = {00225371},
  doi       = {10.1016/S0022-5371(66)80061-0},
  urldate   = {2024-10-07},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid    = {english}
}

@book{stassen1997,
  title     = {Intransitive {Predication}},
  isbn      = {978-0-19-823693-1},
  url       = {https://doi.org/10.1093/oso/9780198236931.001.0001},
  abstract  = {Intransitive Predication constitutes a major contribution to the study of typological linguistics and theoretical linguistics in general. Basing his analysis on a sample of 410 languages, Leon Stassen investigates cross-linguistic variation in one of the core domains of all natural languages. The author views this domain as a `cognitive space', the topography of which is the same for all languages. It is assumed to consist of four subdomains, which correspond to a four-way distinction between the semantic classes of event predicates, property predicates, class predicates, and locational predicates. Leon Stassen offers a typology of the structural manifestations of this domain, in terms of the nature and number of the formal strategies used in its encoding. He discusses a number of abstract principles which can be employed in explaining the cross-linguistic variation embodied by the typology. In the final chapter, he brings together the research results in a universally applicable model, which can be read as a `flow chart' for the encoding of intransitive predications in different language types.},
  publisher = {Oxford University Press},
  author    = {Stassen, Leon},
  month     = sep,
  year      = {1997},
  doi       = {10.1093/oso/9780198236931.001.0001}
}

@article{staub-predictability-2024a,
  title      = {Predictability in {{Language Comprehension}}: {{Prospects}} and {{Problems}} for {{Surprisal}}},
  shorttitle = {Predictability in {{Language Comprehension}}},
  author     = {Staub, Adrian},
  year       = {Forthcoming},
  journal    = {Annual Review of Linguistics},
  publisher  = {Annual Reviews},
  doi        = {10.1146/annurev-linguistics-011724-121517},
  urldate    = {2024-10-14},
  abstract   = {Surprisal theory proposes that a word\&apos;s predictability influences processing difficulty because each word requires the comprehender to update a probability distribution over possible sentences. This article first considers the theory\&apos;s detailed predictions regarding the effects of predictability on reading time and N400 amplitude. Two rather unintuitive predictions appear to be correct based on the current evidence: There is no specific cost when an unpredictable word is encountered in a context where another word is predictable, and the function relating predictability to processing difficulty is logarithmic, not linear. Next, the article addresses the viability of the claim, also associated with Surprisal, that conditional probability is the ``causal bottleneck'' mediating all effects on incremental processing difficulty. This claim fares less well as conditional probability does not account for the difficulty associated with encountering a low-frequency word or the difficulty associated with garden path disambiguation. Surprisal provides a compelling account of predictability effects but does not provide a complete account of incremental processing difficulty.},
  langid     = {english},
  file       = {/Users/coleman/Zotero/storage/7LE5JCVZ/annurev-linguistics-011724-121517.html}
}

@incollection{stekauer2015,
  title     = {14. The delimitation of derivation and inflection},
  author    = {{\v{S}}tekauer, Pavol},
  editor    = {Peter O. Mller and Ingeborg Ohnheiser and Susan Olsen and Franz Rainer},
  booktitle = {Volume 1 Word-Formation},
  pages     = {218--235},
  year      = {2015},
  publisher = {De Gruyter Mouton}
}

@article{striklievers-linguistic-2021,
  title   = {The Linguistic Dimensions of Concrete and Abstract Concepts: Lexical Category, Morphological Structure, Countability, and Etymology},
  author  = {Strik Lievers, Francesca and Bolognesi, Marianna and Winter, Bodo},
  year    = {2021},
  journal = {Cognitive {{Linguistics}}},
  volume  = {32},
  number  = {4},
  pages   = {641--670},
  doi     = {10.1515/cog-2021-0007},
  urldate = {2024-05-15}
}

@book{strunk2020finite,
  title     = {A Finite-State Morphological Analyzer for Central Alaskan Yup'Ik},
  author    = {Strunk, Lonny Alaskuk},
  year      = {2020},
  publisher = {University of Washington}
}

@article{swingley2005,
  title     = {Statistical clustering and the contents of the infant vocabulary},
  author    = {Swingley, Daniel},
  journal   = {Cognitive psychology},
  volume    = {50},
  number    = {1},
  pages     = {86--132},
  year      = {2005},
  publisher = {Elsevier}
}

@misc{Sylak-Glassman2016,
  title  = {The Composition and Use of the Universal Morphological Feature Schema (UniMorph Schema)},
  author = {Sylak-Glassman, John},
  year   = {2016},
  url    = {https://unimorph.github.io/doc/unimorph-schema.pdf}
}


@book{tenhacken-1994-defining,
  title     = {Defining Morphology: A Principled Approach to Determining the Boundaries of Compounding, Derivation, and Inflection},
  author    = {Hacken, P.},
  isbn      = {9783487098913},
  lccn      = {lc95155890},
  series    = {Altertumswissenschaftliche Texte Und Studien},
  url       = {https://books.google.co.uk/books?id=E8mWh\_6mRAcC},
  year      = {1994},
  publisher = {G. Olms Verlag}
}
@inproceedings{thapliyal-crossmodal3600-2022,
  title      = {Crossmodal-3600: {{A Massively Multilingual Multimodal Evaluation Dataset}}},
  shorttitle = {Crossmodal-3600},
  booktitle  = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author     = {Thapliyal, Ashish V. and Pont Tuset, Jordi and Chen, Xi and Soricut, Radu},
  editor     = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
  year       = {2022},
  month      = dec,
  pages      = {715--729},
  publisher  = {Association for Computational Linguistics},
  address    = {Abu Dhabi, United Arab Emirates},
  doi        = {10.18653/v1/2022.emnlp-main.45},
  urldate    = {2024-10-07},
  abstract   = {Research in massively multilingual image captioning has been severely hampered by a lack of high-quality evaluation datasets. In this paper we present the Crossmodal-3600 dataset (XM3600 in short), a geographically diverse set of 3600 images annotated with human-generated reference captions in 36 languages. The images were selected from across the world, covering regions where the 36 languages are spoken, and annotated with captions that achieve consistency in terms of style across all languages, while avoiding annotation artifacts due to direct translation. We apply this benchmark to model selection for massively multilingual image captioning models, and show superior correlation results with human evaluations when using XM3600 as golden references for automatic metrics.},
  file       = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Thapliyal et al_2022_Crossmodal-3600.pdf;/Users/coleman/Zotero/storage/V7P868ZV/Thapliyal et al. - 2022 - Crossmodal-3600 A Massively Multilingual Multimod.pdf}
}

@article{theil-estimation-1970,
  title      = {On the {{Estimation}} of {{Relationships Involving Qualitative Variables}}},
  author     = {Theil, Henri},
  year       = {1970},
  journal    = {American Journal of Sociology},
  volume     = {76},
  number     = {1},
  eprint     = {2775440},
  eprinttype = {jstor},
  pages      = {103--154},
  publisher  = {The University of Chicago Press},
  issn       = {0002-9602},
  urldate    = {2024-10-15},
  abstract   = {This article is concerned with the specification and estimation of relationships whose dependent variable is qualitative in nature (such as "yes" or "no"). It discusses logit equations with and without interaction, and the estimation procedure is generalized least squares. Part I deals with dependent variables that take only two values, Par II with variables taking more than two values, and part III describes informational measures for the explanatory power of the determining factors. The discussion of more advanced technical matters is contained in various appendixes.},
  file       = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Theil_1970_On the Estimation of Relationships Involving Qualitative Variables.pdf}
}

@article{thiessenetal2013,
  title     = {The extraction and integration framework: a two-process account of statistical learning.},
  author    = {Thiessen, Erik D and Kronstein, Alexandra T and Hufnagle, Daniel G},
  journal   = {Psychological bulletin},
  volume    = {139},
  number    = {4},
  pages     = {792},
  year      = {2013},
  publisher = {American Psychological Association}
}

@article{thiessensaffran2003,
  title     = {When cues collide: use of stress and statistical cues to word boundaries by 7-to 9-month-old infants.},
  author    = {Thiessen, Erik D and Saffran, Jenny R},
  journal   = {Developmental psychology},
  volume    = {39},
  number    = {4},
  pages     = {706},
  year      = {2003},
  publisher = {American Psychological Association}
}

@article{thompsonnewport2007,
  title     = {Statistical learning of syntax: The role of transitional probability},
  author    = {Thompson, Susan P and Newport, Elissa L},
  journal   = {Language learning and development},
  volume    = {3},
  number    = {1},
  pages     = {1--42},
  year      = {2007},
  publisher = {Taylor \& Francis}
}

@article{demarneffe-et-al-2021-universal,
  title     = {Universal {{Dependencies}}},
  author    = {{de Marneffe}, Marie-Catherine and Manning, Christopher D. and Nivre, Joakim and Zeman, Daniel},
  year      = {2021},
  month     = jun,
  journal   = {Computational Linguistics},
  volume    = {47},
  number    = {2},
  pages     = {255--308},
  publisher = {MIT Press},
  address   = {Cambridge, MA},
  doi       = {10.1162/coli_a_00402},
  urldate   = {2025-05-14},
  abstract  = {Universal dependencies (UD) is a framework for morphosyntactic annotation of human language, which to date has been used to create treebanks for more than 100 languages. In this article, we outline the linguistic theory of the UD framework, which draws on a long tradition of typologically oriented grammatical theories. Grammatical relations between words are centrally used to explain how predicate--argument structures are encoded morphosyntactically in different languages while morphological features and part-of-speech classes give the properties of words. We argue that this theory is a good basis for crosslinguistically consistent annotation of typologically diverse languages in a way that supports computational natural language understanding as well as broader linguistic studies.},
  file      = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/de Marneffe et al_2021_Universal Dependencies2.pdf}
}

@article{unider,
  title     = {Universal {D}erivations 1.0, A Growing Collection of Harmonised Word-Formation Resources},
  author    = {Kyjnek, Luk and abokrtsk, Zdenk and evkov, Magda and Vidra, Jon},
  journal   = {The Prague Bulletin of Mathematical Linguistics},
  number    = {115},
  volume    = {2},
  pages     = {333--348},
  year      = {2020},
  publisher = {Karolinum Press},
  address   = {Prague, Czech Republic}
}

@inproceedings{mccarthy-et-al-2020-unimorph,
  title     = {{U}ni{M}orph 3.0: {U}niversal {M}orphology},
  author    = {McCarthy, Arya D.  and
               Kirov, Christo  and
               Grella, Matteo  and
               Nidhi, Amrit  and
               Xia, Patrick  and
               Gorman, Kyle  and
               Vylomova, Ekaterina  and
               Mielke, Sabrina J.  and
               Nicolai, Garrett  and
               Silfverberg, Miikka  and
               Arkhangelskiy, Timofey  and
               Krizhanovsky, Nataly  and
               Krizhanovsky, Andrew  and
               Klyachko, Elena  and
               Sorokin, Alexey  and
               Mansfield, John  and
               Ern{\v{s}}treits, Valts  and
               Pinter, Yuval  and
               Jacobs, Cassandra L.  and
               Cotterell, Ryan  and
               Hulden, Mans  and
               Yarowsky, David},
  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
  month     = may,
  year      = {2020},
  address   = {Marseille, France},
  publisher = {European Language Resources Association},
  url       = {https://aclanthology.org/2020.lrec-1.483},
  pages     = {3922--3931},
  abstract  = {The Universal Morphology (UniMorph) project is a collaborative effort providing broad-coverage instantiated normalized morphological paradigms for hundreds of diverse world languages. The project comprises two major thrusts: a language-independent feature schema for rich morphological annotation and a type-level resource of annotated data in diverse languages realizing that schema. We have implemented several improvements to the extraction pipeline which creates most of our data, so that it is both more complete and more correct. We have added 66 new languages, as well as new parts of speech for 12 languages. We have also amended the schema in several ways. Finally, we present three new community tools: two to validate data for resource creators, and one to make morphological data available from the command line. UniMorph is based at the Center for Language and Speech Processing (CLSP) at Johns Hopkins University in Baltimore, Maryland. This paper details advances made to the schema, tooling, and dissemination of project resources since the UniMorph 2.0 release described at LREC 2018.},
  language  = {English},
  isbn      = {979-10-95546-34-4}
}
@inproceedings{batsuren-et-al-2022-unimorph,
  title     = {{U}ni{M}orph 4.0: {U}niversal {M}orphology},
  author    = {Batsuren, Khuyagbaatar  and
               Goldman, Omer  and
               Khalifa, Salam  and
               Habash, Nizar  and
               Kiera{\'s}, Witold  and
               Bella, G{\'a}bor  and
               Leonard, Brian  and
               Nicolai, Garrett  and
               Gorman, Kyle  and
               Ate, Yustinus Ghanggo  and
               Ryskina, Maria  and
               Mielke, Sabrina  and
               Budianskaya, Elena  and
               El-Khaissi, Charbel  and
               Pimentel, Tiago  and
               Gasser, Michael  and
               Lane, William Abbott  and
               Raj, Mohit  and
               Coler, Matt  and
               Samame, Jaime Rafael Montoya  and
               Camaiteri, Delio Siticonatzi  and
               Rojas, Esa{\'u} Zumaeta  and
               L{\'o}pez Francis, Didier  and
               Oncevay, Arturo  and
               L{\'o}pez Bautista, Juan  and
               Villegas, Gema Celeste Silva  and
               Hennigen, Lucas Torroba  and
               Ek, Adam  and
               Guriel, David  and
               Dirix, Peter  and
               Bernardy, Jean-Philippe  and
               Scherbakov, Andrey  and
               Bayyr-ool, Aziyana  and
               Anastasopoulos, Antonios  and
               Zariquiey, Roberto  and
               Sheifer, Karina  and
               Ganieva, Sofya  and
               Cruz, Hilaria  and
               Karah{\'o}{\v{g}}a, Ritv{\'a}n  and
               Markantonatou, Stella  and
               Pavlidis, George  and
               Plugaryov, Matvey  and
               Klyachko, Elena  and
               Salehi, Ali  and
               Angulo, Candy  and
               Baxi, Jatayu  and
               Krizhanovsky, Andrew  and
               Krizhanovskaya, Natalia  and
               Salesky, Elizabeth  and
               Vania, Clara  and
               Ivanova, Sardana  and
               White, Jennifer  and
               Maudslay, Rowan Hall  and
               Valvoda, Josef  and
               Zmigrod, Ran  and
               Czarnowska, Paula  and
               Nikkarinen, Irene  and
               Salchak, Aelita  and
               Bhatt, Brijesh  and
               Straughn, Christopher  and
               Liu, Zoey  and
               Washington, Jonathan North  and
               Pinter, Yuval  and
               Ataman, Duygu  and
               Wolinski, Marcin  and
               Suhardijanto, Totok  and
               Yablonskaya, Anna  and
               Stoehr, Niklas  and
               Dolatian, Hossep  and
               Nuriah, Zahroh  and
               Ratan, Shyam  and
               Tyers, Francis M.  and
               Ponti, Edoardo M.  and
               Aiton, Grant  and
               Arora, Aryaman  and
               Hatcher, Richard J.  and
               Kumar, Ritesh  and
               Young, Jeremiah  and
               Rodionova, Daria  and
               Yemelina, Anastasia  and
               Andrushko, Taras  and
               Marchenko, Igor  and
               Mashkovtseva, Polina  and
               Serova, Alexandra  and
               Prud{'}hommeaux, Emily  and
               Nepomniashchaya, Maria  and
               Giunchiglia, Fausto  and
               Chodroff, Eleanor  and
               Hulden, Mans  and
               Silfverberg, Miikka  and
               McCarthy, Arya D.  and
               Yarowsky, David  and
               Cotterell, Ryan  and
               Tsarfaty, Reut  and
               Vylomova, Ekaterina},
  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},
  month     = jun,
  year      = {2022},
  address   = {Marseille, France},
  publisher = {European Language Resources Association},
  url       = {https://aclanthology.org/2022.lrec-1.89},
  pages     = {840--855},
  abstract  = {The Universal Morphology (UniMorph) project is a collaborative effort providing broad-coverage instantiated normalized morphological inflection tables for hundreds of diverse world languages. The project comprises two major thrusts: a language-independent feature schema for rich morphological annotation, and a type-level resource of annotated data in diverse languages realizing that schema. This paper presents the expansions and improvements on several fronts that were made in the last couple of years (since McCarthy et al. (2020)). Collaborative efforts by numerous linguists have added 66 new languages, including 24 endangered languages. We have implemented several improvements to the extraction pipeline to tackle some issues, e.g., missing gender and macrons information. We have amended the schema to use a hierarchical structure that is needed for morphological phenomena like multiple-argument agreement and case stacking, while adding some missing morphological features to make the schema more inclusive.In light of the last UniMorph release, we also augmented the database with morpheme segmentation for 16 languages. Lastly, this new release makes a push towards inclusion of derivational morphology in UniMorph by enriching the data and annotation schema with instances representing derivational processes from MorphyNet.}
}

@inproceedings{unsup-pos-vec,
  title     = {Unsupervised Learning of Syntactic Structure with Invertible Neural Projections},
  author    = {He, Junxian  and
               Neubig, Graham  and
               Berg-Kirkpatrick, Taylor},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  month     = oct # {-} # nov,
  year      = {2018},
  address   = {Brussels, Belgium},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D18-1160},
  doi       = {10.18653/v1/D18-1160},
  pages     = {1292--1302},
  abstract  = {Unsupervised learning of syntactic structure is typically performed using generative models with discrete latent variables and multinomial parameters. In most cases, these models have not leveraged continuous word representations. In this work, we propose a novel generative model that jointly learns discrete syntactic structure and continuous word representations in an unsupervised fashion by cascading an invertible neural network with a structured generative prior. We show that the invertibility condition allows for efficient exact inference and marginal likelihood computation in our model so long as the prior is well-behaved. In experiments we instantiate our approach with both Markov and tree-structured priors, evaluating on two tasks: part-of-speech (POS) induction, and unsupervised dependency parsing without gold POS annotation. On the Penn Treebank, our Markov-structured model surpasses state-of-the-art results on POS induction. Similarly, we find that our tree-structured model achieves state-of-the-art performance on unsupervised dependency parsing for the difficult training condition where neither gold POS annotation nor punctuation-based constraints are available.}
}
@article{unsuplemm,
  author     = {Rudolf Rosa and
                Zdenek Zabokrtsk{\'{y}}},
  title      = {Unsupervised Lemmatization as Embeddings-Based Word Clustering},
  journal    = {CoRR},
  volume     = {abs/1908.08528},
  year       = {2019},
  url        = {http://arxiv.org/abs/1908.08528},
  eprinttype = {arXiv},
  eprint     = {1908.08528},
  timestamp  = {Sat, 23 Jan 2021 01:11:13 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1908-08528.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}
@book{vogel2011approaches,
  title     = {Approaches to the typology of word classes},
  author    = {Vogel, Petra M and Comrie, Bernard},
  volume    = {23},
  year      = {2011},
  publisher = {Walter de Gruyter}
}

@inproceedings{wartena2013distributional,
  title     = {Distributional Similarity of Words with Different Frequencies},
  author    = {Wartena, Christian},
  booktitle = {Proceedings of the 13th edition of the Dutch-Belgian information retrieval Workshop (DIR 2013)},
  pages     = {8--11},
  year      = {2013},
  publisher = {Hochschule Hannover}
}

@phdthesis{weber-grammar-1983,
  title     = {A {{Grammar}} of {{Huallaga}} (Huanuco) {{Quechua}}.},
  author    = {Weber, David John},
  address   = {United States -- California},
  urldate   = {2024-10-07},
  abstract  = {This is a reference grammar of Huallaga (Huanuco) Quechua, an American Indian language spoken in central Peru. After (1) a general introduction and (2) an introduction to HgQ syntax, it contains chapters of the following topics: on word and suffix classes for (3) verbs, (4) substantives, (5) adverbs, and (6) other classes; on morphology: (7) word formation generally, (8) the "transitions," i.e., the complex which indicates the person of the subject and object, and (9) the suffixes which occur between the root and the transition; on grammatical relations: (10) case markers (11) and passives; (12) on substantive phrases; (13) on relative clauses and complements; (14) on adverbial clauses; (15) on reduplication; (16) on question formation; (17) on negation; (18) on conjunction; on the post-transition suffixes: (19) the "shading" suffixes (-lla, -pis, -na, and -raq), (20) the (so-called) "topic" marker -qa, and (21) the evidential suffixes (-mi, -shi and -chi); (22) on idiomatic and formulaic expressions; and (23) on phonology and loan processes.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  year      = {1983},
  isbn      = {9798403416092},
  langid    = {english},
  school    = {University of California, Los Angeles},
  keywords  = {Language literature and linguistics,Linguistics},
  file      = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/WEBER_A Grammar of Huallaga (huanuco) Quechua.pdf}
}
@book{wetzer_typology_2013,
  title     = {The {Typology} of {Adjectival} {Predication}},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  isbn      = {978-3-11-081358-6},
  url       = {https://www.degruyterbrill.com/document/doi/10.1515/9783110813586/html?lang=en&srsltid=AfmBOopO-jMOVe12th7PUlYdT-lMYbXysoiLUh3x0gQrsjocyc10DfOl},
  abstract  = {The Typology of Adjectival Predication by Harrie Wetzer was published on March 1, 2013 by De Gruyter Mouton.},
  language  = {en},
  urldate   = {2025-08-09},
  publisher = {De Gruyter Mouton},
  author    = {Wetzer, Harrie},
  month     = mar,
  year      = {1996},
  doi       = {10.1515/9783110813586},
  keywords  = {Adjektiv, Kontrastive Grammatik, adjectives, cognitive}
}

@book{wiltschko-2014-universal,
  title     = {The {{Universal Structure}} of {{Categories}}},
  author    = {Wiltschko, Martina},
  year      = {2014},
  series    = {Cambridge {{Studies}} in {{Linguistics}}},
  publisher = {Cambridge University Press},
  address   = {Cambridge},
  doi       = {10.1017/CBO9781139833899},
  abstract  = {Using data from a variety of languages such as Blackfoot, Halkomelem, and Upper Austrian German, this book explores a range of grammatical categories and constructions, including tense, aspect, subjunctive, case and demonstratives. It presents a new theory of grammatical categories - the Universal Spine Hypothesis - and reinforces generative notions of Universal Grammar while accommodating insights from linguistic typology. In essence, this new theory shows that language-specific categories are built from a small set of universal categories and language-specific units of language. Throughout the book the Universal Spine Hypothesis is compared to two alternative theories - the Universal Base Hypothesis and the No Base Hypothesis. This valuable addition to the field will be welcomed by graduate students and researchers in linguistics.},
  isbn      = {978-1-107-03851-6}
}
@inproceedings{word2vec,
  author    = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  title     = {Distributed Representations of Words and Phrases and Their Compositionality},
  year      = {2013},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  abstract  = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
  booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
  pages     = {31113119},
  numpages  = {9},
  location  = {Lake Tahoe, Nevada},
  series    = {NIPS'13}
}
@inproceedings{wu-composition-2023,
  title      = {Composition and {{Deformance}}: {{Measuring Imageability}} with a {{Text-to-Image Model}}},
  shorttitle = {Composition and {{Deformance}}},
  booktitle  = {Proceedings of the 5th {{Workshop}} on {{Narrative Understanding}}},
  author     = {Wu, Si and Smith, David},
  editor     = {Akoury, Nader and Clark, Elizabeth and Iyyer, Mohit and Chaturvedi, Snigdha and Brahman, Faeze and Chandu, Khyathi},
  year       = {2023},
  month      = jul,
  pages      = {106--117},
  publisher  = {Association for Computational Linguistics},
  address    = {Toronto, Canada},
  doi        = {10.18653/v1/2023.wnu-1.16},
  urldate    = {2024-07-23},
  abstract   = {Although psycholinguists and psychologists have long studied the tendency of linguistic strings to evoke mental images in hearers or readers, most computational studies have applied this concept of imageability only to isolated words. Using recent developments in text-to-image generation models, such as DALLE mini, we propose computational methods that use generated images to measure the imageability of both single English words and connected text. We sample text prompts for image generation from three corpora: human-generated image captions, news article sentences, and poem lines. We subject these prompts to different deformances to examine the model's ability to detect changes in imageability caused by compositional change. We find high correlation between the proposed computational measures of imageability and human judgments of individual words. We also find the proposed measures more consistently respond to changes in compositionality than baseline approaches. We discuss possible effects of model training and implications for the study of compositionality in text-to-image models.},
  file       = {/Users/coleman/Library/Mobile Documents/com~apple~CloudDocs/Zotero/Wu_Smith_2023_Composition and Deformance.pdf}
}
@inproceedings{xlmr,
  title     = {Unsupervised Cross-lingual Representation Learning at Scale},
  author    = {Conneau, Alexis  and
               Khandelwal, Kartikay  and
               Goyal, Naman  and
               Chaudhary, Vishrav  and
               Wenzek, Guillaume  and
               Guzm{\'a}n, Francisco  and
               Grave, Edouard  and
               Ott, Myle  and
               Zettlemoyer, Luke  and
               Stoyanov, Veselin},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.acl-main.747},
  doi       = {10.18653/v1/2020.acl-main.747},
  pages     = {8440--8451},
  abstract  = {This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6{\%} average accuracy on XNLI, +13{\%} average F1 score on MLQA, and +2.4{\%} F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7{\%} in XNLI accuracy for Swahili and 11.4{\%} for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.}
}

@misc{ye2024computervisiondatasetsmodels,
  title         = {Computer Vision Datasets and Models Exhibit Cultural and Linguistic Diversity in Perception},
  author        = {Andre Ye and Sebastin Santy and Jena D. Hwang and Amy X. Zhang and Ranjay Krishna},
  year          = {2024},
  eprint        = {2310.14356},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2310.14356}
}

@inproceedings{zhai-sigmoid-2023,
  title     = {Sigmoid Loss for Language Image Pre-Training},
  booktitle = {2023 {{IEEE}}/{{CVF}} International Conference on Computer Vision ({{ICCV}})},
  author    = {Zhai, X. and Mustafa, B. and Kolesnikov, A. and Beyer, L.},
  year      = {2023},
  month     = oct,
  pages     = {11941--11952},
  publisher = {IEEE Computer Society},
  address   = {Los Alamitos, CA, USA},
  doi       = {10.1109/ICCV51070.2023.01100},
  abstract  = {We propose a simple pairwise sigmoid loss for imagetext pre-training. Unlike standard contrastive learning with softmax normalization, the sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. With only four TPUv4 chips, we can train a Base CLIP model at 4k batch size and a Large LiT model at 20k batch size, the latter achieves 84.5},
  keywords  = {computer vision,memory management,robustness,self-supervised learning,standards}
}



% \usepackage{times}
% \usepackage{latexsym}
% \usepackage{amsmath}
% \usepackage{covington}
% \usepackage{amsthm}
% \usepackage{amsfonts}
% \usepackage{amssymb}
% \usepackage{xspace}
% \usepackage{booktabs}
% \usepackage{mathtools}
% \usepackage{bbm}
% \usepackage{graphicx}
% \usepackage{microtype}
% \usepackage{fontawesome}

\def\naadjs{{\em na}-adjectives\xspace}
\def\iadj{{\em i}-adjective\xspace}
\def\iadjs{{\em i}-adjectives\xspace}
\def\naadj{{\em na}-adjective\xspace}

\def\vcontext{\mathbf{w}_{<t}}
\def\vclass{\mathcal{C}_i}
\def\vword{w_{t}}
\def\vvocab{\mathcal{W}}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.

%Including images in your LaTeX document requires adding
%additional package(s)

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}
\chapter{Splitting and lumping: Visual groundedness as an organizing factor among lexical classes}\label{ch:splitlump}
\chaptermark{Splitting and lumping}

% \maketitle

\epigraph{The detail of the pattern is movement.}{T.S. Eliot, \textit{Four Quartets}}

\section{Introduction}

% Since classical times, one of the fundamental ideas in linguistic theory is that words are divided into categories with shared syntactic and morphological behaviour. Often called ``word classes'' or ``parts of speech'', these classes represent an intersection between linguistic form and semantic function. For example, nouns prototypically refer to objects, and verbs to actions or events.

What is the theoretical status of the relationship between meaning and word class? Within any word class in a given language, exceptions to their semantic properties abound. Nevertheless, there is a great degree of cross-linguistic consistency in the relationship between the meaning of lexical items and their syntactic behaviour--the vast majority of languages clearly handle object words differently from action words. Property words also tend to have special morphosyntactic expression across languages, differing from both objects and events. But for each of these distinctions, there are languages where it is not clearly formally and distibutionally relevant \citep{bisang-2010-word}. How can a theory explain both these strong universal tendencies and well-established deviations from them?

In Chapter~\ref{ch:grounded}, we investigated the lexical--functional distinction: the distinction between word classes that are semantically rich and referential (lexical), and those that serve grammatical and syntactic functions. As discussed previously, this distinction has played an important role in theoretical, traditional, and experimental linguistics, but a clear definition is elusive. In chapter~\ref{ch:grounded}, I proposed a computational measure, visual groundedness, which could help to clarify this distinction. Visual groundedness shows a clear relationship to the distinction between lexical and functional word classes across 30 languages, demonstrating substantial cross-linguistic consistency--the same classes have similar groundedness across languages.

However, the distinction between lexical and functional classes identified by groundedness is not categorical, but gradient. Traditionally ``functional'' items sometimes exhibit high groundedness, and ``lexical'' items range substantially in how grounded they are. In this chapter, I investigate whether groundedness has the potential to explain not just the cross-linguistic consistency in which classes are lexical and which are functional, but also deviations and gradations within word class organization. Specifically, I focus on the traditionally ``lexical'' side of classes in the lexical--functional distinction. The three ``major'' word classes---nouns, adjectives, and verbs--have often been argued to form a continuum organized around semantic prototypes \citep{ross-1972-category,croft-2001-parts,rogers-2016-illustrating,givon-1979-understanding,rauhut-2023-quantitative}. I found a similar continuum between nouns, adjectives, and verbs in Chapter~\ref{ch:grounded}. Because these continuum and prototype theories have been argued to explain {\em deviations} from typical lexical class organization, a question naturally arises: ``Can a groundedness continuum help explain how and why some languages split a major lexical class, or collapse two classes together?'' In this chapter, I focus on the adjective class, which has an especially variable cross-linguistic expression and status. I conduct two studies, one focused on word class ``splitting'', and one focused on word class ``lumping.'' With respect to splitting, the first study presents evidence from Japanese, where adjectives are split into two formally distinct classes, \naadjs and \iadjs, which are formally similar to nouns and verbs respectively. While prior work has failed to find a semantic distinction between these classes, I show that their differences in groundedness are iconic of their formal similarities to nouns and verbs, respectively.

To study when and how languages collapse two major word classes together, I present a second study inspired by \citet{wetzer-1996-nouniness} and \citet{stassen-1997-intransitive}'s {\em Tensedness  Correlation}, which proposes that more verb-like encoding vs. more noun-like encoding of adjectives in a language is representative of a difference in how {\em statively} they conceive of verbs---with languages that have a more stative conceptions of verbs using a verb-like encoding for adjectives. \citet{wetzer-1996-nouniness} identified languages with a more stative conception of verbs as those that do not obligatorily mark tense on verbs, and showed this is strongly associated with ``noun-y'' vs ``verb-y'' encoding of adjectives. The proxy of tense expression was necessary because \cite{wetzer-1996-nouniness} did not have access to the conceptual prototype of verbs; however, I investigate the hypothesis that groundedness, which is higher for more stative concepts like adjectives and nouns, could display a similar pattern, with ``verb-y'' languages having higher verbal groundedness than ``noun-y'' languages. However, using present models and corpora, I am unable to find such convergent evidence for the Tensedness Correlation. This study highlights potential difficulties in comparing groundedness values between languages.

% The finding that groundedness

% If word classes are organized in part by the (visual) groundedness of the meanings they express, then variation in word classes should be associated with differences in groundedness of the expressed meanings. In this study, we focus on Japanese property words, which have the unusual property of constituting two formally very distinct word classes, rather than a single ``adjective'' class. Building on the insight that one of these classes is more formally ``nominal'' (\naadjs) and one more ``verbal'' (\iadjs), we hypothesise that we should see analogous trends in function: one class serving more prototypically nominal functions and one more prototypically verbal. In terms of visual groundedness, this corresponds to higher values for the nominal class.%. Adopting \citet{haleyetal2025}'s notion of visual {\em groundedness}, this corresponds to a hypothesis that the formally nominal class will express more grounded meanings than the verbal class. We find this trend is indeed borne out in image captioning data, even for equivalent meanings between the two classes.
\section{Continua among lexical word classes}\label{sec:lexcontinua}
One of the major findings of Chapter~\ref{ch:grounded} was that nouns exhibit significantly higher groundedness than adjectives, and both are significantly more grounded than verbs cross-linguistically--despite all being traditionally lexical classes. While many linguistic theories have treated these categories as entirely separate, there is a substantial literature in cognitive linguistics and typology which explores the idea that these categories constitute some kind of continuum within and across languages, especially that adjectives represent an intermediate category between nouns and verbs.

An early work in this direction is \citet{ross-1972-category}, who suggested a continuum with adjectives between nouns and verbs, based on syntactic behaviour. In particular, his argument hinges on further intermediate categories, such as different participle uses and ``adjectives used as nouns'' (e.g. {\em fun}). He shows an asymmetry and continuum across the application of several phenomena, like preposition deletion and postponing.
While influential, \citet{ross-1972-category}'s approach to treating the major parts of speech as a continuum through a ``category squish'' was criticized on a number of fronts \citep{newmeyer-1999-discrete}. Firstly, the ordering within/across categories was motivated formally, but lacked any functional justification. Secondly, the squish being formalized as positions on a real number line between 0 and 1 was criticized as arbitrary--there was no clear external criteria for assigning a particular word/noun-phrase/element its real-valued position in the squish. Structurally, the groundedness approach expanded upon in this part of the thesis is very Rossian in its approach, addressing these two criticisms by adding a functional formalization for assigning real-valued positions (groundedness) to linguistic elements, but ultimately maintaining the unidimensional flavour of Ross's approach.

Subsequent work built on Ross's ideas by adding functional justifications to both category prototypicality effects and fuzzy boundaries among the lexical classes, and by creating multifactorial accounts. For example, Giv\'{o}n, while considering multiple factors, gives a central role to the notion of {\em temporal stability} in \citet{givon-1979-understanding}, citing a cline between nouns, adjectives, and verbs in terms of their prototypical temporal stability, with verbs being the least prototypically stable. \citet{thompson-1989-discourse} proposes a view on which adjectives are intermediate between nouns and verbs in terms of discourse function: they are both prototypically {\em referent introducing} (like nouns) and {\em predicative} (like verbs). \citet{croft-1991-syntactic} takes a more multifactorial approach, defining four dimensions across which objects, properties, and actions (the semantic prototypes of nouns, adjectives, and verbs respectively) vary. Notably, most of Croft's properties have a monotonic continuum between nouns, adjectives, and verbs---the exception being gradability.

\begin{table*}[t]
  \centering
  \small
  \setlength{\tabcolsep}{12pt}
  \begin{tabular}[width=\textwidth]{lccc}
    \toprule
    & \textbf{Objects} & \textbf{Properties} & \textbf{Actions} \\
    \midrule
    \
    \textbf{Prototypical Class} & Noun & Adjective & Verb \\
    \textbf{Relationality} & nonrelational & relational & relational \\
    \textbf{Stativity} & state & state & process \\
    \textbf{Transitoriness} & permanent & permanent & transitory \\
    \textbf{Gradability} & nongradable & gradable & nongradable \\
    \textbf{Valency} & 0 & 1 & $\geq 1$ \\
    \bottomrule
  \end{tabular}
  \caption{\citet{croft-1991-syntactic}'s analysis of the conceptual categories of the major parts of speech and their semantic properties.}
\end{table*}

% \begin{table*}[h]
%   \centering
%   \small
%   \begin{tabular}{lcccccc}

%     \toprule
%     & \shortstack{Prototypical \\ Class} & Relationality & Stativity & Transitoriness & Gradability & Valency \\
%     \midrule
%     Objects   &  Noun & nonrelational & state   & permanent   & nongradable & 0 \\
%     Properties & Adjective & relational    & state   & permanent   & gradable & 1 \\
%     Actions    & Verb & relational    & process & transitory  & nongradable & \geq 1 \\
%     \bottomrule
%   \end{tabular}
%   \caption{\citet{croft-2001-radical}'s analysis of the conceptual categories of the major parts of speech and their semantic properties.}
% \end{table*}

While these accounts differ in the specific way they break down the parts of speech into a continuum, they are unified in the idea that adjectives represent a position which is in some important way(s) intermediate to nouns and verbs. This idea is supported not just by monolingual evidence, like \citet{ross-1972-category}'s English data, but also by a plethora of typological data. \citet{dixon-1977-where} presents a seminal survey, investigating 17 languages, and proposing 7 categories of properties which vary with how likely they are to pattern with nouns or verbs in the sample. In some languages, there are only a handful of ``adjectives'' with morphosyntax distinguished from nouns and verbs. For example, Bemba has less than twenty adjectives according to Dixon.  Dixon identifies a cline of semantic categories of properties which are more or less likely to pattern with nouns or verbs. For example, \textsc{material} properties (e.g. {\em wood(en)}, {\em metal}) tend to pattern with nouns,\footnote{This was actually identified in refinements to Dixon's work by \citet{wetzer-1992-nouny}.} while \textsc{human propensities} (e.g. {\em kind}, {\em angry}) tend to pattern with verbs. Other semantic categories fall between these extremes.

This typological evidence suggests a universal conceptual space between nouns, verbs, and adjectives---a semantic map.\footnote{\citet{rogers-2016-illustrating} investigated this tendency using a multidimensional scaling analysis on eleven languages, finding a rich two-dimensional prototype structure which largely aligned with previously proposed semantic dimensions.} However, while evidence for the fine-grained tendencies of semantic categories to pattern with nouns or verbs is compelling, the evidence for more abstract {\em motivations} for these tendencies is less clear. For example, \citet{stassen-1997-intransitive} links his own Dixon-like hierarchy of property meanings to \citet{givon-1979-understanding}'s temporal stability idea, but the direct typological evidence is for these semantic categories, not for temporal stability itself. As noted by \citet[p. 281]{croft-1991-syntactic} and \citet[pp. 214--215]{uehara-1995-syntactic}, persistence and transitoriness is often more complex than such hierarchies suggest, with certain property predicates being persistent for certain entities but not others (e.g. {\em hard} for a rock vs. {\em hard} for bread). Further, time-stability is necessarily gradient, and depends on the scale of reference. As such, the more fine-grained generalizations about semantic categories stand on firmer ground than the more abstract generalizations that motivate them.

The notion of temporal stability has often been treated as the key dimension distinguishing nouns, adjectives, and verbs \citep{verkerk-et-al-2008-encoding}. However, this account faces serious challenges. Words such as {\em lightning}, {\em explosion}, {\em puff}, {\em snowflake}, {\em bubble}, and {\em glimmer} describe highly ephemeral phenomena, yet they function naturally as nouns. Their success as nouns suggests that temporal stability alone cannot explain word class distinctions.

What these ``ephemeral nouns'' have in common is that, despite their brevity in time, they are spatially contained and identifiable. An explosion, for instance, may last only a moment, but it occupies a bounded region in space and forms a coherent visual object. Indeed, in \citet{givon-2001-syntax}, Giv\'{o}n amended his account to implicate \textsc{spatial compactness}---not just temporal persistence---in the nominal prototype, with spatial diffuseness tending to characterize verbs.

Yet temporal and spatial properties alone do not capture the full conceptual space of word classes. As discussed in Chapter~\ref{ch:background}, \textsc{relationality} of meaning as distinct from (but iconically related to) formal valency provides another crucial dimension. Taken together, temporal stability, spatial compactness/diffuseness, and relationality jointly shape how concepts are realized in lexical categories.

These dimensions are not independent. For instance, a concept with high relationality (e.g. give) tends to involve multiple participants distributed across space and time, thus exhibiting greater spatial and temporal diffuseness. Conversely, temporally compact experiences that are perceptually salient (e.g. explosion, blink) often form spatially bounded wholes, encouraging nominalization. Temporally compact experiences which are interesting enough to give a name to often involve motion, which spreads out the reference spatially. Temporal instability also means that, at any given moment, not all the information to fully pin down an event's category can always be perceived--what appears to be a kick could be someone standing still {\em as if} kicking, for example. The interrelation of these dimensions has an intuitive connection to visual grounding in images, as they influence how readily a concept can be visually identified from a static snapshot.

Importantly, groundedness is not limited to lexical categories. It extends into the functional domain, organizing the continuum from content words to grammatical morphemes. On this view, the familiar cline from nouns to verbs to adjectives reflects just one region of a broader \textsc{groundedness} and \textsc{lexicality cline} that also encompasses function words and (in principle) affixes. This offers a unified framework for connecting lexical class organization with the broader architecture of morphology and syntax. I will now explore two case studies that illustrate how groundedness can illuminate both the splitting and lumping of lexical categories across languages, beginning with Japanese adjectives (an instance of splitting).

% The notion of temporal stability as a key dimension between nouns, adjectives, and verbs has been called into question \citep{verkerk-et-al-2008-encoding}. A key piece of data for the inadequacy of temporal stability as a unifying explanation are words such as {\em lightning}, {\em explosion}, {\em puff}, {\em wave}, {\em snowflake}, {\em bubble}, {\em glimmer}. Each of these words is very ephemeral, and yet they make good nouns by shifting reference onto something {\em spatially contained and identifiable}. An {\em explosion} might be brief, but while it exists, it often makes a good object {\em visually}. It seems (at a distance) to have boundaries, and volume. The common displacement of an {\em exploding} event into a nominal expression is in this way not so mysterious. Indeed, while \citet{givon-1979-understanding} is often credited for identifying the role of temporal stability, subsequently \citep{givon1984} Giv\'{o}n himself implicated {\em spatial compactness} as part of the nominal prototype and, to a lesser extent, {\em spatial diffuseness} as part of the verbal prototype.

% As discussed in Chapter~\ref{ch:background}, relationality as distinct from (but iconically related to) formal valency is a key organizing principal in language. This is also implicated in several of these accounts of the conceptual space of work classes. I claim that we should think of valency/relationality, time stability, and spatial compactness/diffuseness as interrelated dimensions which play a role in determining groundedness, so groundedness in turn plays an organizing role in word class organization.

% The co-occurence of these properties in the prototype is not arbitrary; they {\em influence} one another. A concept's valency or relationality influences how many figures are involved in ``picking it out'' from an image, which increases spatial diffuseness. Temporally compact experiences which are interesting enough to give a name to often involve motion, which spreads out the reference spatially. Temporal instability also means that, at any given moment, not all the information to fully pin down an event's category can always be perceived--what appears to be a kick could be someone standing still {\em as if} kicking, for example. Together, these dimensions interrelate, and they interrelate (in principle) with visual {\em grounding} in images.

% Critically, groundedness does not just apply among the lexical word classes, but they extend ``down'' into the functional categories as well. Rather than viewing the cline of compactness, time-stability, and relationality between nouns, verbs, and adjectives as isolated to these three categories, I propose that groundedness serves as a unifying metric for understanding the entire word class cline, from lexical categories to functional elements. This model will not account for as much variation among lexical classes as a full multifactorial account, but allows us to produce new empirically testable hypotheses about lexical class organization as {\em connected} to the overall organization of morphemes.
% \begin{enumerate}
%   \item Next, connect the typology to the general semantic map hypothesis--that humans have a universal shared conceptual space, which is partitioned across linguistic phenomena. Languages differ in the way in which they carve the space up, but not in the underlying space. Further, the partitions of the space are always contiguous, never discontiguous. Here I will mention Japanese adjectives.

%   \item Each of the previous accounts faces significant challenges when accounting for the cross-linguistic data, also the challenge to the prototype or gradient theory of categories generally. Bring up some of the account's difficulties.
%     {\em Lightning} is actually an instance of a linguistic class that I think makes a compelling datum for groundedness as operant in part of speech categorization: words like {\em lightning}, {\em explosion}, {\em puff}, {\em wave}, {\em snowflake}, {\em bubble}, {\em glimmer}.

%     The co-occurence of these properties in the prototype is not arbitrary; they {\em influence} one another. A concept's valency or relationality influences how many figures are involved in ``picking it out'' from an image, which increases spatial diffuseness. Temporally compact experiences which are interesting enough to give a name to often involve motion, which spreads out the reference spatially. Temporal instability also means that, at any given moment, not all the information to fully pin down an event's category can always be perceived--what appears to be a kick could be someone standing still {\em as if} kicking, for example. This is to say, all this dimensions interrelate, and they interrelate (in principle) with visual {\em grounding} in images.

%   \item Accounts differ in their model of the map of the basic word classes. Do they form a triangle (Croft) or a line (Ross, Givon)? I propose a new type of line model, based on groundedness, where the line is not just between the major word classes, but also down into the functional classes.
% \end{enumerate}
% \begin{figure}
%   \begin{subfigure}
%     \begin{tikzpicture}
%       \node[draw=black,minimum height=0.6cm](n) at (0,0) {nouns};
%       \node[draw=black,minimum height=0.6cm](a) at (2,0) {adjectives};
%       \node[draw=black,minimum height=0.6cm](v) at (4,0) {verbs};
%       \draw(n)--(a)--(v);
%     \end{tikzpicture}

%   \end{subfigure}

%   \begin{subfigure}
%     \begin{tikzpicture}
%       \node[draw=black,minimum height=0.6cm](n) at (0, 0) {nouns};
%       \node[draw=black,minimum height=0.6cm](a) at (2, 0) {adjectives};
%       \node[draw=black,minimum height=0.6cm](v) at (1, -1.41) {verbs};
%       \draw(n)--(a)--(v)--(n);
%     \end{tikzpicture}
%   \end{subfigure}
% \end{figure}
%: nouns are prototypically temporally stable, while verbs are prototypically temporally unstable---that is, they are transient.  Adjectives, Giv\'{on} argues, exist in a continuum between nouns and verbs because they are intermediately stable: words like ``old'' or ``red'' are prototypically more temporally stable than ``give'', but less stable than ``person''. In particular, Giv\'{o}n argues that properties that are less temporally stable (``noisy'', ``to love'', ``to have'') are more likely to be expressed by verbs than a dedicated adjective class cross-linguistically.
\section{Japanese adjectives}\label{sec:jaadj}

The two word classes in Japanese typically described as adjectives are \iadjs and \naadjs. These classes are clearly distinguished from each other in Japanese in terms of their syntax and morphology:
\digloss[fspostamble=\bfseries,ex, postamble={ (\iadj) }]{yama-ga takai / takakatta.}
{mountain-\textsc{nom} high \textit{/} high.\textsc{past}}
{The mountain is/was tall.}
\digloss[fspostamble=\bfseries,ex, postamble={ (\naadj) }]{Taroo-ga sizuka da / sizuka datta}{Taro-\textsc{nom} quiet \textsc{cop} \textit{/} quiet \textsc{cop.past}}{Taro is/was quiet.}

\iadjs have an analogous inflectional paradigm to Japanese Verbs (inflecting for aspect and polarity) and both Verbs and \iadjs use a zero-copula strategy as in (6.1). Both \iadjs and Verbs can  modify nouns simply by appearing pre-nominally. However, the inflectional paradigm of \iadjs exhibits some differences from Verbs, and to be used in reference requires a different construction from Verbs:
% \digloss[fspostamble=\bfseries,ex, postamble={ (\iadj) }]{yama-ga takai / takakatta.}
% {mountain-\textsc{nom} high \textit{/} high.\textsc{past}}
% {The mountain is/was tall.}
% \digloss[fspostamble=\bfseries,ex, postamble={ (\naadj) }]{Taroo-ga sizuka da / sizuka datta}{Taro-\textsc{nom} quiet \textsc{cop} \textit{/} quiet \textsc{cop.past}}{Taro is/was quiet.}

As shown in (6.2), \naadjs must be combined with the copula in predication like Japanese Nouns. But nouns and \naadjs require an attributive marker, {\em -no} for nouns and {\em -na} for \naadjs, to modify nouns:

\digloss[fspostamble=\bfseries,ex, postamble={ (\naadj) }]{sizuka-na yoru}{quiet-\textsc{attr} night}{quiet night}
\digloss[fspostamble=\bfseries,ex, postamble={ (noun) }]{shiken-no hi }
{exam-\textsc{attr} day}
{exam day}
\digloss[fspostamble=\bfseries,ex, postamble={ (\iadj) }]{takai yama }
{tall mountain}{tall mountain}

Formally, then, \naadjs and \iadjs are more distinct from each other than either is from Nouns or Verbs respectively.

According to \citet{uehara-1995-syntactic} and traditional accounts, \iadjs and verbs are closed class, in contrast to \naadjs and nouns, which are open class. However, a recent survey \citep{yong-gyun-et-al-2013} found that new \iadjs have been entering the language at an increasing rate in the past century and a half, including loanwords like {\em abui} (``abnormal'') and {\em emoi} (``emotional'')\footnote{Japanese publisher Sanseido's Word of the Year in 2016 \citep{sanseido-2016}.}, suggesting that the class is less closed than traditionally thought.

These categories are not necessarily strictly dichotomous, but rather have fuzzy boundaries. \citet{uehara-2003-diachronic} showed in his sample that as many of 70\% of \naadjs exhibit nominal behaviour in some contexts, such as being used with the nominal attributive marker {\em -no} rather than {\em -na}. Further, the boundary between \iadjs and \naadjs itself is not rigid; some stems can be used as either class, like {\em tisa} (``small''), {\em ooki} (``big''), or {\em atataka} (``warm''). \citet{tariq-2018-fuzzy} performed a corpus study on social media which suggested that there might be more fluidity between the two classes in practice, particularly for infrequent or long adjectives. Nevertheless, ambiguity between \iadjs and \naadjs is quite limited; most adjectives belong clearly to one class or the other.

Despite their clear formal differences, prior work has struggled to find a clear semantic distinction between \iadjs and \naadjs. Various semantic distinctions have been proposed. \citet{oshima-et-al-2019-gradability} found that {\em na-} properties and {\em i-}properties both tend to be gradable, but properties that take {\em -no} in modification (like nouns) tend not to be gradable. \citet{morita-2010-internal} relates it to semantic hierarchies of adjectives, but finds mixed results (e.g. colours are split between the two classes). \citet{uehara-1995-syntactic} conducted a survey proposing a persistent--transitory distinction between the two classes, but failed to find a significant correlation in corpus data. Overall, semantic accounts of the distinction have proven inconclusive. While \citet{uehara-2003-diachronic} provides a compelling diachronic account of the origin of the two classes, suggesting that almost all \naadjs arose from nouns through the recruitment of locational modification constructions, the prevailing view is that there is no synchronically relevant non-formal distinction between the two classes.\footnote{\citet{backhouse-1984-have} claims the distinction is purely phonological in the native (non-loaned) lexical stratum, analogous to the distinction between adjectives which inflect for degree in English (``hard'') and those that do not (``difficult''). \citet{uehara-1995-syntactic} finds Backhouse's generalization holds for a large portion of adjectives, but suggests that it is due to diachronic factors around the phonological structure of nouns and verbs, rather than representing a synchronic phonological distinction.}
\subsection{Method}
I use the models and methods introduced in Chapter~\ref{ch:grounded} to compute visual groundedness scores. Groundedness is formally defined as the pointwise mutual information between a word/linguistic unit in the context of an utterance, and the meaning of that utterance. I focus on {\em visual groundedness}--representing meaning with an image. As a reminder, for an image $I$ and word $w_t$ in an utterance $W = w_1,w_2,w_3...w_t...$, we formalize groundedness as:
\begin{align} \label{pmi2}
  \text{Groundedness}(w_t) = \log p(w_t \mid I, \mathbf{w}_{<t})  - \log p(w_t \mid \mathbf{w}_{<t}),
\end{align}

Which allows us to compute groundedness as a {\em difference in surprisal} between an image captioning model and a (domain-matched) language model.
% This allows us to compute groundedness as a {\em difference in surprisal} between an image captioning model and a (domain-matched) language model. In contrast to typical psycholinguistic norms like concreteness and imageability, groundedness is computed at the (word) {\em token} level. This implies the same word may be more or less grounded in different contexts.

% The simplifying assumption of treating an image as meaning makes estimating (visual) groundedness with existing datasets and neural models tractable, and has interesting connections to relevant notions like imageability and concreteness. % TODO: expand a bit

We focus on three datasets: the Japanese subsets of COCO-35L and Crossmodal-3600 \citep{thapliyal-et-al-2022-crossmodal3600}, and STAIR \citep{yoshikawa-etal-2017-stair}. Each of these datasets consists of images paired with one or more captions. COCO-35L is machine-translated from English using Google's translation service (c.a. 2022), but STAIR and Crossmodal-3600 are human-captioned by native Japanese speakers. Importantly, STAIR is a Japanese re-captioning effort for COCO, so the same images are captioned manually in STAIR that were captioned automatically in COCO-35L. I consider two splits of STAIR: STAIR-dev, which is a set of captions for exactly the same images as COCO-35L-dev, and STAIR-dev-full, a larger split of STAIR that includes additional images from the COCO dataset. This allows me to consider the effect of caption quality and human choice on groundedness estimates for \iadjs and \naadjs. For COCO-35L and Crossmodal-3600 I use the groundedness scores computed in Chapter~\ref{ch:grounded}, while for STAIR I compute the scores using the same methods and models to ensure comparability between the datasets.
%: I use PaliGemma as an image-captioning model, and the fine-tuned PaliGemma language model with matched training data to the captioning model to achieve comparable surprisals between the two models for the PMI estimates, as argued in Chapter~\ref{ch:grounded}.

All datasets are first tagged by the Stanza part of speech tagger to coarsely identify adjectives. However, because this tagger doesn't support the Japanese-specific classes of \iadjs and \naadjs, I use the Sudachi part of speech tagger \citep{takaoka-et-al-2018-sudachi}, as implemented in the \texttt{sudachipy}\footnote{https://pypi.org/project/SudachiPy/} Python package, to tag identified adjectives with these fine-grained labels. I use this two-stage approach because, while, to my knowledge, Sudachi is the best performing tagger for Japanese that supports \iadjs and \naadjs, it is a simpler, rule-based model, and its overall POS tagging accuracy is much lower than Stanza's (73.7\% vs. 95.8\%--though note the datasets and tagsets are not directly comparable). Manual inspection revealed that all \iadj and \naadj lemmas identified by Sudachi were correctly classified---as expected given the large differences between the classes in terms of form and formal distribution.

As noted in Chapter~\ref{ch:grounded}, single groundedness estimates can be noisy, so we filter for only adjective types which occur at least 5 times in our corpus. This is especially important as \naadjs are less frequent than \iadjs in our corpora.

\paragraph*{Statistical model} As our datasets are unbalanced and we have multiple observations per word type, we use a linear mixed effects model to estimate the effect of word class on groundedness. We include fixed effects for word class (\iadj vs \naadj).

I have found that position often has idiosyncratic effects on groundedness (e.g. first tokens having a unique groundedness distribution), so I include it as a categorical fixed effect. This control is conservative; positions may not be uniformly distributed across word classes due to their distinct distributional properties, so in the presence of a true effect of word class, this positional control may reduce the estimated effect size.

Finally, I include a random intercept for word type to account for repeated measures. This very strong control allows each word to have its own baseline groundedness, with the only restriction being that all these intercepts are drawn from the same distribution. This accounts for the fact that we have repeated measures for each word type, and that our dataset might be biased towards certain types of words. A significant effect in this regime suggests that even if we had a different sample of word types, we would see the same effect. % TODO: better justification
We fit this model using the \texttt{nlme} package in R \citep{pinheiro-et-al-2025-nlme}.

\subsection{Results}

\begin{table*}[ht]
  \centering
  \small
  \begin{tabular}{rcccccccc}
    \toprule
    &&&\multicolumn{2}{c}{Types} & \multicolumn{2}{c}{Tokens} \\
    Dataset & MT? & \# Captions & \textit{i-} & \textit{na-} & \textit{i-} & \textit{na-} &\shortstack{bits \\ Effect(\textit{na-})} & $p$-value \\
    \midrule
    COCO-35L-dev & Yes & 5316 & 55 & 56 & 4060 & 1655 & 0.16 & 0.68   \\
    XM3600       & No  & 2810 & 42 & 26 & 3058 & 399  & \textbf{\underline{0.90}} & \textbf{\underline{0.029}}  \\
    STAIR-dev    & No  & 6139 & 60 & 33 & 6292 & 632  & 1.07 & 0.12  \\
    STAIR-full-dev   & No  & 51805 & 142 & 142 & 52828 & 6424 & \textbf{\underline{0.94}} & \textbf{\underline{0.015}} \\ % TODO: update values

    % TODO: compare to text corpus
    \bottomrule
  \end{tabular}
  \caption{Differences in groundedness between adjective classes across datasets. ``MT?'' indicates whether the captions were machine-translated from English. The effect size is the increase in groundedness (in bits) associated with \naadj-hood, estimated using a linear mixed effects model with fixed effects of word class and position and a random effect for word type. Overall, \naadjs tend to be more grounded than \iadjs. (\textbf{\underline{Significant results}})}\label{tab:naadj_results}
\end{table*}

Results are shown in Table~\ref{tab:naadj_results}. We observe a consistent trend of higher groundedness across all datasets for \naadjs as opposed to \iadjs, though this trend is not significant in all datasets. However, the estimated effect size is remarkably consistent across STAIR and XM3600, hovering around 1 bit. The exception is COCO-35L, where the effect is very small and not significant ($p=0.68$, $\beta=0.16\pm0.29$). COCO-35L was produced by machine translation from English. Thus, their selection of when to use \iadjs and \naadjs to describe images is likely to be heavily influenced by the English captions, which were not made with awareness of such a distinction. In contrast, the other datasets were captioned manually by native Japanese speakers. We get some indication of this difference by looking at the number of \iadj and \naadj tokens in each dataset. In COCO-35L, \naadjs make up 29\% of adjective tokens, while in the three other samples, \naadjs make up 9--12\%---so \naadjs are over-represented in the captions translated from English. COCO-35L-dev and STAIR-dev caption the same images, so we can directly compare their results. While in neither case do we find a significant effect on this set of images, the estimated size of the effect is much larger in STAIR-dev ($\beta=1.07\pm0.68$, $p=0.12$) than in COCO-35L-dev ($\beta=0.16\pm0.29$, $p=0.68$). Finally, STAIR-full-dev, which includes additional images captioned by native speakers, shows a significant effect ($p=0.015$, $\beta=0.94\pm0.39$), again with an effect size similar to XM3600 ($p=0.029$, $\beta=0.90\pm0.41$) and STAIR-dev.

\paragraph*{Decomposing groundedness} Two terms are used to compute our visual groundedness measure: surprisal under a language model and surprisal under an image captioning model. While we have found a consistent effect of adjective class on groundedness, this could correspond to several different underlying patterns. It could be that \naadjs are more surprising in the linguistic signal, but become equally surprising to \iadjs when the image is provided (that is, adjective class predicts language model surprisal, but not captioning surprisal). Alternatively, \naadjs and \iadjs could become more predictable than \iadjs when the image is provided (class predicting captioning surprisal), which might drive the groundedness effect. We carried out the same mixed effects analysis as before, but with captioning surprisal and LM surprisal as the dependent variables. These results are shown in Table~\ref{tab:naadj_decomp}.

Generally, we do not find significant effectgs of adjective class on either LM surprisal or captioning surprisal alone. Our estimates suggest that \naadjs tend to be more surprising in the language model than \iadjs, in line with their overall lower frequency, but this effect is only at $p<0.05$ in STAIR-full-dev, our largest dataset. However, this surprisal difference is not reflected in the captioning surprisal in 3 out of 4 datasets. This suggests that the greater groundedness of \naadjs is driven by their greater surprisal in the language model, which is then largely mitigated by the image information in the captioning model. On COCO-35L-dev, where we saw the least evidence for a groundedness difference, we see that \naadjs are significantly more surprising under the captioning model as well, suggesting that the image information does not mitigate their greater surprisal in the language model. This may be related to the unnatural use of \naadjs in COCO-35L, as discusssed above.

\begin{table*}[ht]
  \centering
  \small
  \begin{tabular}{rccccc}
    \toprule
    &&\multicolumn{2}{c}{LM surprisal} & \multicolumn{2}{c}{Captioning surprisal}\\
    Dataset & MT? & bits Effect(\textit{na-}) & $p$-value & bits Effect(\textit{na-}) & $p$-value \\
    \midrule
    COCO-35L-dev   & Yes & $1.34\pm0.71$ & 0.063 & \underline{\textbf{1.15$\pm$0.46}} & \underline{\textbf{0.014}} \\
    XM3600         & No  & $1.13\pm0.78$ & 0.154 & $0.278\pm0.61$ & 0.65  \\
    STAIR-dev      & No  & $2.01\pm1.15$ & 0.085 & $0.96\pm0.78$ & 0.22  \\
    STAIR-full-dev & No  & \underline{\textbf{1.26$\pm$0.58}} & \underline{\textbf{0.030}} & $0.35\pm0.40$ & 0.38 \\

    % TODO: compare to text corpus
    \bottomrule
  \end{tabular}
  \caption{The effect of adjective class on LM surprisal and captioning surprisal. We find that \naadjs tend to be more surprising in the language model than \iadjs, but this effect is reduced by conditioning on the images, resulting in higher overall groundedness. (\textbf{\underline{Significant results}})}\label{tab:naadj_decomp}
\end{table*}

%Is the association between groundedness and the word class distinction above primarily due to one of these terms? Of particular concern is the first term: perhaps \naadjs are just {\em a priori} more surprising in the linguistic signal (e.g. expressing lower-frequency concepts). If we find a strong correlation between word class and LM surprisal, it may be that the information provided by the image is dominated by these effects. Fitting the same fixed and random effects as before to instead predict LM surprisal, we do not find a significant effect ($p=0.133,\beta=1.17\pm0.77$). Similarly, we do not find a significant effect of word class on the captioning surprisal alone ($p=0.591,\beta=0.38\pm0.61$). So it is only through the interaction between these two factors (groundedness) that an association with word class emerges.

\subsection{Discussion}

Overall, our results suggest that \naadjs express more visually grounded meanings than \iadjs in Japanese. This is in line with the formal similarities of \naadjs to nouns and \iadjs to verbs, as nouns tend to be more grounded than verbs cross-linguistically. This finding contrasts with prior work which failed to find evidence for a semantic distinction between these classes \citep{morita-2010-internal, oshima-et-al-2019-gradability, uehara-1995-syntactic}, suggesting that visual groundedness may be a useful tool for uncovering semantic distinctions that are not easily captured by traditional semantic features.

The results suggest that both categories of adjectives have similar levels of predictability when the image is provided, but \naadjs are more a-priori surprising. This suggests that \naadjs are used to express properties which are less frequent and more specific, but still highly salient in the visual context.

While STAIR-dev and COCO-35L-dev caption the same images, XM3600 and STAIR-full-dev cover very different image distributions and were captioned by different people, so the similarity between the findings in these datasets is encouraging. The differences between COCO-35L-dev and STAIR-dev suggest that naturalistic use of \naadjs results in a stronger groundedness effect, as COCO-35L was machine-translated from English captions which do not make the \iadj/\naadj distinction.

In a few instances, there are closely-related \iadj and \naadj lemmas which appear in the datasets. For example, with colour terms, both {\em akai} (\iadj, red) and {\em makka} (\naadj, completely red) appear. Figure~X shows the groundedness scores for these two words in STAIR-full-dev. We can see that {\em makka} has consistently higher groundedness scores than {\em akai}, suggesting that even for very similar meanings, \naadjs exhibit higher visual groundedness than \iadjs. We observe a similar pattern with other closely-related pairs, such as {\em shiroi} (\iadj, white) and {\em masshiro} (\naadj, completely white) or {\em koudai} (\naadj, vast) and {\em hiroi (\iadj, wide)}.
\begin{figure}
  \centering
  \includesvg[width=0.9\textwidth]{figures/splitlump/makka}
  \caption{Groundedness scores for \naadj\xspace {\em makka} (completely red; right) and \iadj\xspace {\em akai} (red; left) in the STAIR-full-dev dataset.}
\end{figure}

While it is clear that the \naadj/\iadj distinction is not syncronically purely semantic, our results suggest that visual groundedness plays a role in how these classes are organized and used by speakers. This finding supports the broader hypothesis that groundedness plays a role in how word classes are organized cross-linguistically. Future work should explore other idiosyncratic splits in word classes across languages to see if similar groundedness effects are observed.

\section{The Tensedness Correlation}

In the previous section, I showed evidence from Japanese that groundedness could provide a novel explanation for seemingly idiosyncratic {\em splits} within the major word classes. Could groundedness also account for similarities or lumping behaviour among the major word classes?

Forming a quantitative hypothesis for testing whether groundedness is operative in how word classes split was relatively straightforward. If there are multiple classes for a single class, it follows that an influence of groundedness on word class structure implies a difference in groundedness between those classes. Due to the formal and distributional differences which are constitutive of a split in word classes, these classes are further easily identified with existing classifiers. Finally, the general prototype theory of cognitive science, as has been applied in cognitive linguistics, gave a clear hypothesis for the directionality of the groundedness effects: greater formal similarity to nouns should imply higher groundedness, while similarity to verbs has the reverse implication.

However, identifying a hypothesis for the behaviour of lumped classes is less clear. If a language lumps together verbs and adjectives, or adjectives and nouns, how could this be predicted by groundedness? Simply measuring the groundedness of the combined verb--adjective class, for example, seems potentially tautological: we already know that adjectives, and by extension, property meanings, tend to have higher groundedness than verbs cross-linguistically; therefore, an observation of ``higher''  groundedness with respect to some comparative base (to be specified) is simply to be expected, and such an observation of the mean does not seem on its face informative about why \textit{this language} has organized its part of speech in this way.

Despite this fundamental difference between lumping and splitting, in this section, I expound on a theory from typology and cognitive linguistics which purports to explain a type of ``lumping'' behaviour among the major classes and demonstrate that it can be translated into a number of more specific hypotheses about groundedness, which invoke different interpretations of this hypothesis.

\subsection{The typological finding}
\citet{wetzer-1992-nouny} first noted that in predication, languages rarely employ a unique strategy for adjectives/property words; rather, languages generally fall into two camps: those which encode property predication identically to/similarly to nouns, and those which encode property predication like (intransitive verbs). Wetzer calls such languages ``nouny'' and ``verby'' with regard to adjectivals respectively. As a canonical example of a nouny language, we can consider a language like German:

\digloss[fspostamble=\bfseries,ex,postamble={ (Property predication)}]{Der Mann ist alt}
{The-\textsc{Masc} man is old}
{The man is old.}

\digloss[fspostamble=\bfseries,ex,postamble={ (Nominal predication)}]{Der Mann ist Arzt.}{The-\textsc{Masc} man is doctor}{The man is a doctor.}

\digloss[fspostamble=\bfseries,ex,postamble={ (Verbal predication)}]{Der Mann luft.}{The-\textsc{Masc} man walk-\textsc{Pres.3Sg}}{The man is walking.}
As we can see, the same strategy (in German, copular marking) is used for nominal predication, but not On the other hand, Mandarin Chinese is a canonical verby language \citep[p. 148, 143]{li-et-al-1981-mandarin}:
\digloss[fspostamble=\bfseries,ex,postamble={ (Nominal predication)}]{Zh\={a}ngs\={a}n sh\`{i} yi-ge h\`{u}shi\`{i}.}{Zhangsan \textsc{cop} one-\textsc{clf} nurse}{Zhangsan is a nurse.}
\digloss[fspostamble=\bfseries,ex,postamble={ (Property predication)}]{t\={a} p\`{a}ng}{\textsc{3sg} fat}{She is fat.}
\digloss[fspostamble=\bfseries,ex,postamble={ (Verbal predication)}]{t\={a} yuy\v{o}ng}{\textsc{3sg} swim}{She swims.}
% \digloss[fspostamble=\bfseries,ex,postamble={ (Adjectival predication)}]{Dia bahagia}
% {\textsc{Pron.3G} happy}
% {They$_\textsc{Sg}$ are happy.}

% \digloss[fspostamble=\bfseries,ex,postamble={ (Nominal predication)}]{Dia adalah guru.}{\textsc{Pron.3Sg} \textsc{Cop} teacher}{They$_\textsc{Sg}$ are a teacher.}

% \digloss[fspostamble=\bfseries,ex,postamble={ (Verbal predication)}]{Der Mann luft.}{The-\textsc{Masc} man walk-\textsc{Pres.Sg}}{The man is walking.}
\citet{wetzer-1996-nouniness}, and subsequently and more comprehensively \citet{stassen-1997-intransitive}, identify that most languages (85\% in \citet{stassen-1997-intransitive}'s sample of 410 languages)
exhibit only a single strategy for all adjectives--either nouny or verby.\footnote{Given that these strategies do not cover all constructions involving adjectives, it is not ``lumping'' in the sense of those that seek to identify if a language ``has'' or ``lacks'' adjectives. However, such questions fall prety to what \citet{croft-2001-radical} calls ``methodological opportunism'': the fact is that it is not clear in which constructions adjectives need to differ from nouns and verbs to count as a distinct class. Rather than studying whether a language ``has'' or ``doesn't have'' adjectives, I am studying whether the similarity of adjectives in key constructions to other classes reflects something about their groundedness.} The remaining languages exhibit some kind of {\em mixed} strategy (Japanese representing an extreme of this type of language).

Wetzer and Stassen show extensive cross-linguistic evidence for what they call the \textbf{{\em Tense}} or \textbf{{\em Tensedness Correlation}} (going forward, I will refer to it as Stassen does, as the ``Tensedness Correlation''). They define the typological parameter of ``Tensedness'' for a language. A language is \textbf{tensed} if it has obligatory morphologically bound marking which distinguishes (at least) between past and non-past time reference. If such marking does not exist, it is expressed as something other than a bound form, or it is not obligatory, the language is non-tensed. The Tensedness Correlation claims:
\begin{enumerate}
  \item A language is nouny if and only if it is tensed.
  \item A language is verby if and only if it is non-tensed.
\end{enumerate}

\citet{stassen-1997-intransitive} shows overwhelming cross-linguistic evidence for this claim. While exceptions exist, in the vast majority of cases we see a bidirectional relationship of tensedness and nouny coding of adjectives in predication. \citet{stassen-1997-intransitive} and \citet{wetzer-1996-nouniness} argue extensively that exceptions to this generalization can largely be understood as artefacts of recent diachronic changes in languages or cases on the margins of being a tense system. For example, the \iadj category in Japanese uses verby encoding, but Japanese in the present day seems to have a tense system, though its tense properties are more recent and the status of tense as opposed to aspect in the language is a matter of some debate \citep{}.

\subsection{Theoretical explanation of the finding}
While the typological finding is widely considered to be robust, on its own it lacks motivation--why should it be that these factors are correlated?

Situated in the cognitive linguistics literature around the prototype and continuum structure of parts of speech I summarized in Section~\ref{sec:lexcontinua}, both \citet{wetzer-1996-nouniness} and \citet{stassen-1997-intransitive} focus on the dimension of {\em time-stability}. Drawing on \citet{givon-1979-understanding}, they argue that adjectives/properties represent an intermediate level of time-stability between nouns and verbs. \citet{stassen-1997-intransitive} gives a particularly detailed argument that in many languages with some degree of mixed encoding for properties, more time-stable properties more likely to be encoded nounily. \citet{wetzer-1996-nouniness} argues that given the intermediate time-stability of properties, the Tensedness Correlation reflects the prototypes of verbs in different languages. Specifically, Wetzer claims that languages that have a more stative, temporally extended, and stable verbal prototype reflect this through their lack of tense marking, while languages that conceptualize verbs as more time-bound and less stative reflect this through their obligatory morphological tense.

Stassen does not directly invoke a conceptual verbal prototype, but makes a similar argument. \citet{bybee-1985-morphology} argued that morphological boundness reflects the {\em semantic relevance} of a morpheme to the stem. Events (the prototype of verbs), as the least time-stable predicate type, ``attract'' bound tense morphemes, in Stassen's view. This is, he argues, a more specific instantion of \citet{haiman-1980-iconicity}'s {\em Structural Iconicity}: the tendency of linguistic structure to reflect the conceptual structure of human experience. Obligatory, bound tense marking is iconically motivated by a conceptual closeness/entanglement between an event and its location in time. He goes on to argue that, for prototypical properties (e.g. forms, dimensions, colours)--bound tense marking is at best non-iconic, and possibly `anti-iconic': given their time-stability, marking them with tense is inappropriate. That is, rather than a shift in verbal prototype per se, he sees the emergence of bound tense marking as a boundary which initiates a process of kicking property meanings out of the verbal category and towards a new, more noun-like expression.

\subsection{Methodological background}

With this theoretical groundwork laid, I will now argue for an analogous testable hypothesis about visual groundedness.

\subsubsection{Shifted prototypes}
While Wetzer argues explicitly for a shift in the time stability of a verbal prototype towards nouns, such an explicit argument is lacking from Stassen's exposition. Stassen removes the causal role of the prototype shift in the Tensedness Correlation, replacing it with the interacting, conflicting forces of iconicity for the temporal specificity of events and the temporal extendedness of prototypical properties.

I do not wish to present a picture in which Wetzer argues for a prototype shift and Stassen argues against it. If the verbal prototype represents some kinds of summary of the types or tokens in the verbal class, Stassen's argument, I argue, also suggests some shift in the prototype of verbs. First, though verby encoding is not the same thing as adjectives being morphosyntactically undistinguished from verbs, in many verby languages this is (roughly) the case. In such a case, the ejection of adjectives from the verbal class proper should shift the verb prototype. Further, verbs can very substantially in their temporal stability: prototypical verbs are punctual, like {\em jump,} {\em kick,} or {\em hit;} however, verbs can be durative to varying degrees, like {\em rain}, {\em dwell}, {\em believe}, and {\em sit} in English. Durative meanings should also be more likely to abandon a verbal encoding if tensedness is required.\footnote{Factors such as relationality (e.g. the transitivity of dwell and believe, which is non-prototypical for properties) can block this transition.} Overall, such individual attritions, could, over time, accumulate into a shifted underlying verbal prototype in time-stability.

The typological evidence and argumentation I have presented has focused on the temporal stability dimension of the noun--adjective--verb cline, as this fits cleanly with the notion of tensedness. However, given the issues with temporal stability of the spectrum discussed in Section~\ref{sec:lexcontinua}, and the positive findings in Japanese with groundedness in contrast to previous negative findings with temporal stability, I propose investigating the Tensedness Correlation from the perspective of groundedness. I argue that the same logic applies: if a language has a shifted verbal prototype towards more grounded meanings, this should be reflected in both the absence of bound tense marking and the encoding of properties as verbal.
% \subsubsection{Why groundedness and not temporal stability?}
% The typological evidence and argumentation I have presented has focused on the temporal stability dimension of the noun--adjective--verb cline, as this fits cleanly with the notion of tensedness.

% % However, the temporal stability argument is not without its holes. The first, as mentioned in the previous section, is ``naturally transitive states,'' which tend to pattern with verbs, plausibly because of their valency/relationality. Additionally, \citet{verkerk-et-al-2008-encoding} point out the existence of very non-temporally extended nouns, like {\em lightning}.

% Therefore, I claim that the observed cross-linguistic Tensedness Correlation provides compelling data for the idea that there is at least some shift in the verbal groundedness prototype.

\subsubsection{Measuring a groundedness shift}
Comparing surprisals across languages is fraught with complexity. Previous studies have occasionally assumed surprisal
First, it must be noted that because we are unable to train independent language and captioning models on multi-parallel data, the raw surprisals (and thus groundedness scores) of our model may not be comparable.

If the models were trained on a parallel corpus, then the only difference between the models should be the differences in the way a language encodes the same content. However, when the corpus is not parallel, the models have different exposure to words, constructions, and concepts, and so may learn different distributions. If we assume there is an underlying ``true'' distribution for a given language that these models are approximating, then as the quantity of data grows, this effect should diminish. However, we must definitely take this possibility seriously here, as the languages in our sample vary widely in their resource level, and the quality of captions the model is trained and evaluated on may also vary (both because of differences in the quality of machine translation, and differences between captioners in the XM3600 dataset).

% Indeed, evaluative comparison is also tricky here; CIDEr scores (the standard evaluation metric for image captioning models) don't compare properly across languages, because they are based on $n$-grams, so they are sensitive to the amount of information borne by an orthographic word in a language (which varies considerably).
Identifying which languages might have worse models is also tricky. The full language composition of the multilingual pretraining corpus for PaliGemma (WebLI) is not public, so I cannot directly measure the amount of data per language. Further, while we have CIDEr scores for the captioning model on each language's test set, these are not directly comparable across languages, as CIDEr is based on $n$-grams, so they are sensitive to the amount of information borne by an orthographic word in a language (which varies considerably)\footnote{Accordingly, I observe the lowest CIDEr scores in the dataset for Finnish.}. There is also the issue of {\em language relatedness}: modelling of less-resourced languages may be improved by transfer from related higher-resource languages in the pretraining corpus, which complicates the relationship between resource level and model quality. Additionally, orthography may play a role: languages with non-Latin scripts may be disadvantaged by suboptimal tokenization in the multilingual model.

\paragraph*{Control variables} To address these concerns, I introduce a few (imperfect) controls. First, I include a binary variable in my statistical model indicating whether the language is \textbf{written in a Latin script or not}. This should enable us to understand how much of the cross-linguistic variation in groundedness could be related to orthographic differences. We would also like to control for model quality more generally. While the word is not a comparable unit across the languages in the study, the data is parallel at the level of sentences, so sentence-level metrics should be comparable. I use the ratio of sentence-level negative log-likelihood (NLL) under the captioning model to that under the language model as a proxy for model quality---the smaller this ratio, the larger the effect of the image on surprisal:
\begin{equation}
  \text{Quality Ratio} = \frac{\text{NLL}_{\text{Captioning}}}{\text{NLL}_{\text{Language Model}}}
\end{equation}

This measure was chosen because it is independent of the absolute surprisal values, and incorporates both language model and captioning model performance. Intuitively, one might think that higher NLL under either the captioning or language model would indicate a worse model, but this is not necessarily the case here. I observe some of the highest NLL values in the dataset for very high-resource languages which also achieve high CIDEr scores (e.g. English, Spanish). This suggests that for some of the lower-resource languages, the model is overconfident in its predictions, leading to {\em artificially low} NLL values (e.g. we observe the lowest sentence-level NLL for Telugu, which was designated as one of the five lowest-resource languages in the corpus by the authors of XM3600). The ratio metric we use here is independent of the absolute NLL values, and captures how much of the surprisal is being explained by the image. A lower ratio indicates that the image is having a larger effect on surprisal, which should indicate better captioning and language models. I use the ratio as computed over sentences in COCO-35L-dev as a control variable, as the captions in this set are direct translations across languages.

Finally, another factor that could influence verbal surprisal specifically is \textbf{word order}. In languages where the object proceeds the verb, this could make verbs more predictable from the linguistic context when the object is prototypically associated with the verb. Therefore, I include a binary variable indicating whether the language has Object--Verb or Verb--Object word order.\footnote{I code \texttt{fa}, \texttt{te}, \texttt{ko}, \texttt{hi}, and \texttt{tr} as Object--Verb languages, based on \citet{wals}.}

\paragraph*{Relative shift} While the previously mentioned controls should help us assess the true effect of nouny/verby encoding on verbal groundedness, it looks only for evidence of an {\em absolute increase} in verbal groundedness in verby languages. However, the hypothesis could manifest more weakly as a difference in the groundedness of verbs in a language {\em relative to other parts of speech}. To test this idea, I perform Z-score normalization on the groundedness scores in each language, measuring how many standard deviations away a word token is from the mean overall groundedness of tokens in a language. Then, an estimate of the groundedness dimension of the verbal prototype was computed as the token-wise average of groundedness within the class, as in Chapter~\ref{ch:grounded}.

\paragraph*{Boundary between verbs and adjectives} Finally, it is worth noting that the UPOS\footnote{Universal Parts of Speech; the categories utilized in Universal Derivations which are deployed by the Stanza tagger we use in these analyses.} verb category may not perfectly capture the verbal prototype in all languages. In particular, in verby languages, adjectives are often very similar to verbs formally and distributionally, and it is possible that some legitimate members of the verbal prototype are being tagged as adjectives instead of verbs. This could artificially lower the estimated groundedness of the verbal prototype in verby languages.

For example, in Korean, the vast majority of adjectives behave as a type of verb in general (in attribution as well as predication, e.g.), yet in Universal Dependencies (and consequently Stanza), these are always annotated as adjective. Some of these ``adjectives'' could be more stative verbs. This could be further compounded by tagger behaviour--the less formally distinct verbs and adjectives are, the easier it becomes to mis-tag adjectives as verbs. Some (potentially poorly defined, and unknown) amount of members of the verb class could be getting ``lost'' to the adjective class in the annotation scheme used here, and it is possible that an analysis that carefully identified them would demonstrate that verby languages {\em in fact} have a more grounded prototype.

In this final experiment, I combine the UPOS categories of verb and adjective {\em for the verby languages only}.   Adjectives are in general more grounded than verbs, so adding them to the verbal prototype should make verbs more grounded than languages which do not include them. This result should provide an upper bound on the true groundedness estimate of the verbal prototype in verby languages in this dataset. The true effect of verby encoding should lie somewhere between the results of this analysis and the previous one. Relatedly, a finding that verby languages still have lower groundedness even with adjectives included would suggest that the effect of model quality is disproportionately affecting verbs, rather than resulting from an artefact of misclassification of parts of speech.
% \subsubsection{The link between nouniness, verbiness and lumping}

\paragraph*{Nouny and verby languages} We follow \citet{stassen-1997-intransitive}'s analysis of which languages are nouny and verby. Out of the sample, Hindi (\texttt{hi}), Indonesian (\texttt{id}), Chinese (\texttt{zh}), Korean (\texttt{ko}), and Vietnamese (\texttt{vi}) are classified as verby languages. Japanese, having a mixed strategy, is excluded from this analysis. The remaining 24 languages in the sample are classified as nouny languages. Notably, Stassen identified Korean as a slightly problematic case, as it meets his criteria for being tensed, but has clear verby encoding of adjectives. He suggests this is due to recent diachronic changes around tense in Korean.

\subsection{Results}

\begin{figure*}[p!]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/splitlump/verb.pdf}
  \caption{Groundedness of the verbal categories across the 30 languages in this study. Error bars represent standard error in the mean groundedness across the datasets considered (COCO-35L, XM3600, and Multi30K where available). Contra theoretical predictions, verby languages do not exhibit higher mean groundedness of verbs, but are somewhat below average. However, this effect is confounded by model quality issues, as suggested by the lower groundedness of verbs in non-Latin script languages.}
  \label{fig:nounyverby}
\end{figure*}

Figure~\ref{fig:nounyverby} shows the result of the absolute groundedness analysis for verbs across the 29 languages in the sample. I fit a mixed effect model with a random effect for dataset, and fixed effects for the quality ratio, word order (OV vs. VO), and script (Latin vs. non-Latin). This model supports a small effect of verby languages exhibiting {\em lower} verbal groundedness than nouny languages ($\beta=-0.22\pm0.11, p=0.046$). However, we see a clear effect of quality ratio ($\beta=-0.27\pm0.05, p<0.001$), and effects of script ($\beta=0.34\pm0.11, p=0.002$; Latin script) and word order ($\beta=-0.26\pm0.11, p=0.027$; OV order). However, AIC does not support the inclusion of the verby/nouny variable (AIC: 61.17 with vs. 60.85 without; relative likelihood: 0.85). This is {\em counter} to the theoretical prediction that verby languages should have a {\em more} grounded verbal prototype than nouny languages. However, we find much stronger support for the negative effect of a language being written in a non-Latin script (AIC: -18.5), and find little remaining predictive effect after this simple heuristic for languages where the model struggles more (AIC: -20.5 with vs. -18.5 without). These results do not suggest that there is a difference in the average groundedness of the verbal prototype between nouny and verby languages.

\paragraph*{Z-scored groundedness} While the previous experiment did not show a clear difference between nouny and verby languages in terms of absolute groundedness after controlling for model quality, we might find an effect on the {\em relative} groundedness of verbs in these languages. Figure~\ref{fig:verbz} shows the results of this analysis. A fixed effects model with the same model formula was applied. The model no longer supports an effect of the quality ratio ($\beta=-0.03\pm0.03, p=0.24$), or word order ($\beta=-0.05\pm 0.07, p=0.23$; OV order). However, I observe a clear effect of script. ($\beta=0.21\pm0.06, p<0.001$; Latin script). This model does {\em not} support an effect of verby/nouny status ($\beta=-0.07\pm0.06, p=0.24$). So I find no evidence that verby languages have relatively more grounded verbs than nouny languages.
% This is {\em counter} to the theoretical prediction that verby languages should have a {\em more} grounded verbal prototype than nouny languages. However, we find much stronger support for the negative effect of a language being written in a non-Latin script (AIC: -18.5), and find little remaining predictive effect after this simple heuristic for languages where the model stuggles more (AIC: -20.5 with vs. -18.5 without). These results mean we cannot reject the null hypothesis that there is no difference in the average groundedness of the verbal prototype between nouny and verby languages. However, the effects of model quality variation, resource level, and script make definitive statements challenging. Because the data has been Z-scored, this is not an effect of lower overall groundedness or less variation in groundedness in the verby and non-Latin-script languages. Rather, the results indicate that verbs are more below average in terms of groundedness in these languages.

\paragraph*{Including ``Adjective'' tags in the verbal prototype} One possible cause for the lack of a groundedness shift in verby languages is the ``loss'' of more stative verbal meanings to the \texttt{ADJ} UPOS tag. To provide an upper bound on verbal groundedness in these languages, I combined the \texttt{ADJ} and \texttt{VERB} UPOS tags for these languages only.  Verby languages do show an increase in groundedness, with Indonesian and Hindi shifting up in the ranking---in line with the higher average groundedness of adjectives. Nevertheless, the relative groundedness of the verby languages is still less overall than the most grounded nouny languages, and I still observe an association with script--notably, now the two most grounded verby languages are exactly those two which use a Latin script. Replicating the same mixed effects analysis (with random effect for dataset) on this data, nevertheless I still only find a significant effect of script ($\beta=0.22\pm0.06, p=0.001$; Latin script), and no effect of verby/nouny status ($\beta=-0.04\pm0.06, p=0.51$). Our results, then, do not suggest that the lack of a groundedness shift in verby languages is due to misclassification of parts of speech.

\begin{figure*}[p!]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/splitlump/verb-z.pdf}
  \caption{Z-scored groundedness of the verbal categories. Error bars represent standard error in the Z-scores across the datasets considered (COCO-35L, XM3600, and Multi30K where available). The results suggest verbs are not {\em relatively} more grounded than other words in verby languages. However, we observe a clear effect of script, with languages written in Latin script exhibiting relatively more grounded verbs.}
  \label{fig:verbz}
\end{figure*}

\begin{figure*}[p!]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/splitlump/verb-adj.pdf}
  \caption{Z-scored groundedness of the verbal categories, with adjectives included for verby languages. Error bars represent standard error in the Z-scores across the datasets considered (COCO-35L, XM3600, and Multi30K where available). Despite the higher groundedness of adjectives than verbs in general, and concerns that legitimate members of the verbal category could be disproportionally ``lost'' to the adjective tag in verby languages, we still observe lower groundedness for the verby languages. This suggests a disproportionate effect of captioning and language model quality on verbs.}
  \label{fig:verbadj}
\end{figure*}

% With this caveat borne in mind, it appears that, counterintuitively, languages with the model either clearly has worse CIDEr performance or for which prior literature says the model likely struggles\footnote{lower-resource languages and languages with non-Latin orthography}, exhibit {\em lower} mean surprisal under {\em both} the image captioning model and the language model. This suggests a degree of over-fitting to the captioning domain, and perhaps a lack of diversity in pre-training data.

% Figure~\ref{fig:nounyverby} shows the result of this analysis for verbs across the 30 languages in the sample. I fit a mixed effect model with a random effect for dataset, finding a moderate amount of support for an effect of verby languages being {\em less} grounded than the mean (AIC: -7.96 vs. -1.63 for the fixed effect alone). This is {\em counter} to the theoretical prediction that verby languages should have a {\em more} grounded verbal prototype than nouny languages. However, we find much stronger support for the negative effect of a language being written in a non-Latin script (AIC: -18.5), and find little remaining predictive effect after this simple heuristic for languages where the model stuggles more (AIC: -20.5 with vs. -18.5 without). These results mean we cannot reject the null hypothesis that there is no difference in the average groundedness of the verbal prototype between nouny and verby languages. However, the effects of model quality variation, resource level, and script make definitive statements challenging. Because the data has been Z-scored, this is not an effect of lower overall groundedness or less variation in groundedness in the verby and non-Latin-script languages. Rather, the results indicate that verbs are more below average in terms of groundedness in these languages.

\subsection{Discussion}
Across all three experiments, we find no evidence that verby languages have a more grounded verbal prototype than nouny languages, contrary to the theoretical predictions based on the Tensedness Correlation. Instead, we find evidence that where models struggle more (as approximated by non-Latin script and the ratio of captioning to language model NLL), verbs are {\em less} grounded both in absolute and relative terms. This effect persists even when adjectives are included in the verbal prototype for verby languages, suggesting that the lack of a  groundedness shift for verby languages is not due to incomparable part of speech tagging.

Rejecting the explanation that I have ``lost'' genuine verbs to the \texttt{ADJ} tag, I am left with evidence that verbs are  ``more difficult'' to ground--poor models have a greater effect on verbs than at least some other parts of speech. This seems plausible--the exact factors identified as candidates for their lower groundedness than nouns (spatial diffuseness, temporal instability, relationality) suggests that learning verbs from images is harder than nouns, and so may take a bigger hit when models are poor. In the face of results which seem to pattern with orthography and training data availability, I cannot draw a strong conclusion about whether an estimate of visual groundedness computed with more comparable corpora would show correlations with tensedness and verby encoding of adjectives.

This highlights a key limitation of the present study: the lack of large-scale multi-parallel image captioning datasets. While such datasets represent a gold-standard for cross-linguistic comparison, they are very expensive to create, and difficult-to-impossible to extend to the ``long tail'' of the world's languages. As such, I caution future studies to be mindful of the confounding effects of model quality when comparing groundedness estimates across languages. This is not to say that we cannot explore typological questions with existing models. The other groundedness analyses in this thesis have been carefully designed to avoid direct cross-linguistic comparison of groundedness scores. In Section~\ref{sec:jaadj}, we compared groundedness within a single language, while in Chapter~\ref{ch:grounded}, we fit a statistical model of cross-linguistic trends {\em within} languages. While these approaches certainly constrain the kinds of questions we can ask, I believe there are still many exciting avenues for typological research on groundedness that can be pursued with existing data and model.\footnote{In Chapter~\ref{ch:conclusion} I lay out a number of such directions.} Further, improvements in multilingual vision--language pretraining may help alleviate some model quality issues I have observed here.

\section{Conclusion}

In this chapter, I have argued that groundedness and meaning content are operant in grammatical organization not only across the lexical--functional divide, but also among the lexical classes of noun, adjective, and verb themselves. Drawing on the literature from cognitive linguistics on the continuum and prototype structure of lexical classes, I demonstrated that some aspects of this continuum are interestingly similar to the lexical--functional distinction. In particular, nouns, adjectives, and verbs vary in their prototypical {\em relationality}, with nouns being the least relational and verbs the most relational. Functional elements are also more relational than lexical elements. In so doing, I propose the study of a unified {\em lexicality spectrum}, which connects variation between lexical classes to functional classes.

Building on the work in Chapter~\ref{ch:grounded}, I have used groundedness as a computational measure of this relationality dimension. I argue that groundedness has the potential to combine dimensions like time-stability and spatial compactness into a single information-theoretic measure. I then aimed to show that groundedness can provide new evidence about variation in lexical class organization cross-linguistically.

Focusing first on Japanese, which has a well-studied ``split'' among its adjectives which has long been argued to be synchronically arbitrary, I showed that the two adjective classes differ in their groundedness when Japanese speakers chose how to describe images. The more formally noun-like \naadjs are more grounded than the more verb-like \iadjs, suggesting that the split still encodes a synchronic lexicality distinction.

Finally, I investigate ``lumping'' behaviour between the lexical classes through the lens of the Tensedness Correlation, which links obligatory tense marking to nouny encoding of adjectives. Building on cognitive-linguistic theories of this correlation, I proposed that the correlation reflects a shift in the verbal prototype away from nouns in tensed languages in terms of groundedness. However, I found no evidence for this hypothesis in a cross-linguistic comparison of visual groundedness of verbs in 29 languages. Instead, I found that verbs are less grounded in languages where the captioning and language models struggle more, suggesting that verbs are more difficult to ground than other parts of speech. This highlights the challenges of direct cross-linguistic comparison of groundedness estimates.

These results are nevertheless promising initial evidence for the role of groundedness and relationality across the whole lexicality spectrum, including up into the lexical classes themselves. Future work should continue to explore these connections, especially in more split-class languages, and with new measures and datasets.
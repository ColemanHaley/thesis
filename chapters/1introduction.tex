
% chapters start like this. You can refer to them using the label like this: \cref{ch:intro}
\chapter{Introduction}
\label{ch:intro}

\def\naadjs{{\em na}-adjectives\xspace}
\def\iadj{{\em i}-adjective\xspace}
\def\iadjs{{\em i}-adjectives\xspace}
\def\naadj{{\em na}-adjective\xspace}

\section{The role of lexicality in linguistic organization}

The distinction between \textsc{Lexical} and \textsc{Functional} linguistic units has played a role in the theory and analysis of language for millennia. This is to say, a distinction can be drawn between two poles for the role that linguistic units play in communication. On one end, we have the \textsc{Lexical}: linguistic signs which carry specific meanings, often referring out to objects or events in the world---nouns like {\em cat} or {\em tree}. On the other end, we have the \textsc{Functional}: signs which do not so much carry specific meanings, but rather serve to organize and clarify the relationships between lexical elements---like the tense marker {\em -ing} or the word {\em to}.

For as long as people have been studying language, this distinction has been connected to the idea that functional elements bear little semantic content. In the Greek tradition, Aristotle distinguished {\em phōn\'{\={e}} sēmantikḗ} (sign-bearing sounds) from {\em phōnḕ ásēmos} (non-sign-bearing sounds), such as the class of {\em árthron} which includes prepositions and preverbs \citep{chl132-133}. This distinction was not limited to the proto-linguistics of Indo-European languages: in the 12th century the {\em Wén zé (文則)} of Chen Kui (陈骥) catalogued {\em zhùchí} 助词 (lit. ``helping words'')--corresponding to what we would today call function words. In the Yǔzhù (1311) Lu Yiwei defines this class as words that do not have a ``precise concrete meaning'' \citep{chl61}, and future authors would adopt the term {\em yǔcí} 语词 (lit. ``empty words'') to refer to this class \citep{chl62}. Further, psycho- and neurolinguistics provide ample evidence for differential processing of lexical and functional elements \citep{segalowitz-et-al-2000-lexical,diaz-et-al-2009-comparison, boye-et-al-2018-grammatical,chanturidze-et-al-2019-prepositions, neville-et-al-1992-fractionating,pulvermuller-et-al-1995-electrocortical,friederici-et-al-2000-segregating}.

% Evidence for the importance of lexicality to human language processing comes not just from the structure of language, but from the psychological and neurological study of language processing itself. Differential psychological processing for inflection and derivation has been observed by \citet{laudanna-et-al-1992-processing} and \citet{kirkici-et-al-2013-inflection}, and while neurological evidence is somewhat mixed, it does point to differential processing between inflection and derivation \citep{lemien-etal-2019-morphological}. At the level of the traditional lexical--functional distinction, differences have been documented in access speed \citep{segalowitz-lane-2000}, brain region activation \citep{diaz-mccarthy-2009}, and lateraliztion \citep{pulvermuller-etal-1995}. Famously, there is also evidence for the indiduation of the lexical and functional from aphasias: on the one hand, we have agrammatic aphasia\footnote{This is sometimes known as Broca's aphasia.}, where lexical access seems intact, but the use of grammatical words and complex structures is impaired; on the other, there are fluent aphasias like Wernicke's aphasia, where speakers produce fluent sentences with rich syntactic structure, but fail to produce sentences that convey content clearly, struggling to access relevant contentful words. These evidence bases have even shed some light on boundary cases, with aphasiac evidence supporting the ``grammatical'' status of light verbs (in Dutch; \citealp{boye-etal-2018-grammatical}), but aphasias and neurological data providing mixed evidence about lexical vs. functional adpositions (in English and German; \citealp{bennis-etal-1983, matzig-spared, chanturidze-et-al-2019-prepositions}). Aphasiac evidence also supports differential processing among lexical classes (specifically between nouns and verbs; \citealp{caramazza-et-al-1991-lexical, thompson-etal-2013-verb, bird-et-al-2003-verbs}).

Despite this long history, the direct study of semantic contentfulness on linguistic structure remains largely pre-theoretical in linguistics, especially in large-scale cross-linguistic study. A major cause of this theoretical lacuna is the difficulty of specifying {\em semantic contentfulness} in a principled, cross-linguistically applicable way. This has led many linguists to avoid this notion entirely, focusing instead on how notions like frequency shape grammatical expression \citep{abunchofhaspelmath}. In this thesis, I seek to address this gap by developing measures of the {\em semantic} and formal properties of the distinction between lexical and functional elements, and applying these measures to traditional conceptualizations of this distinction.

%and investigating {\em how they relate} to traditional ``problematic'' cross-linguistic grammatical distinctions.

% Because the roles served by \textsc{Functional} units are similar to roles that, in some languages, are expressed not through independent units but through structural patterns or rules (that is, through {\em grammar}), such units are sometimes referred to as \textsc{Grammatical} units. For example, in English, whether a word serves as the subject or object of a verb is indicated through word order alone, while in some languages, there are linguistic signs (called, variously {\em case markers}, {\em adpositions}, or {\em flags}) which explicitly mark these relationships.

% As such, for theories which treat grammar as a separate system from the lexicon, functional units present a key challenge, as they straddle the boundary between these two systems, having the realized form of a linguistic sign, but the organizational role of grammar.

% Yet to say the characteristic role of functional units is clarifying the relationship between lexical elements is an oversimplification. Many types of functional elements rather clarify or elaborate on the meanings of individual lexical elements. Adding to the puzzle, however, is a surprisingly high degree of cross-linguistic consistency in the roles of functional elements. For example, while many languages mark number, definiteness, or tense through functional elements, it has been claimed that no languages mark {\em colour} or {\em shape} through functional elements \citep{talmy-2000-vol1}. Linguists have handled this observation of cross-linguistic consistency in different ways. Many generative linguists have hypothesized a rich, innate, universal set of functional heads which are realized in different ways across languages \citep{wilko-2014, rizzi-cinque-2016-functional}. In contrast, cognitive linguists have attempted to find common cognitive motivations for the cross-linguistic consistency of functional elements.

% However, while there is some consistency in what concepts can be expressed as obligatory, paradigmatic, bound markings across languages, there are also serious definitional issues around where the boundary between lexical and functional units lies. To claim that ``only certain concepts can be expressed functionally'' presupposes a consistent definition of what it means to be functional; for this to avoid circularity, the definition of functional must not rely on the concepts themselves, but rather on something about linguistic {\em distribution}.

% Yet the formal and distributional properties of functional expression are far from clear-cut. Typically, these are identified as (I) being \textsc{closed-class}, (II) being \textsc{bound}, and (III) being \textsc{obligatory}. While functional categories are typically closed-class (resisting new members), prototypically lexical categories can also be closed class, like Bemba adjectives \citep{dixon-1977-where} or Jaminjung verbs \citep{pawley-2006-where}. Further, closedness is not a binary property, with closed classes varying substantially both in their size and their resistance to admitting new members. Purported functional elements can vary significantly in their degree of boundness, and indeed ``boundness'' is a complex property of dubious categorical status \citep{haspelmath-2022-what, croft-2019-comparative}, with no consensus on how to define or measure it. Finally, obligatoriness is also gradient, with some functional elements being optional in some or even many contexts. The problem grows only more complex when we consider that the lexical status of a linguistic unit can vary contextually and diachronically.

%For example, in Mandarin Chinese, the intensifier {\em h\v{e}n} (``very'') is undergoing semantic bleaching into a copular function for adjectives \citep[pp. 508--509]{yml-2016-de}:
% \begin{covexample}
%   \digloss{t\={a} h\v{e}n g\={a}o}{\textsc{3sg} very tall}{She is tall.}
% \end{covexample}
% Here, {\em h\v{e}n} has lost much of its lexical meaning of ``very'' and serves primarily to link the subject to the adjective. Grammaticalization processes like this are universal cross-linguistically, and tend to produce boundary cases between lexical and functional units. Examples of such boundary cases include: ``light nouns'' like quantificational nouns (e.g. {\em lot}) or taxonomic nouns (e.g. {\em type}) ; ``light verbs'' like {\em have} in {\em have a rest}; and adpositions \citep{on-light-nouns,semi-lexical}. Typically, the loss of semantic force in these cases is accompanied by changes in formal and distributional properties (decategorization).

% Despite the seemingly central notion that weakness/lack of meaning and the properties of functional units are deeply linked, direct investigation of this relationship has been limited.

While typical discussions of the distinction between lexical and functional units tend to focus on a contrast between lexical words or roots and functional words or affixes, a closely related distinction is drawn {\em within} the domain of morphology between \textsc{Derivation} and \textsc{Inflection}. Morphology described as {\em inflectional} typically have all the prototypical properties of functional units: inflection is typically closed-class, bound, and obligatory, and align closely with ``possible grammatical concepts.'' In contrast, {\em derivational} morphology may express rich and perhaps unconstrained meaning, interacting idiosyncratically with the meaning of roots, and is typically optional. However, there are a few key differences with the lexical--functional distinction at the level of words. First, there are strong cross-linguistic tendencies for derivations to occur closer to the root than inflections do (Greenberg's Universal 28) \citep{greenberg-1966-universals, lieber-et-al-2014-universals}. Second, simple conversions or transpositions of word class (e.g. converting an adjective to a noun: {\em happy}\rightarrow{\em happiness}) tend to pattern more similarly to derivations than inflection (e.g. scoping inside inflections), despite their apparent lack of semantic weight and highly obligatory and productive nature. This has led to substantial debate over whether such morphological constructions are better considered inflection \citep{haspelmath-1996-wordclasschanging} or derivation \citep{tenhacken-1994-defining}. Further, where exactly the boundary between ``transpositions of word class'' and more semantically rich derivations lies is also unclear. For example, the {\em -er} nominalizer in English forms agentive nouns from verbs ({\em teach}\rightarrow{\em teacher}). Again, we have encountered a distinction between two poles that seem to be related to semantic weight, but with unclear boundaries. While derivational morphology tends to be more semantically rich than inflectional morphology, it is typically less semantically rich than lexical roots; however, in highly agglutinative languages like Inuktitut, derivational morphemes can carry semantic content comparable to roots in other languages.

This thesis concerns itself with these divisions between more lexical and more functional linguistic units, at multiple levels of linguistic structure. I treat both the lexical--functional distinction at the level of words and the inflection--derivation distinction at the level of morphology, and connect them to prototype phenomena among the major lexical classes of nouns, verbs, and adjectives. {\em In this thesis, I refer to this gradient of semantic contentfulness at different levels of linguistic structure as the spectrum of {\em \textsc{lexicality}}.}

% In so doing, I argue that a number of phenomena that have been handled disparately in theoretical linguistics can and should be unified under the umbrella of lexicality. I argue
\section{Approach}
\paragraph*{Multiple Levels of Linguistic Structure} This thesis spans a {\em wide range} of levels of linguistic structure. While previous work has largely treated lexicality distinctions at different formal and semantic levels as separate, unrelated problems (e.g. the inflection--derivation distinction at the morphological level; the distinction between lexical and functional word classes at the word level; or the distinctions between the major lexical classes of nouns, verbs, and adjectives), I investigate lexicality across multiple levels of linguistic structure, showing new parallels and connections between them. That being said, I limit my focus to {\em sub-phrasal} linguistic units (morphemes and words), leaving phrases, semantic frames, and more schematic constructions to future work.

\paragraph*{Cross-linguistic investigation} This thesis focuses on the {\em cross-linguistic} consistency of lexicality distinctions.\footnote{While in \cref{ch:splitlump}, I do conduct a language-particular analysis of Japanese word classes, the motivation for this analysis is to investigate whether cross-linguistically validated measures of lexicality can explain unusual language-particular patterns.
} A large body of work in linguistic typology has argued that language-specific categories do not map onto some clean set of universal grammatical categories \citep{haspelmath-2007-preestablished, croft-2001-radical, dixon-1977-where}. Instead, typologists have argued for the importance of cross-linguistically valid {\em comparative concepts}---which need not necessarily map onto the structure of individual language's grammar \citep{haspelmath-2010-comparative, croft-2016-comparative}. Studies that focus on the distinction between inflection and derivation or between lexical and functional word classes which consider only a single language risk conflating language-particular properties and categories with cross-linguistic generalizations. While finding a consistent distinction  between inflection and derivation, or between lexical and functional word classes in an individual language is interesting, it does {\em not} a-priori tell us whether such a distinction has cross-linguistic descriptive value. Thus, this thesis aims to cover a large and diverse sample of languages wherever possible.

\paragraph*{Computational and quantitative methods} To study the cross-linguistic consistency of lexicality distinctions, I take inspiration from the successes of empirical grounding in certain areas of typology (like vowel, colour, and kinship systems; \citealp{schwartz-et-al-1997-dispersionfocalization}; \citealp{zaslavsky-et-al-2019-color}; \citealp{kemp-et-al-2012-kinship}) and from recent advances in deep learning models of language. These models have been shown to learn rich representations of semantics and the world, without requiring direct instruction on this structure, but rather learning it implicitly from learning to predict word sequences associated with a (linguistic and/or visual) context, and have been successfully employed to study a range of linguistic phenomena \citep{petersen-potts-2023-lexical, hamilton-etal-2016-diachronic, matthews-et-al-2025-disentangling, boguraev-et-al-2025-causal,scivetti-et-al-2025-construction}. This capacity makes these tools ideal for operationalizing semantic dimensions of lexicality distinctions.
% Further, in the second part of the thesis I leverage {\em multimodal} models which ground languages in images. This image grounding provides a language- and form-neutral representation of semantics, which enables a new method for separating out contextual contentfulness from formal linguistic predictability.
The computational approach also aids in our goal of cross-linguistic investigation---while psychological and neurological evidence for lexicality distinctions exists, scaling it to typological study is challenging. Computational methods require only corpus data, which enables the simultaneous study of many languages, though it biases the study towards languages with sufficient digital resources. Further, the quantitative focus of this thesis enables the study and quantification of consistency, in contrast with many previous studies that focus primarily on problematic cases.

% One major dimension of linguistic organization is the notion that there are more lexical linguistic units, which express meanings, and more functional linguistic units, which are determined by syntax and/or discourse and serve to organize and clarify the relationships between lexical elements. This dichotomy has been described at levels of linguistic structure and motivates at least two classical distinctions in linguistics. At the level of words, it motivates the so-called lexical-functional distinction, while within morphology, a related distinction is drawn between derivation (which forms new lexical items) and inflection (which produces forms of lexical items). These dichotomies have many noted boundary cases, which have led to many linguists rejecting them, or treating them as gradient.  In this thesis, I refer to this gradient of semantic weight at different levels of formal structure as lexicality.

% There is substantial neurological and psychological evidence for the importance of lexicality to human language processing. Further, lexicality dichotomies also emerge in cross-linguistic trends in grammatical organization, such as asymmetries between inflection and derivation, or between the properties of functional and lexical word classes. Yet the lexicality of a particular linguistic unit varies contextually and diachronically.

% In linguistic practices that proceed from analysis of language-particular data to a language-general analysis, issues of lexicality have played a role of central importance. However, in the functional—typological tradition, which proceeds from cross-linguistic analysis to the language particular, the relationship of this dimension to linguistic organization has had little theoretical impact. A major factor is that typological research must be conducted with cross-linguistically applicable comparative concepts. In this thesis, I leverage deep learning models to produce empirically grounded measures for lexicality, which I argue can serve as interesting and useful comparative concepts for typological study.

% This is an example chapter. You cite like this: \cite{burchell-etal-2023-open}. \Cref{fig:tiredwired} shows an example of a figure.

% One major challenge of typology is the large-scale nature of the problem. While important typological insights have been gained from the study of individual languages with interesting properties, robust typological generalizations generally require data from a wide and diverse set of languages. However, manually collecting and comparing data across many languages is inherently challenging, requiring much time investment and breadth and depth of expertise.

% In recent years, the field has increasingly turned to computational methods to address these challenges \citep{futrell-2015-largescale, cotterell-et-al-2019-complexity, levshina-2020-how, ostling-2015-word, gerdes-et-al-2021-typometrics}. These techniques can serve to automate and systemize manual comparisons done by typologists, enabling new scales of analysis. Large typological datasets such as Universal Dependencies \citep{demarneffe-et-al-2021-universal} and UniMorph \citep{batsuren-et-al-2022-unimorph} which use a hybrid of manual and automatic annotations have facilitated large-scale comparison, and natural language processing (NLP) tools have been used to extract and analyze linguistic features from corpora in a more scalable way.

% These cross-linguistic concepts used for typological comparison (like ``subject,'' ``object,'' and ``verb'' in the previous example) have been the subject of substantial discussion in typology, with typologist debating both the nature and theoretical status of these concepts. Much early work and work in the generative linguistics traditions have sought {\em universal grammatical concepts}, which not only describe the variation across languages, but also have descriptive validity for the grammmars of individual languages (\citealp{greenberg-1966-universals}; \citealp[p. 50]{chomsky-1957-syntactic}; \citealp{newmeyer-2007-linguistic}; \citealp{wiltschko-2014-universal}). A growing body of work has questioned the existence of such universal grammatical concepts, instead attempting to define cross-linguistically valid {\em comparative concepts}---which need not necessarily map onto the structure of individual language's grammar (\citealp{haspelmath-2007-preestablished}; \citealp[pp. 32--34]{croft-2001-radical}).

% \citet{croft-2016-comparative}, following \citet{haspelmath-2010-comparative}, identifies two major types of these cross-linguistic comparative concepts. The first, purely functional concepts, are similar to those argued for by Haspelmath \citep{haspelmath-2010-comparative,haspelmath-2003-geometry,haspelmath-2012-how} Relying only on general semantics and discourse structure, they achieve cross-linguistic validity under fairly weak assumptions about the degree of cross-linguistic semantic relativity. As a prototypical example, take Haspelmath's suggestion to study the cross-linguistic syntax of words which denote ``properties'' instead of ``adjectives.'' In contrast to this first type, Croft identifies ``hybrid'' comparative concepts as the other major type; these concepts have both a formal and a functional component (e.g.,
% ``adjectives''). These categories often have an intuitive appeal stemming from their similarities to language-specific grammatical categories and superficial broad cross-linguistic similarity. However, their hybrid nature makes cross-linguistic consistency incredibly challenging. Form generally varies from language to language: while in English, nouns are words which follow determiners, other languages may not have separate determiner words.

% \textbf{ It is such hybrid comparative concepts with which this thesis concerns itself.} While mainstream typologists have long been debating and investigating these concepts directly, computational work has dominantly employed hybrid comparative concepts while treating them as primitive or well-defined, using datasets such as UniMorph and Universal Dependencies and implicitly following whatever practices diverse dataset creators used to produce categorisations. In this thesis, I seek to directly interrogate hybrid cross-linguistic comparative concepts as instantiated in these typological datasets, and directly measure the cross-linguistic consistency of their application through computational techniques. Contrasting with the dominant paradigm in typology which has qualitatively identified prototypical semantics and formal properties of these concepts, I seek quantitative measures which do not rely on human judgements.

% Besides scalability, another major advantage of the computational approach is the tools it provides for formalising functional and semantic aspects of language. In recent decades, a range of deep learning models of language (including word embedding methods, (vision-and-)language models, and so-called masked language models) have been introduced in the field of natural language processing, which have enabled formal theories to work with more intangible, continuous aspects of language like semantics \citep{copot-et-al-2022-idiosyncratic}. These models learn a representational space which provides rich representations of semantics and the world. Further, they do so without requiring direct instruction on this structure, but rather learn it implicitly from learning to predict words in a (linguistic and/or visual) context. This capacity makes these tools ideal for cross-linguistically operationalizing semantic dimensions of comparative concepts.

% In this thesis, then, I provide the first explicit computational study of the cross-linguistic application of several comparative concepts \textbf{(Contribution 1)}. While some theoretical linguists have argued that these concepts are under-specified and inconsistently deployed, \textbf{(Claim 1)} I find that the application of deep learning models to provide concrete measures of the semantic dimensions of these concepts shows a high degree of cross-linguistic consistency in the way linguists use these concepts.

% Another major contribution of this thesis is the development of a multimodal approach to typological investigation \textbf{(Contribution 2)}. Recent advances in deep learning applications have produced sophisticated cross-lingual joint models of image and text. A central challenge for computational typological investigation of semantics is the difficulty of having semantics which is {\em aligned} across languages. Prior studies attempt to use multilingual models with a shared representation space or to align representation spaces across multiple models. However, these approaches are fairly ad-hoc. An image {\em grounds} language in a language-agnostic model of the world state in which the language was produced. While this representation is necessarily imperfect, it also enables new directions.

% I also demonstrate that the notion of {\em semantic contentfulness} underlies a wide range of hybrid cross-linguistic concepts \textbf{(Claim 2)}. In this thesis, I apply deep learning models to quantify semantic contentfulness and investigate how it underlies and organizes these cross linguistic concepts. Starting with simple, decontextual, word-vector based methods in Chapter 3, I then introduce a new, contextual measure, groundedness, which I investigate in Chapters 4 and 5. Moving beyond more straightforward applications of SSLMs within typology, it uses multimodal language models to provide a model of semantics which goes beyond the distributions of words alone.  In Chapter 5, I demonstrate the potential of my approach to provide new cross-linguistic comparative concepts and insights, showing that groundedness can help explain edge cases of word class organization in a way that existing comparative concepts within typology cannot.
%While emphasising the semantic/functional core of cross-linguistic comparative concepts has been fruitful in traditional typology, not all functional concepts are equally easy for linguists to consistently identify across languages. Concepts like the inflection-derivation distinction or the lexical-functional distinction, rather than invoking specific semantic prototypes, tend to be described in terms of semantic \textit{quality}, rather than kind: being ``more semantically rich/contentful'', or being ``more abstract or variable in meaning.'' The semantic cores of these concepts are inherently scalar, rather than categorical.

% In contrast to work by more traditional typologists, much of this work has treated comparative concepts as primitive or well-defined and avoided interrogating them, using datasets such as UniMorph and Universal Dependencies and implicitly following whatever practices diverse dataset creators used to produce categorisations. In this thesis, I seek to directly interrogate hybrid cross-linguistic comparative concepts as instantiated in these typological datasets, and directly measure the cross-linguistic consistency of their application.

% Linguistic typology seeks to identify the limits of variation within human languages (\citealp{plank-extent-2017}; \citealp[pp. 30--31]{comrie-language-1988}).
% To do so, a frame of \textit{alignment} must be identified, with common concepts identified across languages.

%To try and tease apart the issues with these hybrid comparative concepts, Croft further introduces two subcategories of hybrid concepts. The first, constructions, simply consist of the cross-linguistic comparison of the formal expression of a functional concept: e.g.
% To define such cross-linguistic comparative concepts, typologists rely on some other type of universal concept, often universal semantic notions
%  (cf. \citealp{haspelmath-2007-preestablished, haspelmath-2003-geometry, croft-2001-radical}) or universal formal concepts---which have their own definitional challenges.

% Perhaps one factor underlying the status quo around these cross-linguistic concepts within computational approaches to typology is the difficulty of cross-linguistic semantic alignment and identification at scale. Traditionally done manually by the typologist, the identification and alignment of linguistic functions across languages has been approached computationally

%\textbf{C2} I claim that the application of DMLs is particularly useful for typologists for measuring scalar or quality semantic dimensions. \textbf{C3} Additionally, this research demonstrates that, understood correctly, today's DMLs capture subtle aspects of language beyond either what they have been trained explicitly to model, or what has been claimed in the literature so far.}
% Linguists arguing for the value of particular cross-linguistic concepts have often described intuitions about their properties which are difficult to formalise because of their reliance on functional and semantic aspects of language. In recent decades, a range of self-supervised models of language (including word embedding methods, language models, and so-called masked language models) have been introduced in the field of natural language processing, which have enabled formal theories to work with more intangible, continuous aspects of language like semantics \citep{copot-et-al-2022-idiosyncratic}. However, work in computational typology has often taken these cross-linguistic concepts as primitive or well-defined, or focused more on formal aspects of language.

% As such, this thesis seeks to interrogate several potentially problematic cross-linguistic comparative concepts using methods made possible by advances in self-supervised models of language (SSMLs), formalising linguist intuitions and investigating their cross-linguistic applicability. I aim to demonstrate that in many cases, thorny and under-specified theoretical concepts can become empirically grounded by the application of these SSMLs. This provides convergent evidence for claims made in typology from a radically different source than traditionally used. Additionally, it aims to demonstrate that, understood correctly, today's SSMLs capture subtle aspects of language beyond either what they have been trained explicitly to model, or what has been claimed in the literature so far.

%Overall, the results of this thesis show that semantic contentfulness (as measured with the help of DMLs) can be used to differentiate and specify cross-linguistic grammatical concepts, and suggest that it even plays a role in grammatical organisation cross-linguistically.

\section{Structure of the Thesis}

I explore three key questions in this thesis:
\begin{enumerate}

  \item[Q1:] How can we operationalize semantic contentfulness in a cross-lin\-guis\-ti\-cal\-ly applicable way? (Chapters 3, 5)
  \item[Q2:] How {\em cross-linguistically consistent} are lexicality-related divisions like the division between the lexical and functional word classes, or the inflection--derivation distinction? (Chapters 4, 5)
  \item[Q3:] What is the relationship between {\em form} and {\em semantics} across the lexicality spectrum? (Chapters 4, 6)
\end{enumerate}

\paragraph*{Chapter 2: (Computational) Comparative Concepts and Lexicality} In this chapter, I expand on the theoretical framework for the thesis. I introduce in more detail the problems of cross-linguistic category comparison, and the method of comparative concepts for resolving these issues in typological research. I review the ways in which semantic function has been handled in typological comparative concepts, highlighting a role for deep learning models of language in the creation of semantic comparative concepts. Next, I trace the history of how comparative concepts have been (at times, implicitly) employed, handled, and studied in computational typology. Through this background, I highlight how the creation of empirically grounded comparative concepts through the development of technologies and measures outside of typology (like perceptual theories of vowels and colours) has enabled major advances in typological research in the domains where it has been possible. This serves as further motivation for the computational approach to comparative concepts taken in this thesis. Finally, I provide a broad-scale overview of the lexicality spectrum, identifying different manifestations of a correlation between semantic contentfulness and formal linguistic structure, providing the connective tissue for the studies that follow. More detailed background on specific lexicality-related distinctions is provided in the relevant chapters.

\subsection*{Part I: Inflection and Derivation}

\nobibliography*
Part I of this thesis consists of Chapters 3--4, which focus on the inflection--derivation distinction drawn in morphology. These chapters are based primarily on the following journal article:

\begin{quote}
  \textbf{Haley, C.}, Ponti, E. M., and Goldwater, S. (2024). Corpus-based
  measures discriminate inflection and derivation cross-linguistically.
  \textit{Journal of Language Modelling}, 12(2):477–529
\end{quote}

\paragraph*{Chapter 3: Corpus-based Measures for Inflection and Derivation} In this chapter, we introduce a computational framework for studying the inflection--derivation distinction.  Inspired by \citet{spencer-2013-lexical}'s description of the distinction, we introduce a set of four quantitative measures of morphological constructions, including measures of both the magnitude and the variability of the changes to {\em form} and {\em distribution} introduced by each construction. These measures operationalize the intuition that derivations are more semantically contentful and semantically variable, as well as proposed formal differences between the categories. Crucially, these measures can be computed directly from a linguistic corpus, allowing us to consistently operationalize them across many languages and morphological constructions.

In contrast to prior computational studies that focus on a single language, we investigate 26 languages using the UniMorph 4.0 corpus \citep{batsuren-et-al-2022-unimorph}. We demonstrate that the measures are not explained by simple frequency effects, and that the distributional measures capture a limited amount of syntactic information in addition to semantic information. Using these measures, we find differences between inflection and derivation for all four measures, but substantial overlap for each individual measure, and that the distributional measures are more strongly associated with the distinction than the formal measures. Interestingly, we find that inflectional constructions are {\em more} formally variable than derivational constructions, contrary to some received wisdom in the literature on the topic but in line with the idea of the difference between inflection and derivation being similar to other lexicality distinctions.

%While a cross-linguistically consistent definition of the terms inflection and derivation has been elusive in theoretical linguistics, I demonstrate that by making linguist intuitions about the distinction concrete, much of the way the distinction has been made across languages can be explained, quantifying the cross-lingual consistency of these concepts. In so doing, I both show how linguistic intuitions about the distinction hold up in a corpus-based setting, and provide an indication of why these concepts have been so useful and attractive, despite issues with their cross-lingual descriptive validity.

\paragraph*{Chapter 4: Predicting Inflection and Derivation} In this chapter, we investigate whether a {\em combination} of the measures from \cref{ch:corpus} can discriminate inflection and derivation cross-linguistically, in contrast to prior studies which focused on single measures. We train classifier models to predict whether a construction is labelled as inflection or derivation in UniMorph. This novel classification approach allows us to quantify how much of the inflection--derivation distinction a combination of our measures explains. We find that language-agnostic classifiers based on our measures are able to predict inflectional--derivational status with high accuracy (90\%)---indicating a high degree of cross-linguistic {\em consistency} in the application of the distinction. We then analyse various sub-categories of inflection, finding inflectional transpositions like participles are {\em not} more likely to be misclassified as derivational, in line with \citeauthor{haspelmath-1996-wordclasschanging}'s \citeyear{haspelmath-1996-wordclasschanging} argument that these are best considered derivational. Overall, our results are in line with a {\em consistent, yet gradient} view of the inflection--derivation distinction. Our results suggest that distributional and formal {\em variability} are the most important dimensions for the distinction, but the {\em magnitude} of distributional and formal change also play a role. These results indicate that a {\em combination} of formal and distributional/semantic properties can explain much of the way linguists have applied the inflection--derivation distinction in UniMorph, in line with the view of lexicality as a combination of formal and functional properties.

\subsection*{Part II: Word Classes}
Part II of this thesis consists of Chapters 5--6, which focus on lexicality among (functional {\em and} lexical) word classes.
\paragraph*{Chapter 5: Groundedness and the Lexical-Functional Distinction} This chapter introduces a quantitative, language-agnostic measure of semantic contentfulness called {\em groundedness}. This measure attempts to overcome the {\em structuralist} nature of traditional information-theoretic measures of contentfulness, which treat linguistic form as the only source of information about meaning, and thus conflate formal and semantic information. To quantify groundedness, I leverage image--caption corpora, treating the image as a language-neutral proxy for meaning. Using a language model and an image captioning model, I define visual groundedness as the pointwise mutual information between a token and the image, which is estimated by taking the difference in surprisal of the token under the two models. This measure captures how much information about the image is provided by the token in context.

I apply this measure to investigate the visual groundedness of Universal Dependencies parts-of-speech across 30 languages from 10 language families. I find the relative groundedness of parts of speech is consistent across languages, reflecting the classical lexical--functional distinction in the form of a continuum. This continuum interestingly includes a consistent and linguistically interesting ranking between the traditionally {\em lexical} classes (nouns > adjectives > verbs), a fact which I return to in \cref{ch:splitlump}.  Our results suggest that traditionally functional word classes still carry semantic content, in line with prior psycholinguistic findings. These results suggest the utility of this measure as a general tool for studying contentfulness in linguistics, and of taking a visually grounded approach to typological questions.
% In this chapter, I introduce {\em groundedness}, a new semantic contentfulness measure based on multimodal models. Focusing on the domain of image captions, I am able to treat an image as a proxy for a caption's meaning. Using a language model and an image captioning model, I am able to estimate the pointwise mutual information between a token and the image as a surprisal difference under the two models. In this chapter, I focus on the \textbf{lexical-functional distinction} in parts of speech.
% Using image captioning data in 30 languages from 10 language families, I find this groundedness measure largely rediscovers the distinction between lexical and functional word classes across 30 languages. Further, though it correlates only weakly with norms like imageability and concreteness in English, it provides a ranking suggested by cognitive linguists between nouns, verbs, and adjectives (noun > adjectives > verbs) across languages but contradicts the view of adpositions as a ``semi-lexical'' class. However, our results suggest grammatical word classes still carry semantic content. These results suggest the utility of this measure as a general tool for studying contentfulness in linguistics, and of taking a visually grounded approach to typological problems.
This chapter is based on a conference paper at which appeared at NAACL 2025:
\begin{quote}
  \textbf{Haley, C.}, Goldwater, S., and Ponti, E. M. (2025). A Grounded Typology of Word Classes. In \textit{Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)}, pp. 10380–10399, Albuquerque, New Mexico. Association for Computational Linguistics.
\end{quote}

\paragraph*{Chapter 6: Splitting and Lumping} In this chapter, I investigate the relationship between visual groundedness and cross-linguistic variation in \textbf{lexical parts of speech}. While there has been substantial work in linguistic typology investigating cross-linguistic variation in the expression of the major lexical categories of nouns, verbs, and adjectives, this work has previously been largely disconnected from work on semantic contentfulness and the lexical--functional distinction; however, my results in \cref{ch:grounded} suggest cross-linguistic consistency in the relative visual groundedness of these classes. Building on this finding, I connect existing continuum and prototype theories of lexical categories and meanings with groundedness. I conjecture that the role of semantic contentfulness in lexical categories can help explain cross-linguistic variation in lexical category organization. In particular, I focus on languages which have been argued to ``split'' or ``lump'' major lexical categories.

%versal, a range of different organisational principals appear to emerge cross-linguistically. I show that deviations from prototypical part-of-speech organizations (as established in Chapter 4) are associated with groundedness that corresponds with the deviation, indicating an iconic link between groundedness and form.

To establish this, I first investigate Japanese. In Japanese, words denoting ``properties'' have the unusual property of constituting two formally very distinct word classes, rather than a single ``adjective'' class. Building on the insight that one of these classes is more formally ``nominal'' (\naadjs) and one more ``verbal'' (\iadjs), I hypothesize that we should see analogous trends in function: one class serving more prototypically nominal functions and one more prototypically verbal. In terms of visual groundedness, this corresponds to higher values for the nominal class. I investigate two manually captioned datasets and one machine translated dataset, finding significantly higher groundedness for \naadjs in the manually captioned datasets, in line with the theoretical predictions. This stands in contrast to previous studies, which have indicated little synchronic functional difference between the two classes.

To investigate lumping phenomena, I turned to the Tensedness Correlation, which correlates the formal similarities of adjectives to verbs in languages with a lack of obligatory tense marking on verbs. In languages with obligatory tense marking, the expression of adjectives is more similar to nouns. Drawing on prior explanations of the Correlation, I argue that they suggest that languages that lack tense marking (``verby'' languages) should have more grounded verbs. Testing this hypothesis involves direct comparison of groundedness estimates across languages, which can be challenging to interpret. After careful consideration of potential confounds, I find no significant relationship between verb groundedness and tensedness, highlighting some limitations of the kinds of questions that can be answered with current methods for estimating visual groundedness.

% I find no significant relationship, which I argue is due to issues with directly comparing groundedness scores across languages, suggesting the need for careful study design for groundedness-based research, and the difficulty of grounding verbs in particular.

\paragraph*{Chapter 7: Conclusion} In this chapter, I summarize the contributions of this thesis, discuss limitations, and outline directions for future work.
% \section{Contributions}
% Is this needed?
% \begin{itemize}
%   \item groundedness
%   \item consistency of lexicality-related distinctions
%   \item connections across levels of linguistic structure
%   \item new computational methods and approaches to typological research
% \end{itemize}

% While I see this as a core result of this chapter, I am continuing to work on additional experiments. Most essentially, I plan to spend the next 4 weeks conducting a large-scale, cross-lingual experiment about non-prototypical word-class use. Building on the word-level groundedness score dataset from Chapter 4, I am annotating the data with information about whether a word is a PROPERTY, ENTITY, CONCEPT, ACTION, EVENT, or OTHER--the protype functions of the major word classes (nouns, adjectives, and verbs). I am using BabelNet 4.0 to first annotate unambiguous instances of each label. I will combine BabelNet with XL-WSD to get more annotations for most of our languages and an evaluation set for those languages, including some ambiguous instances. I then need a method for classification. I will first try in-context learning with an off-the-shelf LLM. If performance is poor, I will train an XLM-R or Aya-101 based classifier on the XL-WSD train and BabelNet annotated data. After annotating the dataset, I will compare parts of speech with semantic class to see which is more strongly associated with groundedness. Further, I will investigate whether captions are more likely to use a non-prototypical part of speech (e.g. using a noun to describe an event) in more highly grounded contexts. Finally, while Japanese is a language where a major part of speech is ``split'', other languages in our dataset have major parts of speech ``joined''--notably Korean (verbs and adjectives) and Tagalog (nouns and verbs). I plan to investigate the patterns of groundedness in these ``joined'' parts of speech more closely--possibly with some finer grained semantics, and compare to other languages--e.g. are Korean property words typically less grounded?

% If time permits, I also hope to carry out some additional experiments on adpositions. In Chapter 4, I found that adpositions have very low average groundedness, despite having previously been described as ``semi-lexical.'' I aim to break apart this heterogenous class. I will first focus on identifying spatial adposition use (following the same techniques used in the previous semantic class experiment with BabelNet) as distinct from more ``abstract'' adposition use, and investigate whether spatial adpositions are more grounded than others, and how they compare to other functional word classes cross-linguistically. If I find that spatial adpositions are not in fact more grounded, this may be due to the noted poor spatial reasoning capacities of VLMs. To test this, I could leverage the image segmentation data of COCO, using a classifier to probe for a set of basic spatial relations based on the captions and seeing whether correct identification of the spatial relation by the classifier is correlated with increased groundedness of the associated spatial adposition. I anticipate that the basic experiments here would take 2 weeks, with the spatial relation probing experiments taking 2-4 additional weeks.

% It may be that I begin to run into limits of the quality of current groundedness scores, as the estimation can be somewhat noisy. I have two proposals for how to get improved score estimations. The first is to leverage advancements in multilingual multimodal models. Gemma 3 is capable of handling both mono- and multimodal inputs, and so using it to produce scores should be straightforward (compared to the fine-tuning routine required for PaliGemma). Additionally, I am considering running some experiments with LaBSE sentence embeddings as the meaning representation, rather than images. While they may be less language-agnostic than images in practice, they would enable this work to scale to domains beyond image captioning text and may help answer some of the research questions in the chapter. Using LaBSE as a meaning representation would require fine-tuning a simple model on top of a base LLM which takes the LaBSE embedding as a soft prompt. I plan to use Aya-101 for such an experiment. I am budgeting 2-4 weeks for producing an improved model as needed.

% Altogether, then, I estimate I need 2-3 months to complete the experimental work for Chapter 5. I estimate 2-4 additional weeks to write these results into a full content chapter. After this, I estimate 1 month for writing the background chapter (for which I have been reading and taken notes, but have not started writing), 2 weeks for writing discussion and conclusion, and 1 month for other changes to the introduction and other content chapters. I therefore anticipate submitting in October of this year.
% \section{Outline}
% \begin{enumerate}
% \item Introduction
% \begin{itemize}
%     \item defining typology
%     \item comparative concepts
%     \item connection to computational typology
%     \item Distributional models of language
%     \item semantic quality and semantic contentfulness
%     \item summary
% \end{itemize}
% \item Background
% \begin{itemize}
%     \item typology and typological sampling
%     \item comparative concepts
%     \begin{itemize}
%     \item inflection and derivation
%     \item word classes
%     \item the lexical-functional distinction
%     \end{itemize}
%     \item semantic contentfulness
%     \item computational models of language
%     \item computation and typology
%     \item iconicity and grammar
% \end{itemize}
% \item Corpus-based measures distinguish inflection and derivation cross-linguistically
% \begin{itemize}
% \item \textbf{Main claims:} taken together, measuring both formal and functional aspects of the inflection--derivation distinction can identify these categories cross-linguistically.
% \item \textbf{C3}: DMLs can capture variability in meaning of a derivation
% \item \textbf{C4}: Magnitude and especially variability in meaning are important for distinguishing inflection and derivation
% \item \textbf{C1}: The DML-based measures in concert w/ formal measures can distinguish inflection and derivation w/ high accuracy
% \end{itemize}
% \item A grounded typology of the lexical-functional distinction
% \begin{itemize}
% \item \textbf{Main claims:} comparing surprisal between a language model and an image captioning model can capture semantic contentfulness (groundedness) in a cross-linguistically general way. This measure shows substantial cross-linguistic consistency in the lexical-functional distinction.
% \item \textbf{C3:} DMLs capture semantic contentfulness
% \item \textbf{C4, C1:} groundedness correlates strongly with the lexical-functional distinction
% \end{itemize}
% \item Word classes are iconic with respect to groundedness
% \begin{itemize}
% \item Japanese adjectives split by groundedness (and not semantic category)
% \item Expressing words with unusual parts of speech correlates with groundedness maybe? (TODO)
% \end{itemize}
% \item Conclusion
% \end{enumerate}
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/tiredwired.png}
%     \caption{This is an example figure.}
%     \label{fig:tiredwired}
% \end{figure}
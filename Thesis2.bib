@book{-1996-phrase,
  title = {Phrase {{Structure}} and the {{Lexicon}}},
  editor = {Rooryck, Johan and Zaring, Laurie and Haegeman, Liliane and Maling, Joan and McCloskey, James},
  year = 1996,
  series = {Studies in {{Natural Language}} and {{Linguistic Theory}}},
  volume = {33},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-015-8617-7},
  urldate = {2025-10-07},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-90-481-4621-5 978-94-015-8617-7}
}

@book{-2023-cambridge,
  title = {The {{Cambridge History}} of {{Linguistics}}},
  editor = {Waugh, Linda R. and {Monville-Burston}, Monique and Joseph, John E.},
  year = 2023,
  month = aug,
  edition = {1},
  publisher = {Cambridge University Press},
  doi = {10.1017/9780511842788},
  urldate = {2025-10-02},
  abstract = {The establishment of language as a focus of study took place over many centuries, and reflection on its nature emerged in relation to very different social and cultural practices. Written by a team of leading scholars, this volume provides an authoritative, chronological account of the history of the study of language from ancient times to the end of the 20th century (i.e., 'recent history', when modern linguistics greatly expanded). Comprised of 29 chapters, it is split into 3 parts, each with an introduction covering the larger context of interest in language, especially the different philosophical, religious, and/or political concerns and socio-cultural practices of the times. At the end of the volume, there is a combined list of all references cited and a comprehensive index of topics, languages, major figures, etc. Comprehensive in its scope, it is an essential reference for researchers, teachers and students alike in linguistics and related disciplines.},
  copyright = {https://www.cambridge.org/core/terms},
  isbn = {978-0-511-84278-8 978-0-521-84990-6},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/MEYXSUIE/Waugh et al. - 2023 - The Cambridge History of Linguistics.pdf}
}

@book{-2023-cambridgea,
  title = {The {{Cambridge History}} of {{Linguistics}}},
  editor = {Waugh, Linda R. and {Monville-Burston}, Monique and Joseph, John E.},
  year = 2023,
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/9780511842788},
  urldate = {2025-11-08},
  abstract = {The establishment of language as a focus of study took place over many centuries, and reflection on its nature emerged in relation to very different social and cultural practices. Written by a team of leading scholars, this volume provides an authoritative, chronological account of the history of the study of language from ancient times to the end of the 20th century (i.e., 'recent history', when modern linguistics greatly expanded). Comprised of 29 chapters, it is split into 3 parts, each with an introduction covering the larger context of interest in language, especially the different philosophical, religious, and/or political concerns and socio-cultural practices of the times. At the end of the volume, there is a combined list of all references cited and a comprehensive index of topics, languages, major figures, etc. Comprehensive in its scope, it is an essential reference for researchers, teachers and students alike in linguistics and related disciplines.},
  isbn = {978-0-521-84990-6},
  file = {/Users/coleman/Zotero/storage/FFQRZ8WI/9E944650E14861BEAFEDF5CE242618FC.html}
}

@misc{-cambridge,
  title = {The {{Cambridge History}} of {{Linguistics}}},
  url = {https://www-cambridge-org.eux.idm.oclc.org/core/books/cambridge-history-of-linguistics/9E944650E14861BEAFEDF5CE242618FC},
  urldate = {2025-11-08},
  file = {/Users/coleman/Zotero/storage/A586UMNZ/9E944650E14861BEAFEDF5CE242618FC.html}
}

@misc{-categories,
  title = {{{CATEGORIES AND RELATIONS IN SYNTAX}}: {{THE CLAUSE-LEVEL ORGANIZATION OF INFORMATION}} ({{LEXICON}}, {{TYPOLOGY}}, {{SEMANTICS}}) - {{ProQuest}}},
  shorttitle = {{{CATEGORIES AND RELATIONS IN SYNTAX}}},
  url = {https://www.proquest.com/openview/e39c5893fc6bcb1dd9d04677cd642267/1?pq-origsite=gscholar&cbl=18750&diss=y},
  urldate = {2025-09-06},
  abstract = {Explore millions of resources from scholarly journals, books, newspapers, videos and more, on the ProQuest Platform.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/UUCEY4BH/1.html}
}

@misc{-chinese,
  title = {"{{Chinese DE}} Constructions in Secondary Predication: {{Historical}} and Typo" by {{You-Min Lin}}},
  url = {https://digitalrepository.unm.edu/ling_etds/42/},
  urldate = {2025-10-22},
  file = {/Users/coleman/Zotero/storage/9XVCUBWQ/42.html}
}

@misc{-comparative,
  title = {Comparative Concepts and Practicing Typology: On {{Haspelmath}}'s Proposal for ``Flagging'' and ``(Person) Indexing''},
  shorttitle = {Comparative Concepts and Practicing Typology},
  journal = {Linguistic Society of New Zealand},
  url = {https://nzlingsoc.org/journal_article/comparative-concepts-and-practicing-typology-on-haspelmaths-proposal-for-flagging-and-person-indexing/},
  urldate = {2025-10-22},
  abstract = {This is a comment article to Martin Haspelmath's article in this volume. (Updated 27 Jan 2021)},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/44GYDGMX/comparative-concepts-and-practicing-typology-on-haspelmaths-proposal-for-flagging-and-person-in.html}
}

@misc{-efficient,
  title = {Efficient Compression in Color Naming and Its Evolution \textbar{} {{PNAS}}},
  url = {https://www.pnas.org/doi/full/10.1073/pnas.1800521115},
  urldate = {2025-10-07}
}

@misc{-handprint,
  title = {Handprint : Modern Color Models},
  url = {https://www.handprint.com/HP/WCL/color7.html#CIELUV},
  urldate = {2025-10-07}
}

@misc{-heidegger,
  title = {Heidegger and {{Duns Scotus}} on {{Truth}} and {{Language}} on {{JSTOR}}},
  eprint = {20131978},
  eprinttype = {jstor},
  url = {https://www.jstor.org/stable/20131978?seq=3},
  urldate = {2025-09-06}
}

@misc{-its,
  title = {It's Not Really Red, Green, Yellow, Blue: An Inquiry into Perceptual Color Space ({{Chapter}} 14) - {{Color Categories}} in {{Thought}} and {{Language}}},
  url = {https://www-cambridge-org.eux.idm.oclc.org/core/books/color-categories-in-thought-and-language/its-not-really-red-green-yellow-blue-an-inquiry-into-perceptual-color-space/D633A5960C9130A0F1C0DBD5A2C68238},
  urldate = {2025-10-07}
}

@misc{-kategorien,
  title = {Die {{Kategorien-}} Und {{Bedeutungslehre}} Des {{Duns Scotus}} : {{Martin Heidegger}} : {{Free Download}}, {{Borrow}}, and {{Streaming}} : {{Internet Archive}}},
  url = {https://archive.org/details/Bedeutungslehre/page/n143/mode/2up},
  urldate = {2025-09-06}
}

@misc{-numerical,
  title = {Numerical {{Simulation}} of {{Vowel Quality Systems}}: {{The Role}} of {{Perceptual Contrast}} on {{JSTOR}}},
  url = {https://www-jstor-org.eux.idm.oclc.org/stable/411991?origin=crossref&seq=1},
  urldate = {2025-10-07},
  file = {/Users/coleman/Zotero/storage/5628CXTM/411991.html}
}

@misc{-proquest,
  title = {{{ProQuest Ebook Central}} - {{Book Details}}},
  url = {https://ebookcentral.proquest.com/lib/ed/detail.action?pq-origsite=primo&docID=622579},
  urldate = {2025-09-20},
  file = {/Users/coleman/Zotero/storage/NAXI4XEB/detail.html}
}

@misc{-proquesta,
  title = {{{ProQuest Ebook Central}} - {{Reader}}},
  url = {https://ebookcentral.proquest.com/lib/ed/reader.action?c=UERG&docID=1784085&ppg=6},
  urldate = {2025-10-07},
  file = {/Users/coleman/Zotero/storage/Z4HSXAIR/reader.html}
}

@misc{-proquestb,
  title = {{{ProQuest Ebook Central}} - {{Reader}}},
  url = {https://ebookcentral.proquest.com/lib/ed/reader.action?c=UERG&docID=1784085&ppg=6},
  urldate = {2025-10-07},
  file = {/Users/coleman/Zotero/storage/QZ8G8NT7/reader.html}
}

@misc{-reference,
  title = {Reference and {{Modification}} in {{Universal Dependencies}} - {{ACL Anthology}}},
  url = {https://aclanthology.org/2025.udw-1.1/},
  urldate = {2025-09-25},
  file = {/Users/coleman/Zotero/storage/2Z5ISG3K/2025.udw-1.1.html}
}

@misc{-request,
  title = {Request: Comparison to Other Tokenizers/{{PoS}} Taggers {$\cdot$} {{Issue}} \#6 {$\cdot$} Taishi-i/Nagisa},
  shorttitle = {Request},
  journal = {GitHub},
  url = {https://github.com/taishi-i/nagisa/issues/6},
  urldate = {2025-09-06},
  abstract = {Could you include some notes briefly comparing this to other parses like Mecab? Mecab includes a comparison to other tokenizers/parsers. I think users would greatly benefit from knowing things like...},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/I9EDTTBW/6.html}
}

@misc{-whorf,
  title = {Whorf Hypothesis Is Supported in the Right Visual Field but Not the Left \textbar{} {{PNAS}}},
  url = {https://www.pnas.org/doi/10.1073/pnas.0509868103},
  urldate = {2025-10-07}
}

@inproceedings{abadji-et-al-2021-ungoliant,
  title = {Ungoliant: {{An}} Optimized Pipeline for the Generation of a Very Large-Scale Multilingual Web Corpus},
  author = {Abadji, Julien and Su{\'a}rez, Pedro Javier Ortiz and Romary, Laurent and Sagot, Beno{\^i}t},
  editor = {L{\"u}ngen, Harald and Kupietz, Marc and Ba{\'n}ski, Piotr and Barbaresi, Adrien and Clematide, Simon and Pisetta, Ines},
  year = 2021,
  series = {Proceedings of the Workshop on Challenges in the Management of Large Corpora ({{CMLC-9}}) 2021. {{Limerick}}, 12 July 2021 (Online-Event)},
  pages = {1--9},
  publisher = {Leibniz-Institut f\"ur Deutsche Sprache},
  address = {Mannheim},
  doi = {10.14618/ids-pub-10468},
  abstract = {Since the introduction of large language models in Natural Language Processing, large raw corpora have played a crucial role in Computational Linguistics. However, most of these large raw corpora are either available only for English or not available to the general public due to copyright issues. Nevertheless, there are some examples of freely available multilingual corpora for training Deep Learning NLP models, such as the OSCAR and Paracrawl corpora. However, they have quality issues, especially for low-resource languages. Moreover, recreating or updating these corpora is very complex. In this work, we try to reproduce and improve the goclassy pipeline used to create the OSCAR corpus. We propose a new pipeline that is faster, modular, parameterizable, and well documented. We use it to create a corpus similar to OSCAR but larger and based on recent data. Also, unlike OSCAR, the metadata information is at the document level. We release our pipeline under an open source license and publish the corpus under a research-only license.},
  langid = {english}
}

@incollection{ackema-et-al-2019-default,
  title = {Default Person versus Default Number in Agreement},
  booktitle = {Agreement, Case and Locality in the Nominal and Verbal Domains},
  author = {Ackema, Peter and Neeleman, Ad},
  year = 2019,
  series = {Open Generative Syntax},
  pages = {21--54},
  publisher = {Language Science Press},
  doi = {10.5281/zenodo.3458062},
  abstract = {In this paper, we compare the behaviour of the default in the person system (third person) withthe default in the number system (singular). We argue, following Nevins (2007; 2011), thatthird person pronouns have person features, while singular DPs lack number features. Theevidence for these claims comes from situations in which a single head agrees with multiple DPs that have contrasting person and number specifications. In case the number of morphological slots in which agreement can be realized is lower than the number of agreement relations established in syntax, such contrasting specification may prove problematic. As it turns out, conflicts between singular and plural do not result in ungrammaticality, but conflicts between third person and first or second person do. Such person clashes can be avoided if the morphological realization of the relevant person features is syncretic. Alternatively, languages may make use of a person hierarchy that regulates the morphological realization of conflicting specifications for person. The argument we present is rooted in, and supports, the theory of person developed in Ackema \& Neeleman (2013; to appear).},
  isbn = {978-3-96110-201-3},
  langid = {english},
  keywords = {agreement,default,number,person,person hierarchy}
}

@article{ackerman-et-al-2013-morphological,
  title = {Morphological {{Organization}}: {{The Low Conditional Entropy Conjecture}}},
  shorttitle = {Morphological {{Organization}}},
  author = {Ackerman, Farrell and Malouf, Robert},
  year = 2013,
  journal = {Language},
  volume = {89},
  number = {3},
  eprint = {24671935},
  eprinttype = {jstor},
  pages = {429--464},
  publisher = {Linguistic Society of America},
  issn = {0097-8507},
  url = {https://www.jstor.org/stable/24671935},
  urldate = {2025-11-09},
  abstract = {Crosslinguistically, inflectional morphology exhibits a spectacular range of complexity in both the structure of individual words and the organization of systems that words participate in. We distinguish two dimensions in the analysis of morphological complexity. Enumerative complexity (E-complexity) reflects the number of morphosyntactic distinctions that languages make and the strategies employed to encode them, concerning either the internal composition of words or the arrangement of classes of words into inflection classes. This, we argue, is constrained by INTEGRATIVE COMPLEXITY (I-complexity). The I-complexity of an inflectional system reflects the difficulty that a paradigmatic system poses for language users (rather than lexicographers) in information-theoretic terms. This becomes clear by distinguishing AVERAGE PARADIGM ENTROPY from AVERAGE CONDITIONAL ENTROPY. The average entropy of a paradigm is the uncertainty in guessing the realization for a particular cell of the paradigm of a particular lexeme (given knowledge of the possible exponents). This gives one a measure of the complexity of a morphological system---systems with more exponents and more inflection classes will in general have higher average paradigm entropy---but it presupposes a problem that adult native speakers will never encounter. In order to know that a lexeme exists, the speaker must have heard at least one word form, so in the worst case a speaker will be faced with predicting a word form based on knowledge of one other word form of that lexeme. Thus, a better measure of morphological complexity is the average conditional entropy, the average uncertainty in guessing the realization of one randomly selected cell in the paradigm of a lexeme given the realization of one other randomly selected cell. This is the I-complexity of paradigm organization. Viewed from this information-theoretic perspective, languages that appear to differ greatly in their E-complexity---the number of exponents, inflectional classes, and principal parts---can actually be quite similar in terms of the challenge they pose for a language user who already knows how the system works. We adduce evidence for this hypothesis from three sources: a comparison between languages of varying degrees of E-complexity, a case study from the particularly challenging conjugational system of Chiquihuitl\'an Mazatec, and a Monte Carlo simulation modeling the encoding of morphosyntactic properties into formal expressions. The results of these analyses provide evidence for the crucial status of words and paradigms for understanding morphological organization.},
  file = {/Users/coleman/Zotero/storage/YE6VDF3B/Ackerman and Malouf - 2013 - Morphological Organization The Low Conditional Entropy Conjecture.pdf}
}

@inproceedings{alkhamissi-et-al-2025-language,
  title = {From Language to Cognition: {{How LLMs}} Outgrow the Human Language Network},
  shorttitle = {From {{Language}} to {{Cognition}}},
  booktitle = {Proceedings of the 2025 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {AlKhamissi, Badr and Tuckute, Greta and Tang, Yingtian and Binhuraib, Taha Osama A and Bosselut, Antoine and Schrimpf, Martin},
  editor = {Christodoulopoulos, Christos and Chakraborty, Tanmoy and Rose, Carolyn and Peng, Violet},
  year = 2025,
  month = nov,
  pages = {24332--24350},
  publisher = {Association for Computational Linguistics},
  address = {Suzhou, China},
  url = {https://aclanthology.org/2025.emnlp-main.1237/},
  urldate = {2025-11-04},
  abstract = {Large language models (LLMs) exhibit remarkable similarity to neural activity in the human language network. However, the key properties of language underlying this alignment---and how brain-like representations emerge and change across training---remain unclear. We here benchmark 34 training checkpoints spanning 300B tokens across 8 different model sizes to analyze how brain alignment relates to linguistic competence. Specifically, we find that brain alignment tracks the development of formal linguistic competence---i.e., knowledge of linguistic rules---more closely than functional linguistic competence. While functional competence, which involves world knowledge and reasoning, continues to develop throughout training, its relationship with brain alignment is weaker, suggesting that the human language network primarily encodes formal linguistic structure rather than broader cognitive functions. Notably, we find that the correlation between next-word prediction, behavioral alignment, and brain alignment fades once models surpass human language proficiency. We further show that model size is not a reliable predictor of brain alignment when controlling for the number of features. Finally, using the largest set of rigorous neural language benchmarks to date, we show that language brain alignment benchmarks remain unsaturated, highlighting opportunities for improving future models. Taken together, our findings suggest that the human language network is best modeled by formal, rather than functional, aspects of language.},
  isbn = {979-8-89176-332-6},
  file = {/Users/coleman/Zotero/storage/VXPYC4ZE/AlKhamissi et al. - 2025 - From Language to Cognition How LLMs Outgrow the Human Language Network.pdf}
}

@incollection{amsler-2023-emergence,
  title = {The {{Emergence}} of {{Linguistic Thinking}} within {{Premodern Cultural Practices}}},
  booktitle = {The {{Cambridge History}} of {{Linguistics}}},
  author = {Amsler, Mark},
  editor = {Joseph, John E. and Waugh, Linda R. and {Monville-Burston}, Monique},
  year = 2023,
  pages = {9--34},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/9780511842788.004},
  urldate = {2025-11-08},
  abstract = {In premodern traditions, the science of language was embedded in other sociocultural practices. Ancient writing prompted linguistic thinking such that literacy and grammar were intertwined and the `grammarian' was an important cultural figure. The Greeks believed that philosophy was the best discipline for understanding language. In all premodern traditions, commentary discourse was central in the shaping of reflection on language. Commentary threads supplemented and/or corrected the original texts (scripture and grammars). A paramount aim was to maintain/explain the language of the sacred texts (Vedas, Qur'an, Vulgate). In their quest for purity grammarians developed the ideals of hellenismos, latinitas, and ,,arabiyya, thus imposing literate standards that created multilingual/diglossic situations.The issue of the separation of disciplines is reflected in grammarians' discourses. Greek rationalism influenced Arabic grammar and, later, medieval language studies. Grammar was to be separate from theology, but the boundary between grammar and logic was controversial. While translations of the Qur'an were prohibited, the Christian Bible was translated (Greek, Hebrew to Latin). But the merit of vernacular scripture translation --- which challenged the authority of Latin --- was debated. Translations stimulated linguistic thinking and language contact produced creative mixed forms.},
  isbn = {978-0-521-84990-6},
  keywords = {boundaries between disciplines,commentary discourse and threads,debates on vernacular translations,Greek philosophy and linguistic thinking,linguistic purity: hellenismos latinitas "arabiyya,linguistic thinking and social/cultural practices,multilingualism and diglossia,preservation/exegesis of sacred texts: Vedas Qur'an Vulgate,translation fostering linguistic thinking,writing literacy and grammar}
}

@article{anderson-1982-wheres,
  title = {Where's Morphology?},
  author = {Anderson, Stephen R.},
  year = 1982,
  journal = {Linguistic Inquiry},
  volume = {13},
  pages = {571--612}
}

@incollection{anderson-1985-inflectional,
  title = {Inflectional Morphology},
  booktitle = {Language Typology and Syntactic Description},
  author = {Anderson, Stephen R.},
  editor = {Shopen, Timothy},
  year = 1985,
  edition = {1st},
  volume = {3},
  pages = {150--201},
  publisher = {Cambridge University Press},
  address = {Cambridge, UK}
}

@inproceedings{ansell-et-al-2022-composable,
  title = {Composable {{Sparse Fine-Tuning}} for {{Cross-Lingual Transfer}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Ansell, Alan and Ponti, Edoardo and Korhonen, Anna and Vuli{\'c}, Ivan},
  editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
  year = 2022,
  month = may,
  pages = {1778--1796},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.125},
  urldate = {2025-11-08},
  abstract = {Fine-tuning the entire set of parameters of a large pretrained model has become the mainstream approach for transfer learning. To increase its efficiency and prevent catastrophic forgetting and interference, techniques like adapters and sparse fine-tuning have been developed. Adapters are modular, as they can be combined to adapt a model towards different facets of knowledge (e.g., dedicated language and/or task adapters). Sparse fine-tuning is expressive, as it controls the behavior of all model components. In this work, we introduce a new fine-tuning method with both these desirable properties. In particular, we learn sparse, real-valued masks based on a simple variant of the Lottery Ticket Hypothesis. Task-specific masks are obtained from annotated data in a source language, and language-specific masks from masked language modeling in a target language. Both these masks can then be composed with the pretrained model. Unlike adapter-based fine-tuning, this method neither increases the number of parameters at inference time nor alters the original model architecture. Most importantly, it outperforms adapters in zero-shot cross-lingual transfer by a large margin in a series of multilingual benchmarks, including Universal Dependencies, MasakhaNER, and AmericasNLI. Based on an in-depth analysis, we additionally find that sparsity is crucial to prevent both 1) interference between the fine-tunings to be composed and 2) overfitting. We release the code and models at https://github.com/cambridgeltl/composable-sft.},
  file = {/Users/coleman/Zotero/storage/MDCB33H7/Ansell et al. - 2022 - Composable Sparse Fine-Tuning for Cross-Lingual Transfer.pdf}
}

@inproceedings{antol-et-al-2015-vqa,
  title = {{{VQA}}: {{Visual Question Answering}}},
  shorttitle = {{{VQA}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C. Lawrence and Parikh, Devi},
  year = 2015,
  month = dec,
  pages = {2425--2433},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2015.279},
  urldate = {2025-10-31},
  abstract = {We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing 0.25M images, 0.76M questions, and 10M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines for VQA are provided and compared with human performance.},
  keywords = {Cognition,Glass,Image color analysis,Knowledge discovery,Measurement,Visualization},
  file = {/Users/coleman/Zotero/storage/HM9PYH7B/Antol et al. - 2015 - VQA Visual Question Answering.pdf;/Users/coleman/Zotero/storage/G8L3NY9U/7410636.html}
}

@article{antonello-et-al-2024-predictive,
  title = {Predictive Coding or Just Feature Discovery? {{An}} Alternative Account of Why Language Models Fit Brain Data},
  shorttitle = {Predictive {{Coding}} or {{Just Feature Discovery}}?},
  author = {Antonello, Richard and Huth, Alexander},
  year = 2024,
  journal = {Neurobiology of Language},
  volume = {5},
  number = {1},
  pages = {64--79},
  issn = {2641-4368},
  doi = {10.1162/nol_a_00087},
  abstract = {Many recent studies have shown that representations drawn from neural network language models are extremely effective at predicting brain responses to natural language. But why do these models work so well? One proposed explanation is that language models and brains are similar because they have the same objective: to predict upcoming words before they are perceived. This explanation is attractive because it lends support to the popular theory of predictive coding. We provide several analyses that cast doubt on this claim. First, we show that the ability to predict future words does not uniquely (or even best) explain why some representations are a better match to the brain than others. Second, we show that within a language model, representations that are best at predicting future words are strictly worse brain models than other representations. Finally, we argue in favor of an alternative explanation for the success of language models in neuroscience: These models are effective at predicting brain responses because they generally capture a wide variety of linguistic phenomena.},
  langid = {english},
  pmcid = {PMC11025645},
  pmid = {38645616},
  keywords = {encoding models,language models,predictive coding},
  file = {/Users/coleman/Zotero/storage/TQAHZNSN/Antonello and Huth - 2024 - Predictive Coding or Just Feature Discovery An Alternative Account of Why Language Models Fit Brain.pdf}
}

@misc{arppe-et-al-2014-finitestate,
  title = {Finite-State Transducer-Based Computational Model of {{Plains Cree}} Morphology},
  author = {Arppe, Antti and Harrigan, Atticus and Schmirler, Katherine and Antonsen, Lene and Trosterud, Trond and N{\o}rsteb{\o} Moshagen, Sjur and Silfverberg, Miikka and Wolvengrey, Arok and Snoek, Conor and Lachler, Jordan and Santos, Eddie Antonio and Okim{\=a}sis, Jean and Thunder, Dorothy},
  year = {2014--2019},
  url = {https://giellalt.uit.no/lang/crk/PlainsCreeDocumentation.html},
  urldate = {2020-11-02}
}

@article{assmann-et-al-1982-vowel,
  title = {Vowel Identification: {{Orthographic}}, Perceptual, and Acoustic Aspects},
  shorttitle = {Vowel Identification},
  author = {Assmann, Peter F. and Nearey, Terrance M. and Hogan, John T.},
  year = 1982,
  month = apr,
  journal = {The Journal of the Acoustical Society of America},
  volume = {71},
  number = {4},
  pages = {975--989},
  issn = {0001-4966},
  doi = {10.1121/1.387579},
  urldate = {2025-09-25},
  abstract = {This study investigates conditions under which vowels are well recognized and relates perceptual identification of individual tokens to acoustic characteristics. Results support recent findings that isolated vowels may be readily identified by listeners. Two experiments provided evidence that certain response tasks result in inflated error rates. Subsequent experiments showed improved identification in a fixed speaker context, compared with randomized speakers, for isolated vowels and gated centers. Performance was worse for gated vowels, suggesting that dynamic properties (such as duration and diphthongization) supplement steady-state cues. However, even-speaker-randomized gated vowels were well identified (14\% errors). Measures of ''steady-state information'' (formant frequencies and f0), ''dynamic information'' (formant slopes and duration), and ''speaker information'' (normalization) were adopted. Discriminant analyses of acoustic measurements indicated relatively little overlap between vowel categories. Using a new technique for relating acoustic measurements of individual tokens with identification by listeners, it is shown that (a) identification performance is clearly related to acoustic characteristics; (b) improvement in the fixed speaker context is correlated with improved statistical separation resulting from formant normalization, for the gated vowels; and (c) ''dynamic information'' is related to identification differences between full and gated isolated vowels.},
  file = {/Users/coleman/Zotero/storage/6JTWLAX2/1.html}
}

@article{auwera-2008-defense,
  title = {In Defense of Classical Semantic Maps},
  author = {Auwera, Johan Van Der},
  year = 2008,
  month = jul,
  journal = {Theoretical Linguistics},
  volume = {34},
  number = {1},
  pages = {39--46},
  publisher = {De Gruyter Mouton},
  issn = {1613-4060},
  doi = {10.1515/THLI.2008.002},
  urldate = {2025-09-20},
  abstract = {Article In defense of classical semantic maps was published on July 1, 2008 in the journal Theoretical Linguistics (volume 34, issue 1).},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/2Y6R8ZKB/Auwera - 2008 - In defense of classical semantic maps.pdf}
}

@inproceedings{babazhanova-et-al-2021-geometric,
  title = {Geometric Probing of Word Vectors},
  booktitle = {{{ESANN}} 2021 Proceedings - 29th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
  author = {Babazhanova, Madina and Tezekbayev, Maxat and Assylbekov, Zhenisbek},
  year = 2021,
  pages = {587--592},
  publisher = {i6doc.com publication},
  address = {Online and Bruges, Belgium},
  doi = {10.14428/esann/2021.ES2021-105}
}

@article{backhouse-1984-have,
  title = {Have All the Adjectives Gone?},
  author = {Backhouse, A. E.},
  year = 1984,
  month = mar,
  journal = {Lingua},
  volume = {62},
  number = {3},
  pages = {169--186},
  issn = {0024-3841},
  doi = {10.1016/0024-3841(84)90074-3},
  urldate = {2025-11-09},
  abstract = {We argue that adjectives in Japanese constitute a large, open word class, and that contrary treatments are the result of an over-reliance on morphological, at the expense of syntactic, criteria. In terms of Dixon's classification (1982), Japanese is thus to be placed together with languages like English, and not with languages where adjectives reportedly constitute only a minor class. The discussion centres on the two major sub-types of Japanese adjectives, their relation to other word classes under which they have sometimes been subsumed, and their relation to each other. In this last respect, it is argued that they are distinguished in terms of phonological and lexical factors, and that their status is thus analogous to that of certain sub-types of English adjectives.},
  file = {/Users/coleman/Zotero/storage/95MJ45SX/Backhouse - 1984 - Have all the adjectives gone.pdf;/Users/coleman/Zotero/storage/CEXWPTEX/0024384184900743.html}
}

@incollection{backhouse-2004-inflected,
  title = {Inflected and {{Uninflected Adjectives}} in {{Japanese}}},
  booktitle = {Adjective {{Classes}}: {{A Cross-Linguistic Typology}}},
  author = {Backhouse, Anthony E},
  editor = {Dixon, R M W and Aikhenvald, Alexandra Y},
  year = 2004,
  month = sep,
  pages = {0},
  publisher = {Oxford University Press},
  doi = {10.1093/oso/9780199270934.003.0002},
  urldate = {2025-10-23},
  abstract = {This chapter deals with adjectives in Japanese. Typologically, Japanese is a dependent-marking language; typical constituent order in the clause is predicate-final, and modifiers precede heads. Nouns function as the head of NPs commonly followed by case markers such as ga (NOM) and o (Ace), as modifier of nouns in NPs followed by the adnominal marker no, as complement of the copula da, and as complement of other copular verbs (such as naru `become') followed by the marker ni. Verbs function as the head of intransitive and transitive predicates, and directly precede NPs in modifying structures. Unlike nouns, verbs and the copula da arc inflected, largely on an agglutinating pattern. Lexically, Japanese has dearly delineated strata. The Sino and foreign strata arc the result of borrowing from classical Chinese and (chiefly) European languages respectively; Sino words are, at least diachronically, typically bimorphemic. In addition, mimetic items form a distinct stratum within the native vocabulary.},
  isbn = {978-0-19-927093-4},
  file = {/Users/coleman/Zotero/storage/5B5U6E7U/9780199270934.003.html}
}

@article{baker-et-al-2017-lexical,
  title = {Lexical {{Categories}}: {{Legacy}}, {{Lacuna}}, and {{Opportunity}} for {{Functionalists}} and {{Formalists}}},
  shorttitle = {Lexical {{Categories}}},
  author = {Baker, Mark and Croft, William},
  year = 2017,
  month = jan,
  journal = {Annual Review of Linguistics},
  volume = {3},
  number = {Volume 3, 2017},
  pages = {179--197},
  publisher = {Annual Reviews},
  issn = {2333-9683, 2333-9691},
  doi = {10.1146/annurev-linguistics-011516-034134},
  urldate = {2025-09-19},
  abstract = {The fundamental importance of lexical categories is uncontroversial within both formal and functional approaches to grammatical analysis. But despite the familiarity of this topic and its foundational nature for grammatical description and analysis, it is paradoxically not among the best-studied or -understood topics from either the functionalist or formalist perspective. Both schools of linguistic theory have inherited their basic assumptions and instincts about lexical categories from the structuralist practice of distributional analysis. We briefly survey approaches to the various lexical categories. We then comment on a few issues of strategic value that arise from these approaches, including the importance of clearly distinguishing roots, stems, words, and syntactic units when it comes to issues of lexical categories; the importance of recognizing when distributional tests are similar across languages in principled ways; and the need for the choice of distributional tests to be informed by theoretical hypotheses.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/C5U9AHCW/annurev-linguistics-011516-034134.html}
}

@article{baker-et-al-2017-lexicala,
  title = {Lexical {{Categories}}: {{Legacy}}, {{Lacuna}}, and {{Opportunity}} for {{Functionalists}} and {{Formalists}}},
  shorttitle = {Lexical {{Categories}}},
  author = {Baker, Mark and Croft, William},
  year = 2017,
  month = jan,
  journal = {Annual Review of Linguistics},
  volume = {3},
  number = {Volume 3, 2017},
  pages = {179--197},
  publisher = {Annual Reviews},
  issn = {2333-9683, 2333-9691},
  doi = {10.1146/annurev-linguistics-011516-034134},
  urldate = {2025-09-20},
  abstract = {The fundamental importance of lexical categories is uncontroversial within both formal and functional approaches to grammatical analysis. But despite the familiarity of this topic and its foundational nature for grammatical description and analysis, it is paradoxically not among the best-studied or -understood topics from either the functionalist or formalist perspective. Both schools of linguistic theory have inherited their basic assumptions and instincts about lexical categories from the structuralist practice of distributional analysis. We briefly survey approaches to the various lexical categories. We then comment on a few issues of strategic value that arise from these approaches, including the importance of clearly distinguishing roots, stems, words, and syntactic units when it comes to issues of lexical categories; the importance of recognizing when distributional tests are similar across languages in principled ways; and the need for the choice of distributional tests to be informed by theoretical hypotheses.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/BFZ5T8KD/annurev-linguistics-011516-034134.html}
}

@article{baker-et-al-2017-lexicalb,
  title = {Lexical {{Categories}}: {{Legacy}}, {{Lacuna}}, and {{Opportunity}} for {{Functionalists}} and {{Formalists}}},
  shorttitle = {Lexical {{Categories}}},
  author = {Baker, Mark and Croft, William},
  year = 2017,
  month = jan,
  journal = {Annual Review of Linguistics},
  volume = {3},
  number = {Volume 3, 2017},
  pages = {179--197},
  publisher = {Annual Reviews},
  issn = {2333-9683, 2333-9691},
  doi = {10.1146/annurev-linguistics-011516-034134},
  urldate = {2025-10-07},
  abstract = {The fundamental importance of lexical categories is uncontroversial within both formal and functional approaches to grammatical analysis. But despite the familiarity of this topic and its foundational nature for grammatical description and analysis, it is paradoxically not among the best-studied or -understood topics from either the functionalist or formalist perspective. Both schools of linguistic theory have inherited their basic assumptions and instincts about lexical categories from the structuralist practice of distributional analysis. We briefly survey approaches to the various lexical categories. We then comment on a few issues of strategic value that arise from these approaches, including the importance of clearly distinguishing roots, stems, words, and syntactic units when it comes to issues of lexical categories; the importance of recognizing when distributional tests are similar across languages in principled ways; and the need for the choice of distributional tests to be informed by theoretical hypotheses.},
  langid = {english}
}

@article{bamberg-adjective,
  title = {The {{Adjective Category}} in {{Japanese}}},
  author = {Bamberg, Otto-Friedrich-Universitat},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/M6LB4HTD/Bamberg - The Adjective Category in Japanese.pdf}
}

@article{bar-hillel-et-al-1953-semantic,
  title = {Semantic {{Information}}},
  author = {{Bar-Hillel}, Yehoshua and Carnap, Rudolf},
  year = 1953,
  journal = {The British Journal for the Philosophy of Science},
  volume = {4},
  number = {14},
  eprint = {685989},
  eprinttype = {jstor},
  pages = {147--157},
  publisher = {[Oxford University Press, The British Society for the Philosophy of Science]},
  issn = {0007-0882},
  url = {https://www.jstor.org/stable/685989},
  urldate = {2025-11-03},
  file = {/Users/coleman/Zotero/storage/ZCRRGC7C/Bar-Hillel and Carnap - 1953 - Semantic Information.pdf}
}

@inproceedings{batsuren-et-al-2021-morphynet,
  title = {{{MorphyNet}}: A Large Multilingual Database of Derivational and Inflectional Morphology},
  booktitle = {Proceedings of the 18th {{SIGMORPHON}} Workshop on Computational Research in Phonetics, Phonology, and Morphology},
  author = {Batsuren, Khuyagbaatar and Bella, G{\'a}bor and Giunchiglia, Fausto},
  year = 2021,
  month = aug,
  pages = {39--48},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.sigmorphon-1.5},
  abstract = {Large-scale morphological databases provide essential input to a wide range of NLP applications. Inflectional data is of particular importance for morphologically rich (agglutinative and highly inflecting) languages, and derivations can be used, e.g. to infer the semantics of out-of-vocabulary words. Extending the scope of state-of-the-art multilingual morphological databases, we announce the release of MorphyNet, a high-quality resource with 15 languages, 519k derivational and 10.1M inflectional entries, and a rich set of morphological features. MorphyNet was extracted from Wiktionary using both hand-crafted and automated methods, and was manually evaluated to be of a precision higher than 98\%. Both the resource generation logic and the resulting database are made freely available and are reusable as stand-alone tools or in combination with existing resources.}
}

@inproceedings{batsuren-et-al-2022-unimorph,
  title = {{{UniMorph}} 4.0: {{Universal Morphology}}},
  booktitle = {Proceedings of the {{Thirteenth Language Resources}} and {{Evaluation Conference}}},
  author = {Batsuren, Khuyagbaatar and Goldman, Omer and Khalifa, Salam and Habash, Nizar and Kiera{\'s}, Witold and Bella, G{\'a}bor and Leonard, Brian and Nicolai, Garrett and Gorman, Kyle and Ate, Yustinus Ghanggo and Ryskina, Maria and Mielke, Sabrina and Budianskaya, Elena and {El-Khaissi}, Charbel and Pimentel, Tiago and Gasser, Michael and Lane, William Abbott and Raj, Mohit and Coler, Matt and Samame, Jaime Rafael Montoya and Camaiteri, Delio Siticonatzi and Rojas, Esa{\'u} Zumaeta and L{\'o}pez Francis, Didier and Oncevay, Arturo and L{\'o}pez Bautista, Juan and Villegas, Gema Celeste Silva and Hennigen, Lucas Torroba and Ek, Adam and Guriel, David and Dirix, Peter and Bernardy, Jean-Philippe and Scherbakov, Andrey and {Bayyr-ool}, Aziyana and Anastasopoulos, Antonios and Zariquiey, Roberto and Sheifer, Karina and Ganieva, Sofya and Cruz, Hilaria and Karah{\'o}{\v g}a, Ritv{\'a}n and Markantonatou, Stella and Pavlidis, George and Plugaryov, Matvey and Klyachko, Elena and Salehi, Ali and Angulo, Candy and Baxi, Jatayu and Krizhanovsky, Andrew and Krizhanovskaya, Natalia and Salesky, Elizabeth and Vania, Clara and Ivanova, Sardana and White, Jennifer and Maudslay, Rowan Hall and Valvoda, Josef and Zmigrod, Ran and Czarnowska, Paula and Nikkarinen, Irene and Salchak, Aelita and Bhatt, Brijesh and Straughn, Christopher and Liu, Zoey and Washington, Jonathan North and Pinter, Yuval and Ataman, Duygu and Wolinski, Marcin and Suhardijanto, Totok and Yablonskaya, Anna and Stoehr, Niklas and Dolatian, Hossep and Nuriah, Zahroh and Ratan, Shyam and Tyers, Francis M. and Ponti, Edoardo M. and Aiton, Grant and Arora, Aryaman and Hatcher, Richard J. and Kumar, Ritesh and Young, Jeremiah and Rodionova, Daria and Yemelina, Anastasia and Andrushko, Taras and Marchenko, Igor and Mashkovtseva, Polina and Serova, Alexandra and Prud'hommeaux, Emily and Nepomniashchaya, Maria and Giunchiglia, Fausto and Chodroff, Eleanor and Hulden, Mans and Silfverberg, Miikka and McCarthy, Arya D. and Yarowsky, David and Cotterell, Ryan and Tsarfaty, Reut and Vylomova, Ekaterina},
  editor = {Calzolari, Nicoletta and B{\'e}chet, Fr{\'e}d{\'e}ric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, H{\'e}l{\`e}ne and Odijk, Jan and Piperidis, Stelios},
  year = 2022,
  month = jun,
  pages = {840--855},
  publisher = {European Language Resources Association},
  address = {Marseille, France},
  url = {https://aclanthology.org/2022.lrec-1.89},
  abstract = {The Universal Morphology (UniMorph) project is a collaborative effort providing broad-coverage instantiated normalized morphological inflection tables for hundreds of diverse world languages. The project comprises two major thrusts: a language-independent feature schema for rich morphological annotation, and a type-level resource of annotated data in diverse languages realizing that schema. This paper presents the expansions and improvements on several fronts that were made in the last couple of years (since McCarthy et al. (2020)). Collaborative efforts by numerous linguists have added 66 new languages, including 24 endangered languages. We have implemented several improvements to the extraction pipeline to tackle some issues, e.g., missing gender and macrons information. We have amended the schema to use a hierarchical structure that is needed for morphological phenomena like multiple-argument agreement and case stacking, while adding some missing morphological features to make the schema more inclusive. In light of the last UniMorph release, we also augmented the database with morpheme segmentation for 16 languages. Lastly, this new release makes a push towards inclusion of derivational morphology in UniMorph by enriching the data and annotation schema with instances representing derivational processes from MorphyNet.}
}

@incollection{bauer-2004-function,
  title = {The Function of Word-Formation and the Inflection-Derivation Distinction},
  booktitle = {Words in Their {{Places}}. {{A Festschrift}} for {{J}}. {{Lachlan Mackenzie}}},
  author = {Bauer, Laurie},
  year = 2004,
  pages = {283--292},
  publisher = {Vrije Universiteit},
  address = {Amsterdam, Netherlands}
}

@article{beard-1982-plural,
  title = {The Plural as a Lexical Derivation},
  author = {Beard, Robert},
  year = 1982,
  journal = {Glossa-an International Journal of Linguistics},
  volume = {16},
  number = {2},
  pages = {133--148}
}

@inproceedings{beekhuizen-2025-spatial,
  title = {Spatial Relation Marking across Languages: Extraction, Evaluation, Analysis},
  shorttitle = {Spatial Relation Marking across Languages},
  booktitle = {Proceedings of the 29th {{Conference}} on {{Computational Natural Language Learning}}},
  author = {Beekhuizen, Barend},
  editor = {Boleda, Gemma and Roth, Michael},
  year = 2025,
  month = jul,
  pages = {571--585},
  publisher = {Association for Computational Linguistics},
  address = {Vienna, Austria},
  doi = {10.18653/v1/2025.conll-1.37},
  urldate = {2025-09-06},
  abstract = {This paper presents a novel task, detecting Spatial Relation Markers (SRMs, like English \_**in** the bag\_), across languages, alongside a model for this task, RUIMTE. Using a massively parallel corpus of Bible translations, the model is evaluated against existing and baseline models on the basis of a novel evaluation set. The model presents high quality SRM extraction, and an accurate identification of situations where language have zero-marked SRMs.},
  isbn = {979-8-89176-271-8},
  file = {/Users/coleman/Zotero/storage/F8MIFBFA/Beekhuizen - 2025 - Spatial relation marking across languages extraction, evaluation, analysis.pdf}
}

@inproceedings{beekhuizen-et-al-2017-semantic,
  title = {Semantic {{Typology}} and {{Parallel Corpora}}: {{Something}} about {{Indefinite Pronouns}}},
  shorttitle = {Semantic {{Typology}} and {{Parallel Corpora}}},
  booktitle = {Proceedings of the {{Annual Meeting}} of the {{Cognitive Science Society}}},
  author = {Beekhuizen, Barend and Watson, Julia and Stevenson, Suzanne},
  year = 2017,
  volume = {39},
  pages = {112--117},
  publisher = {Cognitive Science Society},
  address = {London, UK},
  url = {https://escholarship.org/uc/item/7z59z44h},
  urldate = {2025-11-09},
  abstract = {Patterns of crosslinguistic variation in the expression of wordmeaning are informative about semantic organization, but mostmethods to study this are labor intensive and obscure the gra-dient nature of concepts. We propose an automatic method forextracting crosslinguistic co-categorization patterns from par-allel texts, and explore the properties of the data as a potentialsource for automatically creating semantic representations forcognitive modeling. We focus on indefinite pronouns, com-paring our findings against a study based on secondary sources(Haspelmath 1997). We show that using automatic methods onparallel texts contributes to more cognitively-plausible seman-tic representations for a domain.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/9R4LHM4Q/Beekhuizen et al. - 2017 - Semantic Typology and Parallel Corpora Something about Indefinite Pronouns.pdf}
}

@article{benjamini-et-al-2001-control,
  title = {The {{Control}} of the {{False Discovery Rate}} in {{Multiple Testing}} under {{Dependency}}},
  author = {Benjamini, Yoav and Yekutieli, Daniel},
  year = 2001,
  journal = {The Annals of Statistics},
  volume = {29},
  number = {4},
  eprint = {2674075},
  eprinttype = {jstor},
  pages = {1165--1188},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364},
  url = {https://www.jstor.org/stable/2674075},
  urldate = {2024-10-14},
  abstract = {Benjamini and Hochberg suggest that the false discovery rate may be the appropriate error rate to control in many applied multiple testing problems. A simple procedure was given there as an FDR controlling procedure for independent test statistics and was shown to be much more powerful than comparable procedures which control the traditional familywise error rate. We prove that this same procedure also controls the false discovery rate when the test statistics have positive regression dependency on each of the test statistics corresponding to the true null hypotheses. This condition for positive dependency is general enough to cover many problems of practical interest, including the comparisons of many treatments with a single control, multivariate normal test statistics with positive correlation matrix and multivariate t. Furthermore, the test statistics may be discrete, and the tested hypotheses composite without posing special difficulties. For all other forms of dependency, a simple conservative modification of the procedure controls the false discovery rate. Thus the range of problems for which a procedure with proven FDR control can be offered is greatly increased.},
  file = {/Users/coleman/Zotero/storage/9NQ35YFG/Benjamini_Yekutieli_2001_The Control of the False Discovery Rate in Multiple Testing under Dependency.pdf}
}

@book{bennis-et-al-1983-lexicalsemantic,
  title = {Lexical-Semantic versus Syntactic Disorders in Aphasia: The Processing of Prepositions},
  author = {Bennis, Hans and Prins, Ronald and Vermeulen, Jan},
  year = 1983
}

@misc{berger-et-al-2024-crosslingual,
  title = {Cross-Lingual and Cross-Cultural Variation in Image Descriptions},
  author = {Berger, Uri and Ponti, Edoardo M.},
  year = 2024,
  eprint = {2409.16646},
  primaryclass = {cs.CL},
  url = {https://arxiv.org/abs/2409.16646},
  archiveprefix = {arXiv}
}

@inproceedings{berger-et-al-2025-crosslingual,
  title = {Cross-{{Lingual}} and {{Cross-Cultural Variation}} in {{Image Descriptions}}},
  booktitle = {Proceedings of the 2025 {{Conference}} of the {{Nations}} of the {{Americas Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}} ({{Volume}} 1: {{Long Papers}})},
  author = {Berger, Uri and Ponti, Edoardo},
  editor = {Chiruzzo, Luis and Ritter, Alan and Wang, Lu},
  year = 2025,
  month = apr,
  pages = {9453--9465},
  publisher = {Association for Computational Linguistics},
  address = {Albuquerque, New Mexico},
  doi = {10.18653/v1/2025.naacl-long.478},
  urldate = {2025-11-08},
  abstract = {Do speakers of different languages talk differently about what they see? Behavioural and cognitive studies report cultural effects on perception; however, these are mostly limited in scope and hard to replicate. In this work, we conduct the first large-scale empirical study of cross-lingual variation in image descriptions. Using a multimodal dataset with 31 languages and images from diverse locations, we develop a method to accurately identify entities mentioned in captions and present in the images, then measure how they vary across languages. Our analysis reveals that pairs of languages that are geographically or genetically closer tend to mention the same entities more frequently. We also identify entity categories whose saliency is universally high (such as animate beings), low (clothing accessories) or displaying high variance across languages (landscape). In a case study, we measure the differences in a specific language pair (e.g., Japanese mentions clothing far more frequently than English). Furthermore, our method corroborates previous small-scale studies, including 1) Rosch et al. (1976)'s theory of basic-level categories, demonstrating a preference for entities that are neither too generic nor too specific, and 2) Miyamoto et al. (2006)'s hypothesis that environments afford patterns of perception, such as entity counts. Overall, our work reveals the presence of both universal and culture-specific patterns in entity mentions.},
  isbn = {979-8-89176-189-6},
  file = {/Users/coleman/Zotero/storage/QZKMRZEL/Berger and Ponti - 2025 - Cross-Lingual and Cross-Cultural Variation in Image Descriptions.pdf}
}

@inproceedings{bergmanis-et-al-2017-segmentation,
  title = {From Segmentation to Analyses: A Probabilistic Model for Unsupervised Morphology Induction},
  booktitle = {Proceedings of {{EACL}}},
  author = {Bergmanis, Toms and Goldwater, Sharon},
  year = 2017,
  address = {Valencia, Spain}
}

@book{berlin-et-al-1969-basic,
  title = {Basic {{Color Terms}}: Their {{Universality}} and {{Evolution}}},
  shorttitle = {Basic {{Color Terms}}},
  author = {Berlin, Brent and Kay, Paul},
  year = 1969,
  publisher = {University of California Press},
  address = {Berkeley and Los Angeles, California, USA},
  file = {/Users/coleman/Zotero/storage/TTNJB6QQ/Berlin-and-Kay-1969.html}
}

@book{berman-et-al-1994-relating,
  title = {Relating Events in Narrative:  {{A}} Crosslinguistic Developmental Study},
  shorttitle = {Relating Events in Narrative},
  author = {Berman, Ruth A. and Slobin, Dan Isaac},
  year = 1994,
  pages = {xiv, 748},
  publisher = {Lawrence Erlbaum Associates, Inc},
  address = {Hillsdale, NJ, US},
  abstract = {The focus of our study is the development of linguistic form in children.  [Part I describes the book's] general orientation. Part II concerns "function" in the sense of the narrative task involved in telling the frog story, and how this was performed by children of different ages [from 3 to 9 yrs] compared with adults; Part III deals with "forms" in the sense of the linguistic devices used by speakers of different ages and languages in telling the frog story; while Part IV considers how linguistic forms interact with narrative functions such as temporality, perspective, and segmentation. Part V concludes the book by looking at both form and function from the two perspectives encoded in the subtitle of the book: developmental trends, on the one hand, and crosslinguistic differences, on the other. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  isbn = {978-0-8058-1435-4},
  keywords = {Age Differences,Cross Cultural Differences,Language Development,Linguistics,Storytelling},
  file = {/Users/coleman/Zotero/storage/8J8VID5B/1994-97555-000.html}
}

@misc{beyer-et-al-2024-paligemma,
  title = {{{PaliGemma}}: {{A}} Versatile {{3B VLM}} for Transfer},
  shorttitle = {{{PaliGemma}}},
  author = {Beyer, Lucas and Steiner, Andreas and Pinto, Andr{\'e} Susano and Kolesnikov, Alexander and Wang, Xiao and Salz, Daniel and Neumann, Maxim and Alabdulmohsin, Ibrahim and Tschannen, Michael and Bugliarello, Emanuele and Unterthiner, Thomas and Keysers, Daniel and Koppula, Skanda and Liu, Fangyu and Grycner, Adam and Gritsenko, Alexey and Houlsby, Neil and Kumar, Manoj and Rong, Keran and Eisenschlos, Julian and Kabra, Rishabh and Bauer, Matthias and Bo{\v s}njak, Matko and Chen, Xi and Minderer, Matthias and Voigtlaender, Paul and Bica, Ioana and Balazevic, Ivana and Puigcerver, Joan and Papalampidi, Pinelopi and Henaff, Olivier and Xiong, Xi and Soricut, Radu and Harmsen, Jeremiah and Zhai, Xiaohua},
  year = 2024,
  month = oct,
  number = {arXiv:2407.07726},
  eprint = {2407.07726},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.07726},
  urldate = {2025-10-31},
  abstract = {PaliGemma is an open Vision-Language Model (VLM) that is based on the SigLIP-So400m vision encoder and the Gemma-2B language model. It is trained to be a versatile and broadly knowledgeable base model that is effective to transfer. It achieves strong performance on a wide variety of open-world tasks. We evaluate PaliGemma on almost 40 diverse tasks including standard VLM benchmarks, but also more specialized tasks such as remote-sensing and segmentation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/coleman/Zotero/storage/23NUXY6W/Beyer et al. - 2024 - PaliGemma A versatile 3B VLM for transfer.pdf;/Users/coleman/Zotero/storage/THJWU3XZ/2407.html}
}

@article{bird-et-al-2003-verbs,
  title = {Verbs and Nouns: The Importance of Being Imageable},
  author = {Bird, Helen and Howard, David and Franklin, Sue},
  year = 2003,
  month = mar,
  journal = {Journal of Neurolinguistics},
  volume = {16},
  number = {2},
  pages = {113--149},
  issn = {0911-6044},
  doi = {10.1016/S0911-6044(02)00016-7},
  abstract = {There are many differences between verbs and nouns---semantic, syntactic and phonological. We focus on the semantic distinctions and examine differences in performance in both normal control subjects and individuals with aphasia. In tasks requiring production of particular semantic categories and categorisation of given verbs and nouns, control subjects produced fewer verbs than nouns and were slower and less accurate in verb categorisation. Patients who had shown a verb deficit in naming also had particular difficulties producing both verbs and nouns of relatively low imageability. In reading and writing, some patients exhibited poorer performance with verbs than nouns, even when verb/noun homonyms were used. When imageability was controlled, however, no dissociation was shown. We conclude that in simple single word tasks imageability must be controlled to eliminate this as a factor in apparent verb deficits. Other semantic factors, however, could affect performance, particularly when tasks involve the relationships between category exemplars.},
  keywords = {Aphasia,Imageability,Nouns,Reading,Semantics,Verbs,Writing}
}

@incollection{bisang-2010-word,
  title = {Word {{Classes}}},
  booktitle = {The {{Oxford Handbook}} of {{Linguistic Typology}}},
  author = {Bisang, Walter},
  editor = {Song, Jae Jung},
  year = 2010,
  month = nov,
  pages = {280--302},
  publisher = {Oxford University Press},
  address = {Oxford, UK},
  doi = {10.1093/oxfordhb/9780199281251.013.0015},
  urldate = {2024-05-15},
  abstract = {This article introduces the four prerequisites for distinguishing word classes: semantic criteria; pragmatic criteria/criteria of discourse function; formal criteria; and distinction between lexical and syntactic levels of analysis. The most important approaches to word classes based on the first three prerequisites are addressed. The article also deals with the distinction between content words and function words. It then takes up the discussion of the universal status of the noun/verb distinction by integrating the fourth prerequisite. The languages discussed are Classical Nahuatl, Late Archaic Chinese, and Tongan. The distinction between content words and function words is not identical to the distinction between open and closed word classes. The article reviews Dixon's seminal approach to adjectives. The sub-classes of adverbs are considered. The definition of word classes integrates all the central elements that make language structure, and it integrates a whole paradigm of constructions.},
  isbn = {978-0-19-928125-1}
}

@incollection{bisang-2017-grammaticalization,
  title = {Grammaticalization},
  booktitle = {Oxford {{Research Encyclopedia}} of {{Linguistics}}},
  author = {Bisang, Walter},
  year = 2017,
  month = mar,
  publisher = {Oxford University Press},
  address = {Oxford, UK},
  doi = {10.1093/acrefore/9780199384655.013.103},
  urldate = {2024-10-07},
  abstract = {Linguistic change not only affects the lexicon and the phonology of words, it also operates on the grammar of a language. In this context, grammaticalization is concerned with the development of lexical items into markers of grammatical categories or, more generally, with the development of markers used for procedural cueing of abstract relationships out of linguistic items with concrete referential meaning. A well-known example is the English verb               go               in its function of a future marker, as in               She is going to visit her friend               . Phenomena like these are very frequent across the world's languages and across many different domains of grammatical categories. In the last 50 years, research on grammaticalization has come up with a plethora of (a) generalizations, (b) models of how grammaticalization works, and (c) methodological refinements.                          On (a): Processes of grammaticalization develop gradually, step by step, and the sequence of the individual stages follows certain clines as they have been generalized from cross-linguistic comparison (unidirectionality). Even though there are counterexamples that go against the directionality of various clines, their number seems smaller than assumed in the late 1990s.             On (b): Models or scenarios of grammaticalization integrate various factors. Depending on the theoretical background, grammaticalization and its results are motivated either by the competing motivations of economy vs. iconicity/explicitness in functional typology or by a change from movement to merger in the minimalist program. Pragmatic inference is of central importance for initiating processes of grammaticalization (and maybe also at later stages), and it activates mechanisms like reanalysis and analogy, whose status is controversial in the literature. Finally, grammaticalization does not only work within individual languages/varieties, it also operates across languages. In situations of contact, the existence of a certain grammatical category may induce grammaticalization in another language.             On (c): Even though it is hard to measure degrees of grammaticalization in terms of absolute and exact figures, it is possible to determine relative degrees of grammaticalization in terms of the autonomy of linguistic signs. Moreover, more recent research has come up with criteria for distinguishing grammaticalization and lexicalization (defined as the loss of productivity, transparency, and/or compositionality of former productive, transparent, and compositional structures).             In spite of these findings, there are still quite a number of questions that need further research. Two questions to be discussed address basic issues concerning the overall properties of grammaticalization. (1) What is the relation between constructions and grammaticalization? In the more traditional view, constructions are seen as the syntactic framework within which linguistic items are grammaticalized. In more recent approaches based on construction grammar, constructions are defined as combinations of form and meaning. Thus, grammaticalization can be seen in the light of constructionalization, i.e., the creation of new combinations of form and meaning. Even though constructionalization covers many apects of grammaticalization, it does not exhaustively cover the domain of grammaticalization. (2) Is grammaticalization cross-linguistically homogeneous, or is there a certain range of variation? There is evidence from East and mainland Southeast Asia that there is cross-linguistic variation to some extent.},
  isbn = {978-0-19-938465-5},
  langid = {english}
}

@book{blake-2001-case,
  title = {Case},
  author = {Blake, Barry J},
  year = 2001,
  publisher = {Cambridge University Press}
}

@incollection{Block1998,
  title = {Semantics, Conceptual Role},
  booktitle = {Routledge Encyclopedia of Philosophy},
  author = {Block, Ned},
  editor = {Craig, Edward},
  year = 1998,
  volume = {8},
  pages = {652--657},
  publisher = {Routledge},
  address = {London, UK}
}

@article{boas-2010-syntax,
  title = {The Syntax--Lexicon Continuum in {{Construction Grammar}}: {{A}} Case Study of {{English}} Communication Verbs},
  shorttitle = {The Syntax--Lexicon Continuum in {{Construction Grammar}}},
  author = {Boas, Hans C.},
  year = 2010,
  month = dec,
  journal = {Belgian Journal of Linguistics},
  volume = {24},
  pages = {54--82},
  issn = {0774-5141, 1569-9676},
  doi = {10.1075/bjl.24.03boa},
  urldate = {2025-09-19},
  abstract = {This paper offers an alternative analysis of Goldberg's (1995) account of communication verbs appearing in the ditransitive construction. Based on a more finely-grained frame-semantic analysis of constructional phenomena, it is shown that generalizations over specific syntactic frames are possible at different levels of semantic abstraction. This, in turn, allows us to make across-the-board generalizations that hold not only between lexical units evoking the same frame, but also between lexical units belonging to different frames at different levels of abstraction. The resulting network of constructions combines Goldberg's proposals regarding the status of abstract-schematic constructions with itemspecific knowledge regarding the specific lexical units, with various midpoints in between. This approach has the advantage that there is no need for fusing lexical entries with abstract meaningful constructions, thereby avoiding some of the problems that arise due to the separation of syntax and the lexicon in some constructional approaches.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/MQM9JZL3/Boas - 2010 - The syntaxlexicon continuum in Construction Grammar A case study of English communication verbs.pdf}
}

@article{boas-2010-syntaxa,
  title = {The Syntax--Lexicon Continuum in {{Construction Grammar}}: {{A}} Case Study of {{English}} Communication Verbs},
  shorttitle = {The Syntax--Lexicon Continuum in {{Construction Grammar}}},
  author = {Boas, Hans C.},
  year = 2010,
  month = dec,
  journal = {Belgian Journal of Linguistics},
  volume = {24},
  pages = {54--82},
  issn = {0774-5141, 1569-9676},
  doi = {10.1075/bjl.24.03boa},
  urldate = {2025-09-20},
  abstract = {This paper offers an alternative analysis of Goldberg's (1995) account of communication verbs appearing in the ditransitive construction. Based on a more finely-grained frame-semantic analysis of constructional phenomena, it is shown that generalizations over specific syntactic frames are possible at different levels of semantic abstraction. This, in turn, allows us to make across-the-board generalizations that hold not only between lexical units evoking the same frame, but also between lexical units belonging to different frames at different levels of abstraction. The resulting network of constructions combines Goldberg's proposals regarding the status of abstract-schematic constructions with itemspecific knowledge regarding the specific lexical units, with various midpoints in between. This approach has the advantage that there is no need for fusing lexical entries with abstract meaningful constructions, thereby avoiding some of the problems that arise due to the separation of syntax and the lexicon in some constructional approaches.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/BU46VLPW/Boas - 2010 - The syntaxlexicon continuum in Construction Grammar A case study of English communication verbs.pdf}
}

@inproceedings{boguraev-et-al-2025-causal,
  title = {Causal Interventions Reveal Shared Structure across English Filler--Gap Constructions},
  booktitle = {Proceedings of the 2025 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Boguraev, Sasha and Potts, Christopher and Mahowald, Kyle},
  editor = {Christodoulopoulos, Christos and Chakraborty, Tanmoy and Rose, Carolyn and Peng, Violet},
  year = 2025,
  month = nov,
  pages = {25032--25053},
  publisher = {Association for Computational Linguistics},
  address = {Suzhou, China},
  url = {https://aclanthology.org/2025.emnlp-main.1271/},
  urldate = {2025-11-06},
  abstract = {Language Models (LMs) have emerged as powerful sources of evidence for linguists seeking to develop theories of syntax. In this paper, we argue that causal interpretability methods, applied to LMs, can greatly enhance the value of such evidence by helping us characterize the abstract mechanisms that LMs learn to use. Our empirical focus is a set of English filler--gap dependency constructions (e.g., questions, relative clauses). Linguistic theories largely agree that these constructions share many properties. Using experiments based in Distributed Interchange Interventions, we show that LMs converge on similar abstract analyses of these constructions. These analyses also reveal previously overlooked factors -- relating to frequency, filler type, and surrounding context -- that could motivate changes to standard linguistic theory. Overall, these results suggest that mechanistic, internal analyses of LMs can push linguistic theory forward.},
  isbn = {979-8-89176-332-6},
  file = {/Users/coleman/Zotero/storage/3NGHSUBE/Boguraev et al. - 2025 - Causal Interventions Reveal Shared Structure Across English FillerGap Constructions.pdf}
}

@article{bojanowski-et-al-2017-enriching,
  title = {Enriching Word Vectors with Subword Information},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  year = 2017,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {5},
  pages = {135--146},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  doi = {10.1162/tacl\_a\_00051},
  abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.}
}

@article{bonami-et-al-2018-inflection,
  title = {Inflection vs. Derivation in a Distributional Vector Space},
  author = {Bonami, Olivier and Paperno, Denis},
  year = 2018,
  journal = {Lingue e linguaggio},
  volume = {17},
  number = {2},
  pages = {173--196},
  file = {/Users/coleman/Zotero/storage/3L27WNMQ/Bonami and Paperno - INFLECTION VS. DERIVATION IN A DISTRIBUTIONAL VECT.pdf}
}

@article{bonami-et-al-2019-paradigm,
  title = {Paradigm Structure and Predictability in Derivational Morphology},
  author = {Bonami, Olivier and Strnadov{\'a}, Jana},
  year = 2019,
  month = may,
  journal = {Morphology},
  volume = {29},
  number = {2},
  pages = {167--197},
  issn = {1871-5656},
  doi = {10.1007/s11525-018-9322-6},
  abstract = {In this paper we address the usefulness of the notion of a paradigm in the context of derivational morphology. We first define a notion of paradigmatic system that extends conservatively the notion as it is used in inflection so as to be applicable to collections of structured families of derivationally-related words. We then build on this definition in an empirical quantitative study of derivational families of verbs in French. We apply information-theoretic measures of predictability initially designed by Ackerman et al. (2009) in the context of inflection. We conclude that key quantitative properties are common to inflectional and derivational paradigmatic systems, and hence that (partial) paradigms are an important ingredient of the study of derivation.}
}

@incollection{booij-1996-inherent,
  title = {Inherent versus Contextual Inflection and the Split Morphology Hypothesis},
  booktitle = {Yearbook of Morphology 1995},
  author = {Booij, Geert},
  editor = {Booij, Geert and {\noopsort{marle}}{van Marle}, Jaap},
  year = 1996,
  pages = {1--16},
  publisher = {Springer (Kluwer)},
  address = {Dordrecht, Netherlands}
}

@incollection{booij-2007-inflection,
  title = {Inflection},
  shorttitle = {The {{Grammar}} of {{Words}}},
  booktitle = {The {{Grammar}} of {{Words}}: {{An Introduction}} to {{Linguistic Morphology}}},
  author = {Booij, Geert},
  editor = {Booij, Geert},
  year = 2007,
  month = jul,
  pages = {99--124},
  publisher = {Oxford University Press},
  address = {Oxford, UK},
  doi = {10.1093/acprof:oso/9780199226245.003.0005},
  urldate = {2024-10-15},
  abstract = {Inflection is the expression of morphosyntactic properties on words. Examples are case and number marking on nouns, and number and person marking on verbs. These properties play a role in computing the correct form of word in a sentence. Unlike derivation, inflectional processes do not create new words but forms of a word. There are different theoretical models for inflection: Word-and-Paradigm, Item-and-Arrangement, and Item-and-Process models.},
  isbn = {978-0-19-922624-5},
  file = {/Users/coleman/Zotero/storage/RTAPQKK8/337167067.html}
}

@book{borg-et-al-2005-modern,
  title = {Modern {{Multidimensional Scaling}}},
  author = {Borg, Ingwer and Groenen, Patrick J.F.},
  year = 2005,
  series = {Springer {{Series}} in {{Statistics}}},
  edition = {2nd},
  publisher = {Springer},
  address = {New York, New York, USA},
  doi = {10.1007/0-387-28981-X},
  urldate = {2025-11-09},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-0-387-25150-9},
  langid = {english},
  keywords = {algorithms,best fit,correlation,marketing,modeling,multidimensional scaling,statistics},
  file = {/Users/coleman/Zotero/storage/NCQMLU8E/2005 - Modern Multidimensional Scaling.pdf}
}

@article{boschloo-1970-raised,
  title = {Raised Conditional Level of Significance for the 2 \texttimes{} 2-Table When Testing the Equality of Two Probabilities},
  author = {Boschloo, R.D.},
  year = 1970,
  journal = {Statistica Neerlandica},
  volume = {24},
  number = {1},
  pages = {1--9},
  publisher = {Wiley Online Library}
}

@article{boye-et-al-2012-usagebased,
  title = {A {{Usage-Based Theory}} of {{Grammatical Status}} and {{Grammaticalization}}},
  author = {Boye, Kasper and Harder, Peter},
  year = 2012,
  journal = {Language},
  volume = {88},
  number = {1},
  eprint = {41348882},
  eprinttype = {jstor},
  pages = {1--44},
  publisher = {Linguistic Society of America},
  issn = {0097-8507},
  url = {https://www.jstor.org/stable/41348882},
  urldate = {2025-10-23},
  abstract = {This article proposes a new way of understanding grammatical status and grammaticalization as distinctive types of linguistic phenomena. The approach is usage-based and links up structural and functional, as well as synchronie and diachronic, aspects of the issue. The proposal brings a range of previously disparate phenomena into a motivated relationship, while certain well-entrenched criteria (such as 'closed paradigms') are shown to be incidental to grammatical status and grammaticalization. The central idea is that grammar is constituted by expressions that by linguistic convention are ancillary and as such discursively secondary in relation to other linguistic expressions, and that grammaticalization is the kind of change that gives rise to such expressions.},
  file = {/Users/coleman/Zotero/storage/ZF2HQ6JW/Boye and Harder - 2012 - A Usage-Based Theory of Grammatical Status and Grammaticalization.pdf}
}

@article{boye-et-al-2018-grammatical,
  title = {Grammatical versus Lexical Words in Theory and Aphasia: {{Integrating}} Linguistics and Neurolinguistics},
  shorttitle = {Grammatical versus Lexical Words in Theory and Aphasia},
  author = {Boye, Kasper and Bastiaanse, Roelien},
  year = 2018,
  month = feb,
  journal = {Glossa: a journal of general linguistics},
  volume = {3},
  number = {1},
  publisher = {Open Library of Humanities},
  issn = {2397-1835},
  doi = {10.5334/gjgl.436},
  urldate = {2025-10-22},
  abstract = {The distinction between grammatical and lexical words is standardly dealt with in terms of a semantic distinction between function and content words or in terms of distributional distinctions between closed and open classes. This paper argues that such distinctions fall short in several respects, and that the grammar-lexicon distinction applies even within the same word class. The argument is based on a recent functional and usage-based theory of the grammar-lexicon distinction (Boye \&amp; Harder 2012) and on the assumption that aphasic speech data represent the ideal testing ground for theories and claims about this contrast. A theoretically-based distinction between grammatical and lexical instances of Dutch modal verb forms and the verb form hebben was confronted with agrammatic and fluent aphasic speech. A dissociation between the two aphasia types was predicted and confirmed.},
  copyright = {Copyright: \copyright{} 2018 The Author(s). This is an open-access article distributed under the terms of the Creative Commons Attribution 4.0 International License (CC-BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. See http://creativecommons.org/licenses/by/4.0/.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/F488FEAM/Boye and Bastiaanse - 2018 - Grammatical versus lexical words in theory and aphasia Integrating linguistics and neurolinguistics.pdf}
}

@inproceedings{boyer-et-al-2025-comparative,
  title = {Comparative {{Concepts}} or {{Descriptive Categories}}: A {{UD Case}} Study},
  shorttitle = {Comparative {{Concepts}} or {{Descriptive Categories}}},
  booktitle = {Proceedings of the {{Joint}} 25th {{Nordic Conference}} on {{Computational Linguistics}} and 11th {{Baltic Conference}} on {{Human Language Technologies}} ({{NoDaLiDa}}/{{Baltic-HLT}} 2025)},
  author = {Boyer, Matthieu Pierre and Dehouck, Mathieu},
  editor = {Johansson, Richard and Stymne, Sara},
  year = 2025,
  month = mar,
  pages = {55--65},
  publisher = {University of Tartu Library},
  address = {Tallinn, Estonia},
  url = {https://aclanthology.org/2025.nodalida-1.7/},
  urldate = {2025-09-20},
  abstract = {In this paper, we present a series of methods used to quantify the soundness of using the same names to annotate cases in different languages. We follow the idea described by Martin Haspelmath that descriptive categories and comparative concepts are different objects and we look at the necessary simplification taken by the Universal Dependencies project. We thus compare cases in closely related languages as belonging to commensurable descriptive categories. Then we look at the corresponding underlying comparative concepts. We finally looked at the possibility of assigning cases to adpositions.},
  isbn = {978-9908-53-109-0},
  file = {/Users/coleman/Zotero/storage/XHMHI8R8/Boyer and Dehouck - 2025 - Comparative Concepts or Descriptive Categories a UD Case study.pdf}
}

@inproceedings{brinkmann-et-al-2025-large,
  title = {Large {{Language Models}} Share Representations of Latent Grammatical Concepts across Typologically Diverse Languages},
  booktitle = {Proceedings of the 2025 {{Conference}} of the {{Nations}} of the {{Americas Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}} ({{Volume}} 1: {{Long Papers}})},
  author = {Brinkmann, Jannik and Wendler, Chris and Bartelt, Christian and Mueller, Aaron},
  editor = {Chiruzzo, Luis and Ritter, Alan and Wang, Lu},
  year = 2025,
  month = apr,
  pages = {6131--6150},
  publisher = {Association for Computational Linguistics},
  address = {Albuquerque, New Mexico},
  doi = {10.18653/v1/2025.naacl-long.312},
  urldate = {2025-10-30},
  abstract = {Human bilinguals often use similar brain regions to process multiple languages, depending on when they learned their second language and their proficiency. In large language models (LLMs), how are multiple languages learned and encoded? In this work, we explore the extent to which LLMs share representations of morphsyntactic concepts such as grammatical number, gender, and tense across languages. We train sparse autoencoders on Llama-3-8B and Aya-23-8B, and demonstrate that abstract grammatical concepts are often encoded in feature directions shared across many languages. We use causal interventions to verify the multilingual nature of these representations; specifically, we show that ablating only multilingual features decreases classifier performance to near-chance across languages. We then use these features to precisely modify model behavior in a machine translation task; this demonstrates both the generality and selectivity of these feature's roles in the network. Our findings suggest that even models trained predominantly on English data can develop robust, cross-lingual abstractions of morphosyntactic concepts.},
  isbn = {979-8-89176-189-6},
  file = {/Users/coleman/Zotero/storage/IDYMFQQM/Brinkmann et al. - 2025 - Large Language Models Share Representations of Latent Grammatical Concepts Across Typologically Dive.pdf}
}

@inproceedings{brown-et-al-2020-language,
  title = {Language Models Are Few-Shot Learners},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = 2020,
  volume = {33},
  pages = {1877--1901},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  urldate = {2025-11-04},
  abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
  file = {/Users/coleman/Zotero/storage/ACU9EKX4/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf}
}

@article{bruening-2018-lexicalist,
  title = {The Lexicalist Hypothesis: {{Both}} Wrong and Superfluous},
  shorttitle = {The Lexicalist Hypothesis},
  author = {Bruening, Benjamin},
  year = 2018,
  journal = {Language},
  volume = {94},
  number = {1},
  pages = {1--42},
  publisher = {Linguistic Society of America},
  issn = {1535-0665},
  url = {https://muse.jhu.edu/pub/24/article/688300},
  urldate = {2025-10-27},
  abstract = {The lexicalist hypothesis, which says that the component of grammar that produces words is distinct and strictly separate from the component that produces phrases, is both wrong and superfluous. It is wrong because (i) there are numerous instances where phrasal syntax feeds word formation; (ii) there are cases where phrasal syntax can access subword parts; and (iii) claims that word formation and phrasal syntax obey different principles are not correct. The lexicalist hypothesis is superfluous because where there are facts that it is supposed to account for, those facts have independent explanations. The model of grammar that we are led to is then the most parsimonious one: there is only one combinatorial component of grammar that puts together both words and phrases.}
}

@article{brysbaert-et-al-2014-concreteness,
  title = {Concreteness Ratings for 40 Thousand Generally Known {{English}} Word Lemmas},
  author = {Brysbaert, Marc and Warriner, Amy Beth and Kuperman, Victor},
  year = 2014,
  month = sep,
  journal = {Behavior Research Methods},
  volume = {46},
  number = {3},
  pages = {904--911},
  issn = {1554-3528},
  doi = {10.3758/s13428-013-0403-5},
  urldate = {2024-10-15},
  abstract = {Concreteness ratings are presented for 37,058 English words and 2,896 two-word expressions (such as zebra crossing and zoom in), obtained from over 4,000 participants by means of a norming study using Internet crowdsourcing for data collection. Although the instructions stressed that the assessment of word concreteness would be based on experiences involving all senses and motor responses, a comparison with the existing concreteness norms indicates that participants, as before, largely focused on visual and haptic experiences. The reported data set is a subset of a comprehensive list of English lemmas and contains all lemmas known by at least 85~\% of the raters. It can be used in future research as a reference list of generally known English lemmas.},
  langid = {english},
  keywords = {Concreteness,Crowdsourcing,Ratings,Word recognition},
  file = {/Users/coleman/Zotero/storage/JCYHSQJB/Brysbaert et al_2014_Concreteness ratings for 40 thousand generally known English word lemmas.pdf}
}

@incollection{brysbaert-toappear-concreteness,
  title = {Concreteness and Imageability Norms},
  booktitle = {International Encyclopedia of Language and Linguistics},
  author = {Brysbaert, Marc},
  year = {to appear},
  edition = {3rd},
  publisher = {Elsevier}
}

@book{bursill-hall-1972-speculative,
  title = {Speculative Grammars of the {{Middle Ages}}; the Doctrine of {{Partes}} Orationis of the {{Modistae}}},
  author = {{Bursill-Hall}, G. L.},
  year = 1972,
  publisher = {The Hague : Mouton},
  url = {http://archive.org/details/speculativegramm0000burs},
  urldate = {2025-09-06},
  abstract = {424 p. ; 25 cm. --; Originally presented as the author's thesis, University of London, 1969; Bibliography: p. [400]-406},
  collaborator = {{Internet Archive}},
  langid = {english},
  keywords = {Speculative grammar}
}

@book{bybee-1985-morphology,
  title = {Morphology: {{A}} Study of the Relation between Meaning and Form},
  author = {Bybee, Joan L.},
  year = 1985,
  publisher = {John Benjamins},
  address = {Amsterdam, Netherlands},
  key = {Bybee 1985}
}

@book{bybee-et-al-1994-evolution,
  title = {The {{Evolution}} of {{Grammar}}: {{Tense}}, {{Aspect}}, and {{Modality}} in the {{Languages}} of the {{World}}},
  shorttitle = {The {{Evolution}} of {{Grammar}}},
  author = {Bybee, Joan and Perkins, Revere and Pagliuca, William},
  year = 1994,
  month = nov,
  publisher = {University of Chicago Press},
  address = {Chicago, Illinois, USA},
  url = {https://press.uchicago.edu/ucp/books/book/chicago/E/bo3683926.html},
  urldate = {2025-11-09},
  abstract = {Joan Bybee and her colleagues present a new theory of the evolution of grammar that links structure and meaning in a way that directly challenges most contemporary versions of generative grammar. This study focuses on the use and meaning of grammatical markers of tense, aspect, and modality and identifies a universal set of grammatical categories. The authors demonstrate that the semantic content of these categories evolves gradually and that this process of evolution is strikingly similar across unrelated languages.Through a survey of seventy-six languages in twenty-five different phyla, the authors show that the same paths of change occur universally and that movement along these paths is in one direction only. This analysis reveals that lexical substance evolves into grammatical substance through various mechanisms of change, such as metaphorical extension and the conventionalization of implicature. Grammaticization is always accompanied by an increase in frequency of the grammatical marker, providing clear evidence that language use is a major factor in the evolution of synchronic language states.The Evolution of Grammar has important implications for the development of language and for the study of cognitive processes in general.},
  isbn = {978-0-226-08665-1},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/6LW8NJGA/bo3683926.html}
}

@book{bybee-et-al-2001-frequency,
  title = {Frequency and the {{Emergence}} of {{Linguistic Structure}}},
  author = {Bybee, Joan and Hopper, Paul J.},
  year = 2001,
  publisher = {John Benjamins Publishing Company},
  address = {Philadelphia, NETHERLANDS, THE},
  url = {http://ebookcentral.proquest.com/lib/ed/detail.action?docID=622579},
  urldate = {2025-09-20},
  isbn = {978-90-272-9803-4},
  keywords = {Frequency (Linguistics),Grammar Comparative and general.},
  file = {/Users/coleman/Zotero/storage/KB3BXC58/detail.html}
}

@misc{calderonvilca-et-al-2012-analizador,
  title = {Analizador Morf\'ologico de La Lengua {{Quechua}} Basado En Software Libre {{Helsinkifinite-statetransducer}} ({{HFST}})},
  author = {Calder{\'o}n Vilca, Hugo David and C{\'a}rdenas Mari{\~n}{\'o}, Flor Cagniy and Mamani Calder{\'o}n, Edwin Fredy Mamani},
  year = 2012,
  publisher = {COMTEL}
}

@article{caramazza-et-al-1991-lexical,
  title = {Lexical Organization of Nouns and Verbs in the Brain},
  author = {Caramazza, A. and Hillis, A. E.},
  year = 1991,
  month = feb,
  journal = {Nature},
  volume = {349},
  number = {6312},
  pages = {788--790},
  issn = {0028-0836},
  doi = {10.1038/349788a0},
  abstract = {The analysis of neuropsychological disorders of lexical processing has provided important clues about the general organization of the lexical system and the internal structure of the processing components. Reports of patients with selective dysfunction of specific semantic categories such as abstract versus concrete words, living things versus inanimate objects, animals, fruits and vegetables, proper names and so forth, support the hypothesis that the neural organization of the semantic processing component is organized in these categories. There are reports of selective dysfunction of the grammatical categories noun and verb, suggesting that a dimension of lexical organization is the grammatical class of words. But the results reported in these studies have not provided unambiguous evidence concerning two fundamental questions about the nature and the locus of this organization within the lexical system. Is the noun-verb distinction represented in the semantic or in the phonological and orthographic lexicons? Is grammatical-class knowledge represented independently of lexical forms or is it represented separately and redundantly within each modality-specific lexicon? Here we report the performance of two brain-damaged subjects with modality-specific deficits restricted principally (H.W.) or virtually only (S.J.D) to verbs in oral and written production, respectively. The contrasting performance suggests that grammatical-class distinctions are redundantly represented in the phonological and orthographic output lexical components.},
  langid = {english},
  pmid = {2000148},
  keywords = {Aphasia,Brain,Cerebrovascular Disorders,Female,Humans,Language,Language Disorders}
}

@incollection{carlson-1983-marking,
  title = {Marking {{Constituents}}},
  booktitle = {Linguistic {{Categories}}: {{Auxiliaries}} and {{Related Puzzles}}: {{Volume One}}: {{Categories}}},
  author = {Carlson, Greg N.},
  editor = {Heny, Frank and Richards, Barry},
  year = 1983,
  pages = {69--98},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-009-6989-6_4},
  urldate = {2025-10-07},
  abstract = {The cross-linguistic investigation of auxiliaries reveals that certain types of meanings are expressed by members of that category with nothing short of amazing regularity.1 Steele (1980) reports that one routinely finds tense, aspect, negation, modality, assertability conditions, question, and emphasis expressed in the AUX, to the exclusion of practically all other notions. While this may appear to encompass a fairly wide range of meaning, it is in fact quite narrow if one reflects on the range of possibilities, and linguistic theory should ultimately give an account of this. In a sense, we are confronted with a rather constant relationship between certain meanings and certain forms, and in the end will have to elucidate some principled connection between linguistic form and linguistic meaning. This runs counter to much in modern linguistics which emphasizes the arbitrary nature of language, both in terms of sound-meaning relations as well as whatever semantic content syntactic categories might hold. But, in the case of auxiliaries at least, there does appear to be semantic content to the category AUX, for not just anything is expressible as an auxiliary.},
  isbn = {978-94-009-6989-6},
  langid = {english},
  keywords = {Head Noun,Inflectional Form,Lexical Item,Relative Clause,Semantic Interpretation}
}

@book{carnap-1947-meaning,
  title = {Meaning and Necessity; a Study in Semantics and Modal Logic},
  author = {Carnap, Rudolf},
  year = 1947,
  series = {Meaning and Necessity; a Study in Semantics and Modal Logic},
  pages = {viii, 210},
  publisher = {University of Chicago Press},
  address = {Chicago, IL, US},
  abstract = {A new method of symbolic logic for analyzing and describing the meanings of linguistic expressions is developed. The traditional method of the name-relation which considers each expression as a name of a concrete or abstract entity leads to numerous difficulties. These difficulties can be avoided by the new method which takes an expression as possessing an extension and an intension. A new way of defining L-terms (for example, "L-true" means "logically true") is suggested. This new semantical method is applied to the theory of modalities, such as necessity, possibility, etc. (See also 16: 2755.) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {/Users/coleman/Zotero/storage/HQQBKGNP/1947-02716-000.html}
}

@misc{caucheteux-et-al-2021-disentangling,
  title = {Disentangling {{Syntax}} and {{Semantics}} in the {{Brain}} with {{Deep Networks}}},
  author = {Caucheteux, Charlotte and Gramfort, Alexandre and King, Jean-Remi},
  year = 2021,
  month = jun,
  number = {arXiv:2103.01620},
  eprint = {2103.01620},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.01620},
  urldate = {2025-09-20},
  abstract = {The activations of language transformers like GPT-2 have been shown to linearly map onto brain activity during speech comprehension. However, the nature of these activations remains largely unknown and presumably conflate distinct linguistic classes. Here, we propose a taxonomy to factorize the high-dimensional activations of language models into four combinatorial classes: lexical, compositional, syntactic, and semantic representations. We then introduce a statistical method to decompose, through the lens of GPT-2's activations, the brain activity of 345 subjects recorded with functional magnetic resonance imaging (fMRI) during the listening of \textasciitilde 4.6 hours of narrated text. The results highlight two findings. First, compositional representations recruit a more widespread cortical network than lexical ones, and encompass the bilateral temporal, parietal and prefrontal cortices. Second, contrary to previous claims, syntax and semantics are not associated with separated modules, but, instead, appear to share a common and distributed neural substrate. Overall, this study introduces a versatile framework to isolate, in the brain activity, the distributed representations of linguistic constructs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  file = {/Users/coleman/Zotero/storage/8FZE8A7M/Caucheteux et al. - 2021 - Disentangling Syntax and Semantics in the Brain with Deep Networks.pdf}
}

@book{chafe-1980-pear,
  title = {The {{Pear}} Stories: Cognitive, Cultural, and Linguistic Aspects of Narrative Production},
  author = {Chafe, Wallace L.},
  year = 1980,
  series = {Advances in {{Discourse Processes}}},
  volume = {III},
  publisher = {Ablex},
  address = {Norwood, N.J},
  isbn = {0-89391-032-5},
  keywords = {Discourse analysis,Narrative}
}

@article{chan-2008-codeswitching,
  title = {Code-Switching, Word Order and the Lexical/Functional Category Distinction},
  author = {Chan, Brian Hok-Shing},
  year = 2008,
  month = jun,
  journal = {Lingua},
  series = {Formal Syntactic Approaches to Bilingual Code-Switching},
  volume = {118},
  number = {6},
  pages = {777--809},
  issn = {0024-3841},
  doi = {10.1016/j.lingua.2007.05.004},
  urldate = {2025-09-06},
  abstract = {This paper claims that lexical categories (V, N) and functional categories (D, I, C) behave differently in bilingual code-switching: whereas functional heads always determine the order of their code-switched complements, lexical heads may not do so. This proposal thus deviates from many recent studies which suggest that all heads determine the order of their complements (e.g. Mahootian, 1993; MacSwan, 1999; Nishimura, 1997; Nishimura and Yoon, 1998). Assuming a ``Null Theory'' perspective (Mahootian, 1993; MacSwan, 1999), code-switching data are explained here in terms of existing syntactic apparatus which also governs monolingual syntax. It is proposed that word order between lexical categories and their complements is determined by head parameter instead of feature strength as an intrinsic property of the lexical heads. Nonetheless, head-complement order is inherently specified in functional categories. On this account, prepositions are functional heads instead of lexical heads.},
  keywords = {Code-switching,Functional/lexical distinction,Head-complement order,Null Theory,Prepositions,Word order},
  file = {/Users/coleman/Zotero/storage/E7N23NB7/Chan - 2008 - Code-switching, word order and the lexicalfunctional category distinction.pdf;/Users/coleman/Zotero/storage/SYWJCZKJ/S0024384107000903.html}
}

@article{chanturidze-et-al-2019-prepositions,
  title = {Prepositions as a Hybrid between Lexical and Functional Category: {{Evidence}} from an {{ERP}} Study on {{German}} Sentence Processing},
  shorttitle = {Prepositions as a Hybrid between Lexical and Functional Category},
  author = {Chanturidze, Mari and Carroll, Rebecca and Ruigendijk, Esther},
  year = 2019,
  month = nov,
  journal = {Journal of Neurolinguistics},
  volume = {52},
  pages = {100857},
  issn = {0911-6044},
  doi = {10.1016/j.jneuroling.2019.100857},
  urldate = {2025-11-06},
  abstract = {In syntactic theories of word categorization the status of prepositions as belonging to either a lexical (e.g., nouns, verbs) or a functional category (e.g., determiners, complementizers) is under debate. It has also been suggested that prepositions are a hybrid between the two categories depending on their usage. We investigated this classification question empirically in an ERP study with twelve mono-syllabic German prepositions in lexical (e.g., locative prepositions as in on the table) and subcategorized (e.g., selected by the verb as in waiting for) use. Thirty adult participants listened to sentences containing prepositions either in lexical or subcategorized use. Violations to lexical prepositions elicited an N400 -- a component typically associated with lexical-semantic processing. Violations to subcategorized prepositions elicited a P600 -- a component typically associated with structural/syntactic processing. In addition to lexical and subcategorized prepositions, the processing of sentence-final nouns following each type of preposition was measured. In both cases P600 effects were elicited. In addition to the positive effect, nouns in the context of incongruent lexical prepositions elicited an N400 effect. These qualitatively different processing results for lexical and subcategorized prepositions (and for nouns in the context of prepositions) suggest that depending on their use prepositions are processed like lexical or like functional words. By providing empirical evidence, we conclude that in terms of syntactic categorization, prepositions should be classified as a hybrid between a lexical and functional category.},
  keywords = {Lexical/functional category,N400,P600,Prepositions},
  file = {/Users/coleman/Zotero/storage/3HG5TNRM/Chanturidze et al. - 2019 - Prepositions as a hybrid between lexical and functional category Evidence from an ERP study on Germ.pdf;/Users/coleman/Zotero/storage/JD43RRNB/S0911604418301040.html}
}

@inproceedings{charpentier-et-al-2025-findings,
  title = {Findings of the {{Third BabyLM Challenge}}: {{Accelerating Language Modeling Research}} with {{Cognitively Plausible Data}}},
  shorttitle = {Findings of the {{Third BabyLM Challenge}}},
  booktitle = {Proceedings of the {{First BabyLM Workshop}}},
  author = {Charpentier, Lucas and Choshen, Leshem and Cotterell, Ryan and Gul, Mustafa Omer and Hu, Michael Y. and Liu, Jing and Jumelet, Jaap and Linzen, Tal and Mueller, Aaron and Ross, Candance and Shah, Raj Sanjay and Warstadt, Alex and Wilcox, Ethan Gotlieb and Williams, Adina},
  editor = {Charpentier, Lucas and Choshen, Leshem and Cotterell, Ryan and Gul, Mustafa Omer and Hu, Michael Y. and Liu, Jing and Jumelet, Jaap and Linzen, Tal and Mueller, Aaron and Ross, Candace and Shah, Raj Sanjay and Warstadt, Alex and Wilcox, Ethan Gotlieb and Williams, Adina},
  year = 2025,
  month = nov,
  pages = {399--420},
  publisher = {Association for Computational Linguistics},
  address = {Suzhou, China},
  url = {https://aclanthology.org/2025.babylm-main.28/},
  urldate = {2025-11-08},
  abstract = {This report summarizes the findings from the 3rd BabyLM Challenge and the 1st BabyLM Workshop. The BabyLM Challenge is a shared task aimed at closing the data efficiency gap between human and machine language learners. The goal is to improve the performance of language models given a fixed training budget of no more than 100 million words. This year, the challenge was held as part of an expanded BabyLM Workshop that invited paper submissions on topics relevant to the BabyLM effort, including sample-efficient pretraining and cognitive modeling for LMs. For the challenge, we kept the text-only and text--image tracks from previous years, but also introduced a new interaction track, where student models are allowed to learn from feedback from larger teacher models. Furthermore, we introduce a new set of evaluation tasks to assess the ``human likeness'' of models on a cognitive and linguistic level, limit the total amount of training compute allowed, and measure performance on intermediate checkpoints. We observe that new training objectives and architectures tend to produce the best-performing approaches, and that interaction with teacher models can yield high-quality language models. The strict and interaction tracks saw submissions that outperformed the best-performing methods from previous years. We do not observe a complete correlation between training FLOPs and performance. \%, suggesting that some methods can produce real gains beyond allowing us to spend more compute. This year's BabyLM Challenge shows that there is still room to innovate in a data-constrained setting, and that community-driven research can yield actionable insights for language modeling.},
  file = {/Users/coleman/Zotero/storage/D9S94ACU/Charpentier et al. - 2025 - Findings of the Third BabyLM Challenge Accelerating Language Modeling Research with Cognitively Pla.pdf}
}

@inproceedings{chen-et-al-2023-pali,
  title = {{{PaLI}}: {{A}} Jointly-Scaled Multilingual Language-Image Model},
  booktitle = {The Eleventh International Conference on Learning Representations, {{ICLR}} 2023, Kigali, Rwanda, May 1-5, 2023},
  author = {Chen, Xi and Wang, Xiao and Changpinyo, Soravit and Piergiovanni, Anthony J. and Padlewski, Piotr and Salz, Daniel and Goodman, Sebastian and Grycner, Adam and Mustafa, Basil and Beyer, Lucas and Kolesnikov, Alexander and Puigcerver, Joan and Ding, Nan and Rong, Keran and Akbari, Hassan and Mishra, Gaurav and Xue, Linting and Thapliyal, Ashish V. and Bradbury, James and Kuo, Weicheng},
  year = 2023,
  publisher = {OpenReview.net},
  url = {https://openreview.net/forum?id=mWVoBz4W0u},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  timestamp = {Wed, 24 Jul 2024 16:50:33 +0200}
}

@article{chen-et-al-2025-universal,
  title = {Universal Dimensions of Visual Representation},
  author = {Chen, Zirui and Bonner, Michael F.},
  year = 2025,
  journal = {Science Advances},
  volume = {11},
  number = {27},
  pages = {eadw7697},
  issn = {2375-2548},
  doi = {10.1126/sciadv.adw7697},
  urldate = {2025-11-04},
  abstract = {Do visual neural networks learn brain-aligned representations because they share architectural constraints and task objectives with biological vision or because they share universal features of natural image processing? We characterized the universality of hundreds of thousands of representational dimensions from networks with different architectures, tasks, and training data. We found that diverse networks learn to represent natural images using a shared set of latent dimensions, despite having highly distinct designs. Next, by comparing these networks with human brain representations measured with functional magnetic resonance imaging, we found that the most brain-aligned representations in neural networks are those that are universal and independent of a network's specific characteristics. Each network can be reduced to fewer than 10 of its most universal dimensions with little impact on its representational similarity to the brain. These results suggest that the underlying similarities between artificial and biological vision are primarily governed by a core set of universal representations that are convergently learned by diverse systems., Probing neural representations reveals universal aspects of vision in artificial and biological networks.},
  pmcid = {PMC12219468},
  pmid = {40601729},
  file = {/Users/coleman/Zotero/storage/C8F7X8PF/Chen and Bonner - Universal dimensions of visual representation.pdf}
}

@inproceedings{chi-et-al-2020-finding,
  title = {Finding {{Universal Grammatical Relations}} in {{Multilingual BERT}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Chi, Ethan A. and Hewitt, John and Manning, Christopher D.},
  editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
  year = 2020,
  month = jul,
  pages = {5564--5577},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.493},
  urldate = {2025-10-07},
  abstract = {Recent work has found evidence that Multilingual BERT (mBERT), a transformer-based multilingual masked language model, is capable of zero-shot cross-lingual transfer, suggesting that some aspects of its representations are shared cross-lingually. To better understand this overlap, we extend recent work on finding syntactic trees in neural networks' internal representations to the multilingual setting. We show that subspaces of mBERT representations recover syntactic tree distances in languages other than English, and that these subspaces are approximately shared across languages. Motivated by these results, we present an unsupervised analysis method that provides evidence mBERT learns representations of syntactic dependency labels, in the form of clusters which largely agree with the Universal Dependencies taxonomy. This evidence suggests that even without explicit supervision, multilingual masked language models learn certain linguistic universals.},
  file = {/Users/coleman/Zotero/storage/NBCWDVQQ/Chi et al. - 2020 - Finding Universal Grammatical Relations in Multilingual BERT.pdf}
}

@article{chiarello-et-al-1999-imageability,
  title = {Imageability and Distributional Typicality Measures of Nouns and Verbs in Contemporary {{English}}},
  author = {Chiarello, Christine and Shears, Connie and Lund, Kevin},
  year = 1999,
  month = dec,
  journal = {Behavior Research Methods, Instruments, \& Computers},
  volume = {31},
  number = {4},
  pages = {603--637},
  issn = {1532-5970},
  doi = {10.3758/BF03200739},
  abstract = {Dissociations between noun and verb processing are not uncommon after brain injury; yet, precise psycholinguistic comparisons of nouns and verbs are hampered by the underrepresentation of verbs in published semantic word norms and by the absence of contemporary estimates for part-of-speech usage. We report herein imageability ratings and rating response times (RTs) for 1,197 words previously categorized as pure nouns, pure verbs, or words of balanced noun-verb usage on the basis of the Francis and Ku\v cera (1982) norms. Nouns and verbs differed in rated imageability, and there was a stronger correspondence between imageability rating and RT for nouns than for verbs. For all word types, the image-rating-RT function implied that subjects employed an image generation process to assign ratings. We also report a new measure of noun-verbtypicality that used the Hyperspace Analog to Language (HAL; Lund \& Burgess, 1996) context vectors (derived from a large sample of Usenet text) to compute the mean context distance between each word and all of thepure nouns andpure verbs. For a subset of the items, the resulting HAL noun-verb difference score was compared with part-of-speech usage in a representative sample of the Usenet corpus. It is concluded that this score can be used to estimate the extent to which a given word occurs in typical noun or verb sentence contexts in informal contemporary English discourse. The item statistics given in Appendix B will enable experimenters to select representative examples of nouns and verbs or to compare typical with atypical nouns (or verbs), while holding constant or covarying rated imageability.},
  keywords = {Balance Form,Context Vector,Difference Score,Grammatical Class,Inflected Form}
}

@article{choksi-et-al-2022-multimodal,
  title = {Multimodal Neural Networks Better Explain Multivoxel Patterns in the Hippocampus},
  author = {Choksi, Bhavin and Mozafari, Milad and VanRullen, Rufin and Reddy, Leila},
  year = 2022,
  month = oct,
  journal = {Neural Networks},
  volume = {154},
  pages = {538--542},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2022.07.033},
  urldate = {2025-11-01},
  abstract = {The human hippocampus possesses ``concept cells'', neurons that fire when presented with stimuli belonging to a specific concept, regardless of the modality. Recently, similar concept cells were discovered in a multimodal network called CLIP (Radford et~al., 2021). Here, we ask whether CLIP can explain the fMRI activity of the human hippocampus better than a purely visual (or linguistic) model. We extend our analysis to a range of publicly available uni- and multi-modal models. We demonstrate that ``multimodality'' stands out as a key component when assessing the ability of a network to explain the multivoxel activity in the hippocampus.},
  keywords = {Concept cells,Deep learning,fMRI,Hippocampus,Multimodal networks,Neuroscience},
  file = {/Users/coleman/Zotero/storage/THHWU832/Choksi et al. - 2022 - Multimodal neural networks better explain multivoxel patterns in the hippocampus.pdf;/Users/coleman/Zotero/storage/6473UMQP/S0893608022002982.html}
}

@book{chomsky-1957-syntactic,
  title = {Syntactic {{Structures}}},
  author = {Chomsky, Noam},
  year = 1957,
  publisher = {De Gruyter Mouton},
  address = {Berlin, Germany},
  doi = {10.1515/9783112316009},
  urldate = {2024-05-14},
  isbn = {978-3-11-231600-9}
}

@book{chomsky-et-al-1968-sound,
  title = {The {{Sound Pattern}} of {{English}}},
  author = {Chomsky, Noam and Halle, Morris},
  year = 1968,
  series = {Studies in {{Language}}},
  publisher = {Harper \& Row},
  address = {New York, New York, USA},
  url = {https://web.mit.edu/morrishalle/pubworks/papers/1968_Chomsky_Halle_The_Sound_Pattern_of_English.pdf},
  langid = {english}
}

@misc{cie-1976-colorimetry,
  title = {Colorimetry - {{Part}} 4: {{CIE}} 1976 {{L}}*a*b* {{Colour}} Space},
  author = {{International Committee on Illumination}},
  year = 1976,
  publisher = {International Comission on Illumination},
  address = {Vienna, Austria},
  annotation = {CIE S 014-4:2007}
}

@article{clancy-2006-topology,
  title = {The Topology of {{Slavic}} Case: {{Semantic}} Maps and Multidimensional Scaling},
  author = {Clancy, Steven J.},
  year = 2006,
  journal = {Glossos},
  volume = {7},
  number = {1},
  pages = {1--28}
}

@book{colarusso-1988-northwest,
  title = {The {{Northwest Caucasian Languages}}: {{A Phonological Survey}}},
  author = {Colarusso, John},
  year = 1988,
  edition = {0},
  publisher = {Garland},
  address = {New York},
  doi = {10.4324/9781315852263},
  urldate = {2024-10-07},
  isbn = {978-1-317-91817-2},
  langid = {english}
}

@book{comrie-1988-language,
  title = {Language Universals and Linguistic Typology},
  shorttitle = {Language Universals and Linguistic Typology},
  author = {Comrie, Bernard},
  year = 1988,
  edition = {2nd},
  publisher = {The University of Chicago Press},
  address = {Chicago},
  isbn = {978-0-226-11433-0},
  langid = {english}
}

@incollection{comrie-et-al-1998-great,
  title = {The Great Dagestanian Case Hoax},
  booktitle = {Case, Typology, and Grammar},
  author = {Comrie, Bernard and Polinsky, Maria},
  year = 1998,
  pages = {95--114},
  publisher = {John Benjamins / John Benjamins},
  address = {Amsterdam}
}

@inproceedings{conneau-et-al-2020-unsupervised,
  title = {Unsupervised Cross-Lingual Representation Learning at Scale},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  author = {Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = 2020,
  month = jul,
  pages = {8440--8451},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.747},
  abstract = {This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6\% average accuracy on XNLI, +13\% average F1 score on MLQA, and +2.4\% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7\% in XNLI accuracy for Swahili and 11.4\% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.}
}

@article{connell-et-al-2012-strength,
  title = {Strength of Perceptual Experience Predicts Word Processing Performance Better than Concreteness or Imageability},
  author = {Connell, Louise and Lynott, Dermot},
  year = 2012,
  month = dec,
  journal = {Cognition},
  volume = {125},
  number = {3},
  pages = {452--465},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2012.07.010},
  abstract = {Abstract concepts are traditionally thought to differ from concrete concepts by their lack of perceptual information, which causes them to be processed more slowly and less accurately than perceptually-based concrete concepts. In two studies, we examined this assumption by comparing concreteness and imageability ratings to a set of perceptual strength norms in five separate modalities: sound, taste, touch, smell and vision. Results showed that concreteness and imageability do not reflect the perceptual basis of concepts: concreteness ratings appear to be based on two different intersecting decision criteria, while imageability ratings are visually biased. Analysis of lexical decision and word naming performance showed that maximum perceptual strength (i.e., strength in the dominant perceptual modality) consistently outperformed both concreteness and imageability ratings in accounting for variance in response latency and accuracy. We conclude that so-called concreteness effects in word processing emerge from the perceptual strength of a concept's representation and discuss the implications for theories of conceptual representation.},
  keywords = {Abstract and concrete concepts,Concreteness effects,Context availability,Dual coding,Imageability,Lexical decision,Perceptual strength,Situated simulation,Word naming}
}

@article{conway-et-al-2020-communication,
  title = {Communication Efficiency of Color Naming across Languages Provides a New Framework for the Evolution of Color Terms},
  author = {Conway, Bevil R. and Ratnasingam, Sivalogeswaran and {Jara-Ettinger}, Julian and Futrell, Richard and Gibson, Edward},
  year = 2020,
  month = feb,
  journal = {Cognition},
  volume = {195},
  pages = {104086},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2019.104086},
  urldate = {2025-11-09},
  abstract = {Languages vary in their number of color terms. A widely accepted theory proposes that languages evolve, acquiring color terms in a stereotyped sequence. This theory, by Berlin and Kay (BK), is supported by analyzing best exemplars (``focal colors'') of basic color terms in the World Color Survey (WCS) of 110 languages. But the instructions of the WCS were complex and the color chips confounded hue and saturation, which likely impacted focal-color selection. In addition, it is now known that even so-called early-stage languages nonetheless have a complete representation of color distributed across the population. These facts undermine the BK theory. Here we revisit the evolution of color terms using original color-naming data obtained with simple instructions in Tsimane', an Amazonian culture that has limited contact with industrialized society. We also collected data in Bolivian-Spanish speakers and English speakers. We discovered that information theory analysis of color-naming data was not influenced by color-chip saturation, which motivated a new analysis of the WCS data. Embedded within a universal pattern in which warm colors (reds, oranges) are always communicated more efficiently than cool colors (blues, greens), as languages increase in overall communicative efficiency about color, some colors undergo greater increases in communication efficiency compared to others. Communication efficiency increases first for yellow, then brown, then purple. The present analyses and results provide a new framework for understanding the evolution of color terms: what varies among cultures is not whether colors are seen differently, but the extent to which color is useful.},
  keywords = {Color categories,Communication efficiency,Cross-cultural,Information theory,Universal},
  file = {/Users/coleman/Zotero/storage/Q8URAAG7/Conway et al. - 2020 - Communication efficiency of color naming across languages provides a new framework for the evolution.pdf;/Users/coleman/Zotero/storage/VSVQ8XT6/S0010027719302604.html}
}

@article{copeland-2002-genesis,
  title = {The {{Genesis}} of {{Possible Worlds Semantics}}},
  author = {Copeland, B. Jack},
  year = 2002,
  month = apr,
  journal = {Journal of Philosophical Logic},
  volume = {31},
  number = {2},
  pages = {99--137},
  issn = {1573-0433},
  doi = {10.1023/A:1015273407895},
  urldate = {2025-11-03},
  abstract = {This article traces the development of possible worlds semantics through the work of: Wittgenstein, 1913--1921; Feys, 1924; McKinsey, 1945; Carnap, 1945--1947; McKinsey, Tarski and J\'onsson, 1947--1952; von Wright, 1951; Becker, 1952; Prior, 1953--1954; Montague, 1955; Meredith and Prior, 1956; Geach, 1960; Smiley, 1955--1957; Kanger, 1957; Hintikka, 1957; Guillaume, 1958; Binkley, 1958; Bayart, 1958--1959; Drake, 1959--1961; Kripke, 1958--1965.},
  langid = {english},
  keywords = {history of logic,modal logic,possible worlds semantics},
  file = {/Users/coleman/Zotero/storage/9587HMKP/Copeland - 2002 - The Genesis of Possible Worlds Semantics.pdf}
}

@article{copot-et-al-2022-idiosyncratic,
  title = {Idiosyncratic Frequency as a Measure of Derivation vs. Inflection},
  author = {Copot, Maria and Mickus, Timothee and Bonami, Olivier},
  year = 2022,
  month = dec,
  journal = {Journal of Language Modelling},
  volume = {10},
  number = {2},
  pages = {193--240},
  doi = {10.15398/jlm.v10i2.301},
  annotation = {Abstract note: \&amp;lt;p\&amp;gt;There is ongoing discussion about how to conceptualize the nature of the distinction between inflection and derivation. A common approach relies on qualitative differences in the semantic relationship between inflectionally versus derivationally related words: inflection yields ways to discuss the same concept in different syntactic contexts, while derivation gives rise to words for related concepts. This differential can be expected to manifest in the predictability of word frequency between words that are related derivationally or inflectionally: predicting the token frequency of a word based on information about its base form or about related words should be easier when the two words are in an inflectional relationship, rather than a derivational one. We compare prediction error magnitude for statistical models of token frequency based on distributional and frequency information of inflectionally or derivationally related words in French. The results conform to expectations: it is easier to predict the frequency of a word from properties of an inflectionally related word than from those of a derivationally related word. Prediction error provides a quantitative, continuous method to explore differences between individual processes and differences yielded by employing different predicting information, which in turn can be used to draw conclusions about the nature and manifestation of the inflection--derivation distinction.\&amp;lt;/p\&amp;gt;}
}

@article{corbett-2010-canonical,
  title = {Canonical Derivational Morphology},
  author = {Corbett, Greville G.},
  year = 2010,
  journal = {Word structure},
  volume = {3},
  number = {2},
  pages = {141--155},
  publisher = {Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK}
}

@incollection{corver-et-al-2001-semilexical,
  title = {Semi-Lexical Categories},
  booktitle = {Semi-Lexical {{Categories}}},
  author = {Corver, Norbert and {\noopsort{riemsdijk}}van Riemsdijk, Henk},
  editor = {Corver, Norbert and {\noopsort{riemsdijk}}van Riemsdijk, Henk},
  year = 2001,
  month = dec,
  pages = {1--20},
  publisher = {De Gruyter Mouton},
  address = {Berlin, Germany},
  doi = {10.1515/9783110874006.1},
  urldate = {2024-10-07},
  isbn = {978-3-11-016685-9},
  file = {/Users/coleman/Zotero/storage/Q6K682WK/Corver and Riemsdijk - 2001 - Semi-lexical categories.pdf}
}

@inproceedings{coto-solano-2022-evaluating,
  title = {Evaluating {{Word Embeddings}} in {{Extremely Under-Resourced Languages}}: {{A Case Study}} in {{Bribri}}},
  shorttitle = {Evaluating {{Word Embeddings}} in {{Extremely Under-Resourced Languages}}},
  booktitle = {Proceedings of the 29th {{International Conference}} on {{Computational Linguistics}}},
  author = {{Coto-Solano}, Rolando},
  editor = {Calzolari, Nicoletta and Huang, Chu-Ren and Kim, Hansaem and Pustejovsky, James and Wanner, Leo and Choi, Key-Sun and Ryu, Pum-Mo and Chen, Hsin-Hsi and Donatelli, Lucia and Ji, Heng and Kurohashi, Sadao and Paggio, Patrizia and Xue, Nianwen and Kim, Seokhwan and Hahm, Younggyun and He, Zhong and Lee, Tony Kyungil and Santus, Enrico and Bond, Francis and Na, Seung-Hoon},
  year = 2022,
  month = oct,
  pages = {4455--4467},
  publisher = {International Committee on Computational Linguistics},
  address = {Gyeongju, Republic of Korea},
  url = {https://aclanthology.org/2022.coling-1.393/},
  urldate = {2025-09-25},
  abstract = {Word embeddings are critical for numerous NLP tasks but their evaluation in actual under-resourced settings needs further examination. This paper presents a case study in Bribri, a Chibchan language from Costa Rica. Four experiments were adapted from English: Word similarities, WordSim353 correlations, odd-one-out tasks and analogies. Here we discuss their adaptation to an under-resourced Indigenous language and we use them to measure semantic and morphological learning. We trained 96 word2vec models with different hyperparameter combinations. The best models for this under-resourced scenario were Skip-grams with an intermediate size (100 dimensions) and large window sizes (10). These had an average correlation of r=0.28 with WordSim353, a 76\% accuracy in semantic odd-one-out and 70\% accuracy in structural/morphological odd-one-out. The performance was lower for the analogies: The best models could find the appropriate semantic target amongst the first 25 results approximately 60\% of the times, but could only find the morphological/structural target 11\% of the times. Future research needs to further explore the patterns of morphological/structural learning, to examine the behavior of deep learning embeddings, and to establish a human baseline. This project seeks to improve Bribri NLP and ultimately help in its maintenance and revitalization.},
  file = {/Users/coleman/Zotero/storage/PJU7DSBC/Coto-Solano - 2022 - Evaluating Word Embeddings in Extremely Under-Resourced Languages A Case Study in Bribri.pdf}
}

@article{cotterell-et-al-2019-complexity,
  title = {On the {{Complexity}} and {{Typology}} of {{Inflectional Morphological Systems}}},
  author = {Cotterell, Ryan and Kirov, Christo and Hulden, Mans and Eisner, Jason},
  editor = {Lee, Lillian and Johnson, Mark and Roark, Brian and Nenkova, Ani},
  year = 2019,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {7},
  pages = {327--342},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  doi = {10.1162/tacl_a_00271},
  urldate = {2025-05-14},
  abstract = {We quantify the linguistic complexity of different languages' morphological systems. We verify that there is a statistically significant empirical trade-off between paradigm size and irregularity: A language`s inflectional paradigms may be either large in size or highly irregular, but never both. We define a new measure of paradigm irregularity based on the conditional entropy of the surface realization of a paradigm--- how hard it is to jointly predict all the word forms in a paradigm from the lemma. We estimate irregularity by training a predictive model. Our measurements are taken on large morphological paradigms from 36 typologically diverse languages.},
  file = {/Users/coleman/Zotero/storage/2MKZ4SHR/Cotterell et al_2019_On the Complexity and Typology of Inflectional Morphological Systems.pdf}
}

@book{croft-1991-syntactic,
  title = {Syntactic Categories and Grammatical Relations: {{The}} Cognitive Organization of Information},
  author = {Croft, William},
  year = 1991,
  publisher = {University of Chicago Press},
  address = {Chicago, Illinois, USA},
  url = {https://books.google.co.uk/books?id=h1gLdOkH1GgC},
  isbn = {978-0-226-12090-4},
  lccn = {90038349}
}

@article{croft-1995-autonomy,
  title = {Autonomy and {{Functionalist Linguistics}}},
  author = {Croft, William},
  year = 1995,
  journal = {Language},
  volume = {71},
  number = {3},
  eprint = {416218},
  eprinttype = {jstor},
  pages = {490--532},
  publisher = {Linguistic Society of America},
  issn = {0097-8507},
  doi = {10.2307/416218},
  urldate = {2025-11-03},
  abstract = {Functional analyses of grammatical phenomena, and the functionalist approaches that promote them, are appealing to those who believe that an integrated view of language structure and language function is desirable. But functional analyses have been held to founder on basic grammatical facts that are taken to support the autonomy of grammar. The concept of autonomy is a complex one, and at least two different notions are found in current linguistic theory: arbitrariness and self-containedness. These notions of autonomy apply either to the syntactic component of the grammar, or (a more recent claim) to the grammar itself, with respect to change, use, and acquisition. The arbitrariness of syntax must be accepted; and many functional analyses are compatible with selfcontainedness. However, mixed formal/functional analyses provide an argument against the self-containedness of syntax, and in fact even many formal theories of syntax accept non-self-containedness. The arbitrariness of grammatical knowledge must also be accepted; and many functional analyses of the dynamic process affecting grammar are compatible with self-containedness. An argument against the self-containedness of grammar comes not from these functional analyses but from sociolinguistics.},
  file = {/Users/coleman/Zotero/storage/JSZBC3MY/Croft - 1995 - Autonomy and Functionalist Linguistics.pdf}
}

@book{croft-2000-explaining,
  title = {Explaining {{Language Change}}: {{An Evolutionary Approach}}},
  shorttitle = {Explaining {{Language Change}}},
  author = {Croft, William},
  year = 2000,
  publisher = {Pearson Education},
  address = {London, UK},
  abstract = {William Croft's text weaves together recent research findings from sociolinguistics, historical linguistics, grammatical change, pragmatics, social variation, language contact and genetic linguistics.},
  googlebooks = {5\_Ka7zLl9HQC},
  isbn = {978-0-582-35677-1},
  langid = {english},
  keywords = {Language Arts & Disciplines / Linguistics / General}
}

@incollection{croft-2001-coordination,
  title = {The {{Coordination}}---{{Subordination Continuum}}},
  booktitle = {Radical {{Construction Grammar}}: {{Syntactic Theory}} in {{Typological Perspective}}},
  author = {Croft, William},
  editor = {Croft, William},
  year = 2001,
  month = oct,
  pages = {0},
  publisher = {Oxford University Press},
  doi = {10.1093/acprof:oso/9780198299554.003.0009},
  urldate = {2025-11-08},
  abstract = {The distinction between coordination and subordination is claimed to be a structural universal. However, the structural criteria used to distinguish coordination from subordination do not match up across languages. This chapter proposes a functional analysis of complex sentence structure in terms of the Gestalt distinction between figure and ground. Coordination and complements constitute a complex figure (following Wierzbicka), while adverbial clauses and relative clauses constitute a figure-ground structure (following Talmy and Reinhart). Coordination and complementation are linked by grammaticalization, via serial verb constructions, and adverbial clauses and relative clauses are also linked by grammaticalization. Comparative and conditional relations are ambivalent, and expressed crosslinguistically by either complex figure or figure-ground constructions. A conceptual space is presented to account for Cristofaro's implicational hierarchies for different types of semantic relations between situations and their encoding as balanced (asserted, and more coordinate-like) and deranked (nonasserted, less coordinate-like) complex sentence constructions.},
  isbn = {978-0-19-829955-4},
  file = {/Users/coleman/Zotero/storage/V958P4ZR/Croft - 2001 - The CoordinationSubordination Continuum.pdf;/Users/coleman/Zotero/storage/R3FG5WAQ/9780198299554.003.html}
}

@incollection{croft-2001-parts,
  title = {Parts of {{Speech}}},
  booktitle = {Radical {{Construction Grammar}}: {{Syntactic Theory}} in {{Typological Perspective}}},
  author = {Croft, William},
  editor = {Croft, William},
  year = 2001,
  month = oct,
  pages = {0},
  publisher = {Oxford University Press},
  doi = {10.1093/acprof:oso/9780198299554.003.0002},
  urldate = {2025-08-09},
  abstract = {Most grammatical theories assume that the parts of speech --- noun, verb, adjective --- are categories of particular languages, but may be absent in some languages. But standard analyses are arbitrary and inconsistent about the constructions used to define syntactic categories, which leads some theorists to lump words into fewer categories and others to split words into more categories. One can be rigorous and consistent in analysis by using the same constructions across languages, namely the constructions denoting the propositional acts of reference, predication and modification, and comparing the structural coding and behavioral potential of semantic classes of lexical roots. This rigorous approach leads to universal prototypes for noun (reference to an object), verb (predication of an action), and adjective (modification by a property). Language-specific categories are represented as semantic maps on a universal conceptual space, constrained by the part of speech prototypes.},
  isbn = {978-0-19-829955-4},
  file = {/Users/coleman/Zotero/storage/WW6YDKJP/Croft - 2001 - Parts of Speech.pdf;/Users/coleman/Zotero/storage/M7LA6IML/9780198299554.003.html}
}

@book{croft-2001-radical,
  title = {Radical {{Construction Grammar}}: {{Syntactic Theory}} in {{Typological Perspective}}},
  author = {Croft, William},
  year = 2001,
  month = oct,
  publisher = {Oxford University Press},
  address = {Oxford, UK},
  doi = {10.1093/acprof:oso/9780198299554.001.0001},
  urldate = {2024-05-14},
  abstract = {This book presents a profound critique of syntactic theory and syntactic argumentation. Recent syntactic theories are essentially formal models for the representation of grammatical knowledge. These theories posit complex syntactic structures in the analysis of sentences, consisting of atomic primitive syntactic categories and relations. The result of this approach to syntax has been an endless cycle of new and revised theories of syntactic representation. The book argues that these types of syntactic theories are incompatible with the grammatical variation found within and across languages. The extent of grammatical variation demonstrates that no scheme of atomic primitive syntactic categories and relations can form the basis of an empirically adequate syntactic theory. This book defends three theses: (i) constructions are the primitive units of syntactic representation, and grammatical categories are derivative; (ii) the only syntactic structures are the relations between a construction and the elements that make it up (that is, there is no need to posit syntactic relations); and (iii) constructions are language-specific. Constructions are complex units pairing form and meaning. Grammatical categories within and across languages are mapped onto a universal conceptual space, following the semantic map model in typology. The structure of conceptual space constrains how meaning is encoded in linguistic form, and reflects the structure of the human mind.},
  isbn = {978-0-19-829955-4}
}

@book{croft-2002-typology,
  title = {Typology and {{Universals}}},
  author = {Croft, William},
  year = 2002,
  month = nov,
  edition = {2nd},
  publisher = {Cambridge University Press},
  address = {Cambridge, UK},
  doi = {10.1017/CBO9780511840579},
  urldate = {2024-10-07},
  abstract = {Comparison of the grammars of human languages reveals systematic patterns of variation. Typology and universals research uncovers those patterns to formulate universal constraints on language and seek their exploration. In this essential textbook, William Croft presents a comprehensive introduction to the method and theory used in studying typology and universals. The theoretical issues discussed range from the most fundamental to the most abstract. The book provides students and researchers with extensive examples of language universals in phonology, morphology, syntax and semantics. This second edition has been thoroughly rewritten and updated to reflect advances in typology and universals in the past decade, including: new methodologies such as the semantic map model and questions of syntactic argumentation; discussion of current debates over deeper explanations for specific classes of universals; and comparison of the typological and generative approaches to language.},
  copyright = {https://www.cambridge.org/core/terms},
  isbn = {978-0-521-00499-2 978-0-521-80884-2 978-0-511-84057-9}
}

@article{croft-2008-iconicity,
  title = {On Iconicity of Distance},
  author = {Croft, William},
  year = 2008,
  month = feb,
  journal = {Cognitive Linguistics},
  volume = {19},
  number = {1},
  pages = {49--57},
  publisher = {De Gruyter Mouton},
  issn = {1613-3641},
  doi = {10.1515/COG.2008.003},
  urldate = {2025-11-09},
  abstract = {Haspelmath argues that certain universal asymmetries in linguistic distance previously analyzed as examples of iconicity of distance are better analyzed as the result of frequency. It is argued here that Haspelmath's arguments can be countered by an advocate of iconicity of distance as an explanatory factor. Iconicity of distance is not different in kind from iconicity of contiguity, which Haspelmath endorses. Haspelmath's argument works only if one takes relative frequency instead of absolute frequency; yet it is generally accepted that economy effects are the result of absolute frequency. The empirical frequency data that Haspelmath presents is inconclusive. However, Haspelmath presents data that suggest that an iconicity of distance analysis, at least for possession constructions, must be revised as iconicity of length. Finally, criteria are offered to differentiate the effects of economy, iconicity of distance/length, and iconicity of independence.},
  chapter = {Cognitive Linguistics},
  langid = {english},
  keywords = {distance,economy,frequency,iconicity},
  file = {/Users/coleman/Zotero/storage/8Z8Q2FEY/Croft - 2008 - On iconicity of distance.pdf}
}

@article{croft-2014-comparing,
  title = {Comparing Categories and Constructions Crosslinguistically (Again): {{The}} Diversity of Ditransitives},
  shorttitle = {Comparing Categories and Constructions Crosslinguistically (Again)},
  author = {Croft, William},
  year = 2014,
  month = dec,
  journal = {Linguistic Typology},
  volume = {18},
  number = {3},
  pages = {533--551},
  publisher = {De Gruyter Mouton},
  issn = {1613-415X},
  doi = {10.1515/lingty-2014-0021},
  urldate = {2025-11-09},
  abstract = {Article Comparing categories and constructions crosslinguistically (again): The diversity of ditransitives was published on December 1, 2014 in the journal Linguistic Typology (volume 18, issue 3).},
  langid = {english},
  keywords = {comparative concepts,construction,ditransitive,objects,syntax},
  file = {/Users/coleman/Zotero/storage/XWGLZT2S/Croft - 2014 - Comparing categories and constructions crosslinguistically (again) The diversity of ditransitives.pdf}
}

@article{croft-2016-comparative,
  title = {Comparative Concepts and Language-Specific Categories: {{Theory}} and Practice},
  shorttitle = {Comparative Concepts and Language-Specific Categories},
  author = {Croft, William},
  year = 2016,
  month = oct,
  journal = {Linguistic Typology},
  volume = {20},
  number = {2},
  pages = {377--393},
  publisher = {De Gruyter Mouton},
  issn = {1613-415X},
  doi = {10.1515/lingty-2016-0012},
  urldate = {2025-09-17},
  abstract = {What are comparative concepts and how are they related to language-specific categories used in language description? Three general categories of comparative concepts are defined here: purely functional comparative concepts and two types of hybrid formal-functional concepts, constructions and strategies. The two hybrid types provide more explicit and precise definitions of common typological practice. However a terminological issue is that Western grammatical terms are frequently used to describe strategies which are not universal rather than constructions which are. Language-specific categories appear to be radically different from comparative concepts because the former are defined distributionally whereas the latter are defined in universal functional and formal terms. But language-specific constructions have functions, that is, they are instances of constructions in the comparative sense and their form is an instantiation of a strategy. Typology forms generalizations across language-specific constructions in both their form and their function. Finally, a major issue is the confusion of terminological choices for language-specific categories. Four rules of thumb for useful labeling of language-specific categories, largely following best descriptive practice, are offered.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  keywords = {categories,comparison,construction,methodology},
  file = {/Users/coleman/Zotero/storage/2UP34WAD/Croft - 2016 - Comparative concepts and language-specific categories Theory and practice.pdf}
}

@article{croft-2019-comparative,
  title = {Comparative Concepts and Practicing Typology: On {{Haspelmath}}'s Proposal for ``Flagging'' and ``(Person) Indexing''},
  shorttitle = {Comparative Concepts and Practicing Typology},
  author = {Croft, William},
  year = 2019,
  journal = {Te Reo},
  volume = {62},
  number = {1},
  pages = {116--129},
  url = {https://nzlingsoc.org/journal_article/comparative-concepts-and-practicing-typology-on-haspelmaths-proposal-for-flagging-and-person-indexing/},
  urldate = {2025-11-06},
  abstract = {This is a comment article to Martin Haspelmath's article in this volume. (Updated 27 Jan 2021)},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/542LDUJT/Croft - Comparative concepts and practicing typology on Haspelmaths proposal for flagging and (person).pdf;/Users/coleman/Zotero/storage/99SEIMU3/comparative-concepts-and-practicing-typology-on-haspelmaths-proposal-for-flagging-and-person-in.html}
}

@book{croft-2022-morphosyntax,
  title = {Morphosyntax: {{Constructions}} of the {{World}}'s {{Languages}}},
  shorttitle = {Morphosyntax},
  author = {Croft, William},
  year = 2022,
  month = aug,
  publisher = {Cambridge University Press},
  address = {Cambridge, UK},
  doi = {10.1017/9781316145289},
  urldate = {2025-11-06},
  abstract = {Bringing together the results of sixty years of research in typology and universals, this textbook presents a comprehensive survey of Morphosyntax - the combined study of syntax and morphology. Languages employ extremely diverse morphosyntactic strategies for expressing functions, and Croft provides a comprehensive functional framework to account for the full range of these constructions in the world's languages. The book explains analytical concepts that serve as a basis for cross-linguistic comparison, and provides a rich source of descriptive data that can be analysed within a range of theories. The functional framework is useful to linguists documenting endangered languages, and those writing reference grammars and other descriptive materials. Each technical term is comprehensively explained, and cross-referenced to related terms, at the end of each chapter and in an online glossary. This is an essential resource on Morphosyntax for advanced undergraduate and graduate students, researchers, and linguistic fieldworkers.},
  isbn = {9781316145289},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/7WPS3ZXK/1AAB4F5F9C553F675170DCA3F03F82E2.html}
}

@article{croft-2022-two,
  title = {On Two Mathematical Representations for ``Semantic Maps''},
  author = {Croft, William},
  year = 2022,
  month = jun,
  journal = {Zeitschrift f\"ur Sprachwissenschaft},
  volume = {41},
  number = {1},
  pages = {67--87},
  publisher = {De Gruyter},
  issn = {1613-3706},
  doi = {10.1515/zfs-2021-2040},
  urldate = {2025-09-20},
  abstract = {We describe two mathematical representations for what have come to be called ``semantic maps'', that is, representations of typological universals of linguistic co-expression with the aim of inferring similarity relations between concepts from those universals. The two mathematical representations are a graph structure and Euclidean space, the latter as inferred through multidimensional scaling. Graph structure representations come in two types. In both types, meanings are represented as vertices (nodes) and relations between meanings as edges (links). One representation is a pairwise co-expression graph, which represents all pairwise co-expression relations as edges in the graph; an example is CLICS. The other is a minimally connected co-expression graph -- the ``classic semantic map''. This represents only the edges necessary to maintain connectivity, that is, the principle that all the meanings expressed by a single form make up a connected subgraph of the whole graph. The Euclidean space represents meanings as points, and relations as Euclidean distance between points, in a specified number of spatial dimensions. We focus on the proper interpretation of both types of representations, algorithms for constructing the representations, measuring the goodness of fit of the representations to the data, and balancing goodness of fit with informativeness of the representation.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  keywords = {co-expression,Euclidean space,graph structure,multidimensional scaling,semantic map},
  file = {/Users/coleman/Zotero/storage/CFR2INEE/Croft - 2022 - On two mathematical representations for semantic maps.pdf}
}

@book{croft-et-al-2004-cognitive,
  title = {Cognitive {{Linguistics}}},
  author = {Croft, William and Cruse, D. Alan},
  year = 2004,
  series = {Cambridge {{Textbooks}} in {{Linguistics}}},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9780511803864},
  urldate = {2025-11-05},
  abstract = {Cognitive Linguistics argues that language is governed by general cognitive principles, rather than by a special-purpose language module. This introductory textbook surveys the field of cognitive linguistics as a distinct area of study, presenting its theoretical foundations and the arguments supporting it. Clearly organised and accessibly written, it provides a useful introduction to the relationship between language and cognitive processing in the human brain. It covers the main topics likely to be encountered in a course or seminar, and provides a synthesis of study and research in this fast-growing field of linguistics. The authors begin by explaining the conceptual structures and cognitive processes governing linguistic representation and behaviour, and go on to explore cognitive approaches to lexical semantics, as well as syntactic representation and analysis, focusing on the closely related frameworks of cognitive grammar and construction grammar. This much-needed introduction will be welcomed by students in linguistics and cognitive science.},
  isbn = {978-0-521-66114-0},
  file = {/Users/coleman/Zotero/storage/HGCMY4D8/8CE9230D2E18C120A5274EDE524C606C.html}
}

@article{croft-et-al-2008-inferring,
  title = {Inferring Universals from Grammatical Variation: {{Multidimensional}} Scaling for Typological Analysis},
  shorttitle = {Inferring Universals from Grammatical Variation},
  author = {Croft, William and Poole, Keith T.},
  year = 2008,
  month = jul,
  journal = {Theoretical Linguistics},
  volume = {34},
  number = {1},
  pages = {1--37},
  publisher = {De Gruyter Mouton},
  issn = {1613-4060},
  doi = {10.1515/THLI.2008.001},
  urldate = {2025-09-05},
  abstract = {A fundamental fact about grammatical structure is that it is highly variable both across languages and within languages. Typological analysis has drawn language universals from grammatical variation, in particular by using the semantic map model. But the semantic map model, while theoretically well-motivated in typology, is not mathematically well-defined or computationally tractable, making it impossible to use with large and highly variable crosslinguistic datasets. Multidimensional scaling (MDS), in particular the Optimal Classification nonparametric unfolding algorithm, offers a powerful, formalized tool that allows linguists to infer language universals from highly complex and large-scale datasets. We compare our approach to Haspelmath's semantic map analysis of indefinite pronouns, and reanalyze Dahl's (1985) large tense-aspect dataset. MDS works best with large datasets, demonstrating the centrality of grammatical variation in inferring language universals and the importance of examining as wide a range of grammatical behavior as possible both within and across languages.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/QT3NJRKK/Croft and Poole - 2008 - Inferring universals from grammatical variation Multidimensional scaling for typological analysis.pdf}
}

@article{croft-et-al-2008-inferringa,
  title = {Inferring Universals from Grammatical Variation: {{Multidimensional}} Scaling for Typological Analysis},
  shorttitle = {Inferring Universals from Grammatical Variation},
  author = {Croft, William and Poole, Keith T.},
  year = 2008,
  month = jul,
  journal = {Theoretical Linguistics},
  volume = {34},
  number = {1},
  pages = {1--37},
  publisher = {De Gruyter Mouton},
  issn = {1613-4060},
  doi = {10.1515/THLI.2008.001},
  urldate = {2025-09-20},
  abstract = {A fundamental fact about grammatical structure is that it is highly variable both across languages and within languages. Typological analysis has drawn language universals from grammatical variation, in particular by using the semantic map model. But the semantic map model, while theoretically well-motivated in typology, is not mathematically well-defined or computationally tractable, making it impossible to use with large and highly variable crosslinguistic datasets. Multidimensional scaling (MDS), in particular the Optimal Classification nonparametric unfolding algorithm, offers a powerful, formalized tool that allows linguists to infer language universals from highly complex and large-scale datasets. We compare our approach to Haspelmath's semantic map analysis of indefinite pronouns, and reanalyze Dahl's (1985) large tense-aspect dataset. MDS works best with large datasets, demonstrating the centrality of grammatical variation in inferring language universals and the importance of examining as wide a range of grammatical behavior as possible both within and across languages.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/M6K9F73N/Croft and Poole - 2008 - Inferring universals from grammatical variation Multidimensional scaling for typological analysis.pdf}
}

@inproceedings{croft-nivre-2025-verbal,
  title = {Verbal Predication Constructions in {{Universal Dependencies}}},
  booktitle = {Proceedings of the Second International Workshop on Construction Grammars and {{NLP}}},
  author = {Croft, William and Nivre, Joakim},
  editor = {Bonial, Claire and Torgbi, Melissa and Weissweiler, Leonie and Blodgett, Austin and Beuls, Katrien and Van Eecke, Paul and Tayyar Madabushi, Harish},
  year = 2025,
  month = sep,
  pages = {50--60},
  publisher = {Association for Computational Linguistics},
  address = {D\"usseldorf, Germany},
  url = {https://aclanthology.org/2025.cxgsnlp-1.6/},
  abstract = {Is the framework of Universal Dependencies (UD) compatible with findings from linguistic typology about constructions in the world's languages? To address this question, we need to systematically review how UD represents these constructions, and how it handles the range of morphosyntactic variation attested across languages. In this paper, we present the results of such a review focusing on verbal predication constructions. We find that, although UD can represent all major constructions in this area, the guidelines are not completely coherent with respect to the criteria for core argument relations and not completely systematic in the definition of subtypes for nonbasic voice constructions. To improve the overall coherence of the guidelines, we propose a number of revisions for future versions of UD.},
  isbn = {979-8-89176-318-0}
}

@article{cutler-1981-degrees,
  title = {Degrees of Transparency in Word Formation},
  author = {Cutler, Anne},
  year = 1981,
  journal = {Canadian Journal of Linguistics/Revue canadienne de linguistique},
  volume = {26},
  number = {1},
  pages = {73--77},
  publisher = {Cambridge University Press}
}

@incollection{cysouw-2017-building,
  title = {Building Semantic Maps: {{The}} Case of Person Marking},
  booktitle = {New Challenges in Typology},
  author = {Cysouw, Michael},
  editor = {Miestamo, Matti and W{\"a}lchil, Bernard},
  year = 2017,
  pages = {225--248},
  publisher = {De Gruyter Mouton},
  address = {Berlin, Germany}
}

@book{dahl-1985-tense,
  title = {Tense and {{Aspect Systems}}},
  author = {Dahl, {\"O}sten},
  year = 1985,
  publisher = {Blackwell},
  address = {Oxford, UK},
  file = {/Users/coleman/Zotero/storage/DGDXFZBJ/Dahl-1985.html}
}

@article{demarneffe-et-al-2021-universal,
  title = {Universal {{Dependencies}}},
  author = {{\noopsort{marneffe}}{de Marneffe}, Marie-Catherine and Manning, Christopher D. and Nivre, Joakim and Zeman, Daniel},
  year = 2021,
  month = jun,
  journal = {Computational Linguistics},
  volume = {47},
  number = {2},
  pages = {255--308},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  doi = {10.1162/coli_a_00402},
  urldate = {2025-05-14},
  abstract = {Universal dependencies (UD) is a framework for morphosyntactic annotation of human language, which to date has been used to create treebanks for more than 100 languages. In this article, we outline the linguistic theory of the UD framework, which draws on a long tradition of typologically oriented grammatical theories. Grammatical relations between words are centrally used to explain how predicate--argument structures are encoded morphosyntactically in different languages while morphological features and part-of-speech classes give the properties of words. We argue that this theory is a good basis for crosslinguistically consistent annotation of typologically diverse languages in a way that supports computational natural language understanding as well as broader linguistic studies.},
  file = {/Users/coleman/Zotero/storage/8G86LFQD/De Marneffe et al_2021_Universal Dependencies.pdf;/Users/coleman/Zotero/storage/PASLGE4I/de Marneffe et al_2021_Universal Dependencies2.pdf}
}

@inproceedings{deng-et-al-2009-imagenet,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Li, Fei-Fei},
  year = 2009,
  month = jun,
  pages = {248--255},
  publisher = {IEEE},
  address = {Miami, Florida, USA},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2009.5206848},
  urldate = {2025-10-31},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ``ImageNet'', a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500--1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  keywords = {Explosions,Image databases,Image retrieval,Information retrieval,Internet,Large-scale systems,Multimedia databases,Ontologies,Robustness,Spine},
  file = {/Users/coleman/Zotero/storage/HKBX4XG7/5206848.html}
}

@article{denic-et-al-2022-indefinite,
  title = {Indefinite {{Pronouns Optimize}} the {{Simplicity}}/{{Informativeness Trade-Off}}},
  author = {Deni{\'c}, Milica and {Steinert-Threlkeld}, Shane and Szymanik, Jakub},
  year = 2022,
  journal = {Cognitive Science},
  volume = {46},
  number = {5},
  issn = {1551-6709},
  doi = {10.1111/cogs.13142},
  urldate = {2025-11-09},
  abstract = {The vocabulary of human languages has been argued to support efficient communication by optimizing the trade-off between simplicity and informativeness. The argument has been originally based on cross-linguistic analyses of vocabulary in semantic domains of content words, such as kinship, color, and number terms. The present work applies this analysis to a category of function words: indefinite pronouns (e.g., someone, anyone, no one). We build on previous work to establish the meaning space and featural make-up for indefinite pronouns, and show that indefinite pronoun systems across languages optimize the simplicity/informativeness trade-off. This demonstrates that pressures for efficient communication shape both content and function word categories. In doing so, our work aligns with several concurrent studies exploring the simplicity/informativeness trade-off in functional vocabulary. Importantly, we further argue that the trade-off may explain some of the universal properties of indefinite pronouns, thus reducing the explanatory load for linguistic theories.},
  copyright = {\copyright{} 2022 The Authors. Cognitive Science published by Wiley Periodicals LLC on behalf of Cognitive Science Society (CSS).},
  langid = {english},
  keywords = {complexity,efficiency,function words,indefinites,informativeness,linguistic universals,semantics,trade-off},
  file = {/Users/coleman/Zotero/storage/BWVTG3UJ/Deni et al. - 2022 - Indefinite Pronouns Optimize the SimplicityInformativeness Trade-Off.pdf;/Users/coleman/Zotero/storage/FRJ4S6CA/cogs.html}
}

@book{desaussure-1916-cours,
  title = {Cours de Linguistique G\'en\'erale},
  author = {{\noopsort{saussure}}{de Saussure}, Ferdinand},
  year = 1916,
  publisher = {Payot},
  address = {Paris, France}
}

@inproceedings{devlin-et-al-2019-bert,
  title = {{{BERT}}: {{Pre-training}} of Deep Bidirectional Transformers for Language Understanding},
  booktitle = {Proceedings of the 2019 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long and Short Papers)},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = 2019,
  month = jun,
  pages = {4171--4186},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1423},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}

@article{dewit-et-al-2018-epistemic,
  title = {The Epistemic Import of Aspectual Constructions: The Case of Performatives},
  shorttitle = {The Epistemic Import of Aspectual Constructions},
  author = {{\noopsort{wit}}{de Wit}, Astrid and Brisard, Frank and Meeuwis, Michael},
  year = 2018,
  month = jun,
  journal = {Language and Cognition},
  volume = {10},
  number = {2},
  pages = {234--265},
  issn = {1866-9808, 1866-9859},
  doi = {10.1017/langcog.2017.26},
  urldate = {2025-11-09},
  abstract = {In this study we chart the aspectual characteristics of performative utterances in a cross-linguistic sample of sixteen languages on the basis of native-speaker elicitations. We conclude that there is not one single aspectual type (e.g., perfectives) that is systematically reserved for performative contexts. Instead, the aspectual form of performative utterances in a given language is epistemically motivated, in the sense that the language will turn to that aspectual construction which it generally selects to refer to situations that are fully and instantly identifiable as an instance of a given situation type at the time of speaking. We use the method of Multidimensional Scaling to demonstrate this: whatever the exact value of a given aspectual marker, if it is used to mark performatives, then it also commonly features in the expression of states and habits, which have the subinterval property (they can be fully verified based on a random segment), demonstrations, and other special contexts featuring more or less predictable and therefore instantly identifiable events. On the other hand, our study shows that performative contexts do not normally feature progressive aspect, which is dedicated to the expression of events that are not fully and instantly identifiable.},
  langid = {english},
  keywords = {aspect,epistemic modality,full and instant identifiability,Multidimensional Scaling,performativity},
  file = {/Users/coleman/Zotero/storage/4JTVWAAA/Wit et al. - 2018 - The epistemic import of aspectual constructions the case of performatives.pdf}
}

@article{diaz-et-al-2009-comparison,
  title = {A Comparison of Brain Activity Evoked by Single Content and Function Words: {{An fMRI}} Investigation of Implicit Word Processing},
  shorttitle = {A Comparison of Brain Activity Evoked by Single Content and Function Words},
  author = {Diaz, Michele T. and McCarthy, Gregory},
  year = 2009,
  month = jul,
  journal = {Brain Research},
  volume = {1282},
  pages = {38--49},
  issn = {0006-8993},
  doi = {10.1016/j.brainres.2009.05.043},
  urldate = {2025-11-07},
  abstract = {Content and function words have different roles in language and differ greatly in their semantic content. Although previous research has suggested that these different roles may be mediated by different neural substrates, the neuroimaging literature on this topic is particularly scant. Moreover, fMRI studies that have investigated differences between content and function words have utilized tasks that focus the subjects' attention on the differences between these word types. It is possible, then, that task-related differences in attention, working memory, and decision-making contribute to the differential patterns of activation observed. Here, subjects were engaged in a continuous working memory cover task while single, task-irrelevant content and function words were infrequently and irregularly presented. Nonword letter strings were displayed in black font at a fast rate (2/s). Subjects were required to either remember or retrieve occasional nonwords that were presented in colored fonts. Incidental and irrelevant to the memory task, content and function words were interspersed among nonwords at intervals of 12 to 15~s. Both word types strongly activated temporal--parietal cortex, middle and anterior temporal cortex, inferior frontal gyrus, parahippocampal gyrus, and orbital frontal cortex. Activations were more extensive in the left hemisphere. Content words elicited greater activation than function words in middle and anterior temporal cortex, a sub-region of orbital frontal cortex, and the parahippocampal region. Words also evoked extensive deactivation, most notably in brain regions previously associated with working memory and attention.},
  keywords = {Content and function words,fMRI,Language,Semantic processing},
  file = {/Users/coleman/Zotero/storage/ASYXTL6H/Diaz and McCarthy - 2009 - A comparison of brain activity evoked by single content and function words An fMRI investigation of.pdf;/Users/coleman/Zotero/storage/UACU4ZH7/S0006899309009937.html}
}

@article{dixon-1977-where,
  title = {Where Have All the Adjectives Gone?},
  author = {Dixon, Robert M. W.},
  year = 1977,
  journal = {Studies in Language},
  volume = {1},
  number = {1},
  pages = {19--80},
  publisher = {John Benjamins Publishing Company},
  address = {Amsterdam/Philadephia},
  issn = {0378-4177},
  doi = {10.1075/sl.1.1.04dix},
  aiatsis_callnumber = {MS 163 1 Manuscript and pamphlet manuscripts Closed Access Stacks},
  aiatsis_code = {Y123},
  aiatsis_reference_language = {DYIRBAL},
  citekeys = {langsci120:dixon1977 langsci223:dixon1977 langsci321:dixon1977 langsci336:dixon1979 langsci67:dixon1977 langsci73:dixon1977 langsci80:dixon1977},
  hhtype = {specific\_feature},
  inlg = {English [eng]},
  isreferencedby = {langsci120 langsci223 langsci321 langsci336 langsci67 langsci73 langsci80},
  lgcode = {Dyirbal [dbl]},
  macro_area = {Australia},
  ozbib_id = {1524},
  ozbibnote = {[See also Dixon 1982]},
  ozbibreftype = {17},
  src = {benjamins, hh, langsci, ozbib},
  file = {/Users/coleman/Zotero/storage/KRENSJNR/sl.1.1.html}
}

@inproceedings{dosovitskiy-et-al-2021-image,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  booktitle = {9th {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2021, {{Virtual Event}}, {{Austria}}, {{May}} 3-7, 2021},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = 2021,
  publisher = {OpenReview.net},
  url = {https://openreview.net/forum?id=YicbFdNTTy},
  urldate = {2025-11-08},
  file = {/Users/coleman/Zotero/storage/6RGTCCRY/Dosovitskiy et al. - 2020 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf}
}

@article{dressler-1989-prototypical,
  title = {Prototypical Differences between Inflection and Derivation},
  author = {Dressler, Wolfgang U.},
  year = 1989,
  journal = {STUF-Language Typology and Universals},
  volume = {42},
  number = {1},
  pages = {3--10},
  publisher = {De Gruyter (A)}
}

@article{dryer-1989-large,
  title = {Large {{Linguistic Areas}} and {{Language Sampling}}},
  author = {Dryer, Matthew S.},
  year = 1989,
  month = jan,
  journal = {Studies in Language. International Journal sponsored by the Foundation ``Foundations of Language''},
  volume = {13},
  number = {2},
  pages = {257--292},
  publisher = {John Benjamins},
  issn = {0378-4177, 1569-9978},
  doi = {10.1075/sl.13.2.03dry},
  urldate = {2025-11-07},
  abstract = {Welcome to e-content platform of John Benjamins Publishing Company. Here you can find all of our electronic books and journals, for purchase and download or subscriber access.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/FSFYHUIE/sl.13.2.html}
}

@incollection{dryer-1997-are,
  title = {Are {{Grammatical Relations Universal}}?},
  booktitle = {Essays on {{Language Function}} and {{Language Type}}},
  author = {Dryer, Matthew S.},
  editor = {Bybee, Joan L. and Haiman, John and Thompson, Sandra A.},
  year = 1997,
  pages = {115},
  publisher = {John Benjamins Publishing Company},
  address = {Amsterdam},
  doi = {10.1075/z.82.09dry},
  urldate = {2024-05-15},
  isbn = {978-90-272-2168-1 978-1-55619-522-8 978-90-272-7421-2},
  langid = {english}
}

@article{dube-et-al-2014-independent,
  title = {Independent Effects of Imageability and Grammatical Class in Synonym Judgement in Aphasia.},
  author = {Dub{\'e}, Catherine and Monetta, Laura and {Mart{\'i}nez-Cuiti{\~n}o}, Mar{\'i}a Macarena and Wilson, Maximiliano A.},
  year = 2014,
  journal = {Psicothema},
  volume = {26},
  number = {4},
  pages = {449--456},
  address = {Spain},
  issn = {1886-144X 0214-9915},
  doi = {10.7334/psicothema2014.31},
  abstract = {BACKGROUND: The grammatical class effect in aphasia, i.e. dissociated processing of words according to their respective grammatical class, has been attributed to either grammatical, lexical or semantic (i.e., imageability) deficits. This study explores the hypotheses of impaired semantic treatment as the source of the grammatical class effect in aphasia. METHOD: A synonym judgement task that includes nouns and verbs of high and low imageability has been administered to 30 Spanish-speaking patients suffering from receptive or productive aphasia and 30 controls. RESULTS: Normal controls performed significantly better than aphasic patients. Although globally the productive aphasics performed significantly better than the receptive aphasics, grammatical class (nouns better than verbs) and imageability (high imageability better than low imageability) affected performance in both subgroups. No significant interaction emerged between these two factors. CONCLUSION: The results suggest that the grammatical class effect may emerge from semantic impairment and that it is -at least partially- independent of the imageability of words.},
  langid = {english},
  pmid = {25340890},
  keywords = {*Imagination,*Linguistics,*Vocabulary,Aphasia/*psychology,Argentina,Female,Humans,Male,Middle Aged}
}

@article{elgamal2025,
  title = {Spatial Language and Intuitive Physics in Children and Adults: {{It}}'s Not so Simple},
  shorttitle = {Spatial Language and Intuitive Physics in Children and Adults},
  author = {Elgamal, Karima and Pasquinelli, Rennie and Lakusta, Laura and Landau, Barbara},
  year = 2025,
  journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {47},
  number = {0},
  urldate = {2025-09-08},
  abstract = {Do simple spatial terms such as in or on map directly to intuitive physical judgements about spatial relationships between objects that underlie these terms' meaning? We explored this question in the domain of physical support. Adults (N=120) and 4-year-old children (N=42) were shown videos in which a puppet placed an L-shaped object in contact with a table at locations that varied in whether the object was supported or not. Half of the participants were asked for linguistic judgments ("Is X on Y?") and half were asked for intuitive physics judgments ("Will X fall if (agent) lets go?"). Results revealed that linguistic judgments were largely categorical, with child and adult participants labeling objects as on even when the object was not truly supported. In contrast, intuitive physics judgments aligned closely with the object's actual possibility of true support. However, responses also varied by the orientation of the L-shaped object, with on applying categorically to a regularly oriented L, but in a more graded fashion for a mirror image oriented L. Our findings suggest that the mapping between the simple spatial term on and physical reasoning systems are not completely coupled, and that the ways in which language draws on intuitive physical reasoning is complex.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/PYVMXSGL/Elgamal et al. - 2025 - Spatial language and intuitive physics in children and adults It's not so simple.pdf}
}

@article{elman-2004-alternative,
  title = {An Alternative View of the Mental Lexicon},
  author = {Elman, Jeffrey L},
  year = 2004,
  month = jul,
  journal = {Trends in Cognitive Sciences},
  volume = {8},
  number = {7},
  pages = {301--306},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2004.05.003},
  urldate = {2025-09-20},
  abstract = {An essential aspect of knowing language is knowing the words of that language. This knowledge is usually thought to reside in the mental lexicon, a kind of dictionary that contains information regarding a word's meaning, pronunciation, syntactic characteristics, and so on. In this article, a very different view is presented. In this view, words are understood as stimuli that operate directly on mental states. The phonological, syntactic and semantic properties of a word are revealed by the effects it has on those states.}
}

@article{embick-2021-motivation,
  title = {The {{Motivation}} for {{Roots}} in {{Distributed Morphology}}},
  author = {Embick, David},
  year = 2021,
  month = jan,
  journal = {Annual Review of Linguistics},
  volume = {7},
  number = {1},
  pages = {69--88},
  issn = {2333-9683, 2333-9691},
  doi = {10.1146/annurev-linguistics-040620-061341},
  urldate = {2025-10-07},
  abstract = {Within Distributed Morphology, it has been proposed that the lexical vocabulary consists of Roots: category-less primitives. The motivation for Roots is connected with a line of argument reaching back to Chomsky's ``Remarks on Nominalization'' concerning the representation of lexical categories and their role in syntax. At the center of the theory of Roots is the Two Domains Intuition: the idea that there are two different types of domains in which grammatical interactions (form: allomorphy; meaning: allosemy) occur. Roots are posited as part of an argument against lexicalist approaches to the Two Domains Intuition that reduce it to a modular distinction between the lexicon and the syntax. In place of the modular distinction, Root-based approaches hypothesize that domain differences are derivative of syntactic locality effects in a way that connects with the phase theory of Minimalist syntax. This review examines developments leading to current versions of a Roots-and-contexts theory. A particular focus is on the idea that separating lexical Roots from the morphemes that categorize them is essential to defining the distinct locality domains that are posited to explain the effects subsumed under the Two Domains Intuition.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/YA83G3S3/Embick - 2021 - The Motivation for Roots in Distributed Morphology.pdf}
}

@inproceedings{ettinger-et-al-2016-evaluating,
  title = {Evaluating Vector Space Models Using Human Semantic Priming Results},
  booktitle = {Proceedings of the 1st {{Workshop}} on {{Evaluating Vector-Space Representations}} for {{NLP}}},
  author = {Ettinger, Allyson and Linzen, Tal},
  year = 2016,
  month = aug,
  pages = {72--77},
  publisher = {Association for Computational Linguistics},
  address = {Berlin, Germany},
  doi = {10.18653/v1/W16-2513},
  urldate = {2025-10-29},
  file = {/Users/coleman/Zotero/storage/LWG8PWIL/Ettinger and Linzen - 2016 - Evaluating vector space models using human semantic priming results.pdf}
}

@inproceedings{feucht-et-al-2025-dualroute,
  title = {The {{Dual-Route Model}} of {{Induction}}},
  booktitle = {Proceedings of the {{Second Conference}} on {{Language Modeling}}},
  author = {Feucht, Sheridan and Todd, Eric and Wallace, Byron C. and Bau, David},
  year = 2025,
  month = aug,
  publisher = {OpenReview.net},
  address = {Montreal, Canada},
  url = {https://openreview.net/forum?id=bNTrKqqnG9#discussion},
  urldate = {2025-10-30},
  abstract = {Prior work on in-context copying has shown the existence of *induction heads*, which attend to and promote individual tokens during copying. In this work we discover a new type of induction head: *concept-level* induction heads, which copy entire lexical units instead of individual tokens. Concept induction heads learn to attend to the ends of multi-token words throughout training, working in parallel with token-level induction heads to copy meaningful text. We show that these heads are responsible for semantic tasks like word-level translation, whereas token induction heads are vital for tasks that can only be done verbatim (like copying nonsense tokens). These two "routes" operate independently: we show that ablation of token induction heads causes models to paraphrase where they would otherwise copy verbatim. By patching concept induction head outputs, we find that they contain language-independent word representations that mediate natural language translation, suggesting that LLMs represent abstract word meanings independent of language or form.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/4AF7R36L/Feucht et al. - 2025 - The Dual-Route Model of Induction.pdf}
}

@incollection{firth-1957-synopsis,
  title = {A Synopsis of Linguistic Theory, 1930--55},
  booktitle = {Studies in Linguistic Analysis},
  author = {Firth, J. R.},
  year = 1957,
  pages = {1--31},
  publisher = {Blackwell},
  address = {Oxford, UK}
}

@article{floyd-2011-rediscovering,
  title = {Re-Discovering the {{Quechua}} Adjective},
  author = {Floyd, Simeon},
  year = 2011,
  month = jan,
  journal = {Linguistic Typology},
  volume = {15},
  number = {1},
  pages = {25--63},
  issn = {1430-0532, 1613-415X},
  doi = {10.1515/lity.2011.003},
  urldate = {2024-10-07},
  file = {/Users/coleman/Zotero/storage/4FCFZ47W/Floyd_2011_Re-discovering the Quechua adjective.pdf}
}

@article{fortescue-1980-affix,
  title = {Affix {{Ordering}} in {{West Greenlandic Derivational Processes}}},
  author = {Fortescue, M. D.},
  year = 1980,
  journal = {International Journal of American Linguistics},
  volume = {46},
  number = {4},
  eprint = {1264708},
  eprinttype = {jstor},
  pages = {259--278},
  publisher = {University of Chicago Press},
  issn = {0020-7071},
  url = {https://www.jstor.org/stable/1264708},
  urldate = {2025-11-09},
  file = {/Users/coleman/Zotero/storage/MZGQYVH6/Fortescue - 1980 - Affix Ordering in West Greenlandic Derivational Processes.pdf}
}

@article{friederici-et-al-2000-segregating,
  title = {Segregating Semantic and Syntactic Aspects of Processing in the Human Brain: An {{fMRI}} Investigation of Different Word Types},
  shorttitle = {Segregating Semantic and Syntactic Aspects of Processing in the Human Brain},
  author = {Friederici, Angela D. and Opitz, Bertram and {\noopsort{cramon}}{von Cramon}, D. Yves},
  year = 2000,
  month = jul,
  journal = {Cerebral Cortex},
  volume = {10},
  number = {7},
  pages = {698--705},
  issn = {1047-3211},
  doi = {10.1093/cercor/10.7.698},
  abstract = {The processing of single words that varied in their semantic (concrete/abstract word) and syntactic (content/function word) status was investigated under different task demands (semantic/ syntactic task) in an event-related functional magnetic resonance imaging experiment. Task demands to a large degree determined which subparts of the neuronal network supporting word processing were activated. Semantic task demands selectively activated the left pars triangularis of the inferior frontal gyrus (BA 45) and the posterior part of the left middle/superior temporal gyrus (BA 21/22/37). In contrast, syntactic processing requirements led to an increased activation in the inferior tip of the left frontal operculum (BA 44) and the cortex lining the junction of the inferior frontal and inferior precentral sulcus (BA 44/6). Moreover, for these latter areas a word class by concreteness interaction was observed when a syntactic judgement was required. This interaction can be interpreted as a prototypicality effect: non-prototypical members of a word class, i.e. concrete function words and abstract content words, showed a larger activation than prototypical members, i.e. abstract function words and concrete content words. The combined data suggest that the activation pattern underlying word processing is predicted neither by syntactic class nor semantic concreteness but, rather, by task demands focusing either on semantic or syntactic aspects. Thus, our findings that semantic and syntactic aspects of processing are both functionally distinct and involve different subparts of the neuronal network underlying word processing support a domain-specific organization of the language system.},
  langid = {english},
  pmid = {10906316},
  keywords = {Adult,Brain Mapping,Female,Humans,Language,Linguistics,Magnetic Resonance Imaging,Male,Mental Processes,Nerve Net,Semantics},
  file = {/Users/coleman/Zotero/storage/ITDSPC3N/Friederici et al. - 2000 - Segregating semantic and syntactic aspects of processing in the human brain an fMRI investigation o.pdf}
}

@article{futrell-et-al-2015-largescale,
  title = {Large-Scale Evidence of Dependency Length Minimization in 37 Languages},
  author = {Futrell, Richard and Mahowald, Kyle and Gibson, Edward},
  year = 2015,
  month = aug,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {112},
  number = {33},
  pages = {10336--10341},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1502134112},
  urldate = {2025-05-14},
  abstract = {Explaining the variation between human languages and the constraints on that variation is a core goal of linguistics. In the last 20 y, it has been claimed that many striking universals of cross-linguistic variation follow from a hypothetical principle that dependency length---the distance between syntactically related words in a sentence---is minimized. Various models of human sentence production and comprehension predict that long dependencies are difficult or inefficient to process; minimizing dependency length thus enables effective communication without incurring processing difficulty. However, despite widespread application of this idea in theoretical, empirical, and practical work, there is not yet large-scale evidence that dependency length is actually minimized in real utterances across many languages; previous work has focused either on a small number of languages or on limited kinds of data about each language. Here, using parsed corpora of 37 diverse languages, we show that overall dependency lengths for all languages are shorter than conservative random baselines. The results strongly suggest that dependency length minimization is a universal quantitative property of human languages and support explanations of linguistic variation in terms of general properties of human information processing.},
  file = {/Users/coleman/Zotero/storage/4TITQN37/Futrell et al_2015_Large-scale evidence of dependency length minimization in 37 languages.pdf}
}

@inproceedings{futrell-et-al-2015-quantifying,
  title = {Quantifying {{Word Order Freedom}} in {{Dependency Corpora}}},
  booktitle = {Proceedings of the {{Third International Conference}} on {{Dependency Linguistics}} ({{Depling}} 2015)},
  author = {Futrell, Richard and Mahowald, Kyle and Gibson, Edward},
  editor = {Nivre, Joakim and Haji{\v c}ov{\'a}, Eva},
  year = 2015,
  month = aug,
  pages = {91--100},
  publisher = {Uppsala University, Uppsala, Sweden},
  address = {Uppsala, Sweden},
  url = {https://aclanthology.org/W15-2112/},
  urldate = {2025-05-14},
  file = {/Users/coleman/Zotero/storage/QXQTN3UU/Futrell et al_2015_Quantifying Word Order Freedom in Dependency Corpora.pdf}
}

@article{futrell-et-al-2022-information,
  title = {Information Theory as a Bridge between Language Function and Language Form},
  author = {Futrell, Richard and Hahn, Michael},
  year = 2022,
  month = may,
  journal = {Frontiers in Communication},
  volume = {7},
  publisher = {Frontiers},
  issn = {2297-900X},
  doi = {10.3389/fcomm.2022.657725},
  urldate = {2025-11-03},
  abstract = {Formal and functional theories of language seem disparate, because formal theories answer the question of what a language is, while functional theories answer the question of what functions it serves. We argue that information theory provides a bridge between these two approaches, via a principle of minimization of complexity under constraints. Synthesizing recent work, we show how information-theoretic characterizations of functional complexity lead directly to mathematical descriptions of the forms of possible languages, in terms of solutions to constrained optimization problems.We show how certain linguistic descriptive formalisms can be recovered as solutions to such problems. Furthermore, we argue that information theory lets us define complexity in a way which has minimal dependence on the choice of theory or descriptive formalism. We illustrate this principle using recently-obtained results on universals of word and morpheme order.},
  langid = {english},
  keywords = {Complexity,Information Theory,Language,Linguistic theory,Psycholinguistics},
  file = {/Users/coleman/Zotero/storage/NRCZMTGF/Futrell and Hahn - 2022 - Information Theory as a Bridge Between Language Function and Language Form.pdf}
}

@article{futrell-et-al-2025-how,
  title = {How Linguistics Learned to Stop Worrying and Love the Language Models},
  author = {Futrell, Richard and Mahowald, Kyle},
  year = 2025,
  month = jul,
  journal = {Behavioral and Brain Sciences},
  pages = {1--98},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X2510112X},
  urldate = {2025-10-31},
  abstract = {Language models can produce fluent, grammatical text. Nonetheless, some maintain that language models don't really learn language and also that, even if they did, that would not be informative for the study of human learning and processing. On the other side, there have been claims that the success of LMs obviates the need for studying linguistic theory and structure. We argue that both extremes are wrong. LMs can contribute to fundamental questions about linguistic structure, language processing, and learning. They force us to rethink arguments and ways of thinking that have been foundational in linguistics. While they do not replace linguistic structure and theory, they serve as model systems and working proofs of concept for gradient, usage-based approaches to language. We offer an optimistic take on the relationship between language models and linguistics.},
  langid = {english},
  keywords = {functional linguistics,information theory,language learning,language models,linguistic theory,neural networks,statistical learning},
  file = {/Users/coleman/Zotero/storage/B4XXREDL/Futrell and Mahowald - 2025 - How Linguistics Learned to Stop Worrying and Love the Language Models.pdf}
}

@article{garrod-et-al-1999-investigating,
  title = {In and on: Investigating the Functional Geometry of Spatial Prepositions},
  shorttitle = {In and On},
  author = {Garrod, S. and Ferrier, G. and Campbell, S.},
  year = 1999,
  month = sep,
  journal = {Cognition},
  volume = {72},
  number = {2},
  pages = {167--189},
  issn = {0010-0277},
  doi = {10.1016/s0010-0277(99)00038-4},
  abstract = {Spatial prepositions such as in and on seem to denote semantically indeterminate spatial relations. This reflects, in part, the physical relationships between the objects in the scenes that they are used to portray. We argue that such physical relationships are best represented in terms of an inherently dynamic functional geometry which incorporates notions of location control. Two experiments are reported. They investigate the degree to which independent judgements of location control predict choice of description across a range of scenes. The results show that judgements of location control predict viewer's choice of description under certain circumstances. In the absence of prototypical geometric relations, control information has a strong influence on choice of description. However, when the scenes portray prototypical geometric relations, control information has less of an effect. The results support a hybrid account of the semantic representation underlying the prepositions with both a geometric and a functional component to it.},
  langid = {english},
  pmid = {10553670},
  keywords = {Adult,Female,Humans,Language,Male,Mental Processes,Semantics,Space Perception}
}

@article{geeraerts-chapter,
  title = {Chapter 4 {{Prototype}} Theory},
  author = {Geeraerts, Dirk},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/PZ2WDC6J/Geeraerts - Chapter 4 Prototype theory.pdf}
}

@misc{gemmateam-2024-gemma,
  title = {Gemma: {{Open}} Models Based on {{Gemini}} Research and Technology},
  author = {Gemma Team},
  year = 2024,
  eprint = {2403.08295},
  primaryclass = {cs.CL},
  url = {https://arxiv.org/abs/2403.08295},
  archiveprefix = {arXiv}
}

@article{gerdes-et-al-2021-typometrics,
  title = {Typometrics: {{From Implicational}} to {{Quantitative Universals}} in {{Word Order Typology}}},
  shorttitle = {Typometrics},
  author = {Gerdes, Kim and Kahane, Sylvain and Chen, Xinying},
  year = 2021,
  month = feb,
  journal = {Glossa: a journal of general linguistics},
  volume = {6},
  number = {1},
  publisher = {Open Library of Humanities},
  issn = {2397-1835},
  doi = {10.5334/gjgl.764},
  urldate = {2025-05-14},
  abstract = {This paper develops the concept of word order universals based on a data analysis of the~Universal Dependencies project, which proposes treebanks of more than 90 languages~encoded with the same annotation scheme. The nature of the data we work on allows~us to extract rich details for testing well-known typological implicational universals~and, further, explore new kinds of universals that we call quantitative universals. We~show how such quantitative universals are in essence different from implicational~universals, including statistical universals, by the fact that they no longer lay down~any claims on categorical statements, but rather on continuous parameters, opening~a new field of research we propose to call typometrics.},
  copyright = {Copyright: \copyright{} 2021 The Author(s).                     This is an open-access article distributed under the terms of the                        Creative Commons Attribution 4.0 International License (CC-BY 4.0), which                        permits unrestricted use, distribution, and reproduction in any medium,                        provided the original author and source are credited. See http://creativecommons.org/licenses/by/4.0/.},
  langid = {english},
  keywords = {thesislit},
  file = {/Users/coleman/Zotero/storage/3YTXVX6Q/Gerdes et al_2021_Typometrics.pdf}
}

@article{gerdes-et-al-2021-typometricsa,
  title = {Typometrics: {{From Implicational}} to {{Quantitative Universals}} in {{Word Order Typology}}},
  shorttitle = {Typometrics},
  author = {Gerdes, Kim and Kahane, Sylvain and Chen, Xinying},
  year = 2021,
  month = feb,
  journal = {Glossa: a journal of general linguistics},
  volume = {6},
  number = {1},
  publisher = {Open Library of Humanities},
  issn = {2397-1835},
  doi = {10.5334/gjgl.764},
  urldate = {2025-09-20},
  abstract = {This paper develops the concept of word order universals based on a data analysis of the~Universal Dependencies project, which proposes treebanks of more than 90 languages~encoded with the same annotation scheme. The nature of the data we work on allows~us to extract rich details for testing well-known typological implicational universals~and, further, explore new kinds of universals that we call quantitative universals. We~show how such quantitative universals are in essence different from implicational~universals, including statistical universals, by the fact that they no longer lay down~any claims on categorical statements, but rather on continuous parameters, opening~a new field of research we propose to call typometrics.},
  copyright = {Copyright: \copyright{} 2021 The Author(s).                     This is an open-access article distributed under the terms of the                        Creative Commons Attribution 4.0 International License (CC-BY 4.0), which                        permits unrestricted use, distribution, and reproduction in any medium,                        provided the original author and source are credited. See http://creativecommons.org/licenses/by/4.0/.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/SJYZVDDP/Gerdes et al. - 2021 - Typometrics From Implicational to Quantitative Universals in Word Order Typology.pdf}
}

@article{gibson-et-al-2017-color,
  title = {Color Naming across Languages Reflects Color Use},
  author = {Gibson, Edward and Futrell, Richard and {Jara-Ettinger}, Julian and Mahowald, Kyle and Bergen, Leon and Ratnasingam, Sivalogeswaran and Gibson, Mitchell and Piantadosi, Steven T. and Conway, Bevil R.},
  year = 2017,
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {114},
  number = {40},
  pages = {10785--10790},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1619666114},
  urldate = {2025-11-09},
  abstract = {What determines how languages categorize colors? We analyzed results of the World Color Survey (WCS) of 110 languages to show that despite gross differences across languages, communication of chromatic chips is always better for warm colors (yellows/reds) than cool colors (blues/greens). We present an analysis of color statistics in a large databank of natural images curated by human observers for salient objects and show that objects tend to have warm rather than cool colors. These results suggest that the cross-linguistic similarity in color-naming efficiency reflects colors of universal usefulness and provide an account of a principle (color use) that governs how color categories come about. We show that potential methodological issues with the WCS do not corrupt information-theoretic analyses, by collecting original data using two extreme versions of the color-naming task, in three groups: the Tsimane', a remote Amazonian hunter-gatherer isolate; Bolivian-Spanish speakers; and English speakers. These data also enabled us to test another prediction of the color-usefulness hypothesis: that differences in color categorization between languages are caused by differences in overall usefulness of color to a culture. In support, we found that color naming among Tsimane' had relatively low communicative efficiency, and the Tsimane' were less likely to use color terms when describing familiar objects. Color-naming among Tsimane' was boosted when naming artificially colored objects compared with natural objects, suggesting that industrialization promotes color usefulness.},
  file = {/Users/coleman/Zotero/storage/Z82AIJKY/Gibson et al. - 2017 - Color naming across languages reflects color use.pdf}
}

@book{givon-1979-understanding,
  title = {On {{Understanding Grammar}}},
  author = {Giv{\'o}n, Talmy},
  year = 1979,
  series = {Perspectives in Neurolinguistics and Psycholinguistics},
  publisher = {Academic Press},
  address = {New York, New York, USA},
  isbn = {978-0-12-285450-7},
  lccn = {78067876}
}

@book{givon-1984-syntax,
  title = {Syntax: {{A}} Functional-Typological Introduction},
  author = {Givon, T.},
  year = 1984,
  journal = {z.17},
  volume = {1},
  publisher = {John Benjamins Publishing Company},
  address = {Amsterdam, Netherlands},
  url = {https://benjamins.com/catalog/z.17},
  urldate = {2025-11-09},
  abstract = {This two-volume work on syntax views grammar as a non-arbitrary language-processing device, to be understood in terms of the various substantive parameters relevant to language: communicative function, cognitive processing, socio-culture and neuro-biology.},
  isbn = {978-90-272-3013-3},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/Y86S6GV8/z.html}
}

@book{givon-2018-understanding,
  title = {On {{Understanding Grammar}}: {{Revised}} Edition},
  shorttitle = {On {{Understanding Grammar}}},
  author = {Giv{\'o}n, Talmy},
  year = 2018,
  month = mar,
  publisher = {John Benjamins Publishing Company},
  address = {Amsterdam},
  doi = {10.1075/z.213},
  urldate = {2025-09-06},
  abstract = {In his foreword to the original edition of this classic of functionalism, typology and diachrony, Dwight Bolinger wrote: "I foresee it as one of the truly prizes statements of our current knowledge\dots a book about understanding done with deep understanding -- of language and its place in Nature and in the nature of humankind\dots{} The book is rich in insights, even for those who have been with linguistics for a long time. And beginners could be thankful for having it as a starting point, from which so many past mistakes have been shed". Thoroughly revised, corrected and updated, On Understanding Grammar remains, as its author intended it in 1979, a book about trying to make sense of human language and of doing linguistics. Language is considered here from multiple perspectives, intersecting with cognition and communication, typology and universals, grammaticalization, development and evolution. Within such a broad cross-disciplinary context, grammar is viewed as an automated, structured language-processing device, assembled through evolution, diachrony and use. Cross-language diversity is not arbitrary, but rather is tightly constrained and adaptively motivated, with the balance between universality and diversity mediated through development, be it evolutionary or diachronic. The book's take on language harkens back to the works of illustrious antecedents such as F. Bopp, W. von Humbold, H. Paul, A. Meillet, O. Jespersen and G. Zipf, offering a coherent alternative to the methodological and theoretical strictures of Saussure, Bloomfield and Chomsky.},
  isbn = {978-90-272-1252-8 978-90-272-6471-8},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/9RHDYBI8/Givn - 2018 - On Understanding Grammar Revised edition.pdf}
}

@article{goh-et-al-2021-multimodal,
  title = {Multimodal Neurons in Artificial Neural Networks},
  author = {Goh, Gabriel and Cammarata, Nick and Voss, Chelsea and Carter, Shan and Petrov, Michael and Schubert, Ludwig and Radford, Alec and Olah, Chris},
  year = 2021,
  month = mar,
  journal = {Distill},
  volume = {6},
  number = {3},
  issn = {2476-0757},
  doi = {10.23915/distill.00030},
  urldate = {2025-11-01},
  abstract = {We report the existence of multimodal neurons in artificial neural networks, similar to those found in the human brain.},
  langid = {english},
  note = {Article e30},
  file = {/Users/coleman/Zotero/storage/9ZKIEHXB/multimodal-neurons.html}
}

@article{goldberg-2024-usagebased,
  title = {Usage-Based Constructionist Approaches and Large Language Models},
  author = {Goldberg, Adele E.},
  year = 2024,
  month = oct,
  journal = {Constructions and Frames},
  volume = {16},
  number = {2},
  pages = {220--254},
  publisher = {John Benjamins},
  issn = {1876-1933, 1876-1941},
  doi = {10.1075/cf.23017.gol},
  urldate = {2025-11-04},
  abstract = {Abstract The constructionist framework is more relevant than ever, due to efforts by a broad range of researchers across the globe, a steady increase in the use of corpus and experimental methods among linguists, consistent findings from laboratory phonology, neuroscience, sociolinguistics, and striking progress in transformer-based large language models. These advances promise exciting developments and a great deal more clarity over the next decade. The constructionist approach rests on two interrelated but distinguishable tenets: a recognition that constructions pair form with function at varying levels of specificity and abstraction, and the recognition that our knowledge and use of language are dynamic and based on language use.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/ZWVACSV6/cf.23017.html}
}

@book{gordon-2016-phonological,
  title = {Phonological {{Typology}}},
  author = {Gordon, Matthew K.},
  year = 2016,
  month = apr,
  publisher = {Oxford University Press},
  doi = {10.1093/acprof:oso/9780199669004.001.0001},
  urldate = {2024-10-07},
  isbn = {978-0-19-966900-4}
}

@article{grand-et-al-2022-semantic,
  title = {Semantic Projection Recovers Rich Human Knowledge of Multiple Object Features from Word Embeddings},
  author = {Grand, Gabriel and Blank, Idan Asher and Pereira, Francisco and Fedorenko, Evelina},
  year = 2022,
  month = jul,
  journal = {Nature Human Behaviour},
  volume = {6},
  number = {7},
  pages = {975--987},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-022-01316-8},
  urldate = {2025-10-07},
  abstract = {How is knowledge about word meaning represented in the mental lexicon? Current computational models infer word meanings from lexical co-occurrence patterns. They learn to represent words as vectors in a multidimensional space, wherein words that are used in more similar linguistic contexts---that is, are more semantically related---are located closer together. However, whereas inter-word proximity captures only overall relatedness, human judgements are highly context dependent. For example, dolphins and alligators are similar in size but differ in dangerousness. Here, we use a domain-general method to extract context-dependent relationships from word embeddings: `semantic projection' of word-vectors onto lines that represent features such as size (the line connecting the words `small' and `big') or danger (`safe' to `dangerous'), analogous to `mental scales'. This method recovers human judgements across various object categories and properties. Thus, the geometry of word embeddings explicitly represents a wealth of context-dependent world knowledge.},
  copyright = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Human behaviour,Language and linguistics,Psychology},
  file = {/Users/coleman/Zotero/storage/UTXTAR8I/Grand et al. - 2022 - Semantic projection recovers rich human knowledge of multiple object features from word embeddings.pdf}
}

@incollection{greenberg-1966-universals,
  title = {Some Universals of Grammar with Particular Reference to the Order of Meaningful Elements},
  booktitle = {Universals of Language},
  author = {Greenberg, Joseph H.},
  editor = {Greenberg, Joseph H.},
  year = 1966,
  edition = {2nd},
  pages = {73--113},
  publisher = {M.I.T Press},
  address = {Cambridge, Massachusetts, USA},
  isbn = {978-0-262-57008-4},
  langid = {english}
}

@inproceedings{gregorio-et-al-2025-crosslinguistic,
  title = {The Cross-Linguistic Role of Animacy in Grammar Structures},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Gregorio, Nina and Gay, Matteo and Goldwater, Sharon and Ponti, Edoardo},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  year = 2025,
  month = jul,
  pages = {7349--7363},
  publisher = {Association for Computational Linguistics},
  address = {Vienna, Austria},
  doi = {10.18653/v1/2025.acl-long.364},
  urldate = {2025-10-07},
  abstract = {Animacy is a semantic feature of nominals and follows a hierarchy: personal pronouns \textbackslash ensuremath{$>$} human \textbackslash ensuremath{$>$} animate \textbackslash ensuremath{$>$} inanimate. In several languages, animacy imposes hard constraints on grammar. While it has been argued that these constraints may emerge from universal soft tendencies, it has been difficult to provide empirical evidence for this conjecture due to the lack of data annotated with animacy classes. In this work, we first propose a method to reliably classify animacy classes of nominals in 11 languages from 5 families, leveraging multilingual large language models (LLMs) and word sense disambiguation datasets. Then, through this newly acquired data, we verify that animacy displays consistent cross-linguistic tendencies in terms of preferred morphosyntactic constructions, although not always in line with received wisdom: animacy in nouns correlates with the alignment role of agent, early positions in a clause, and syntactic pivot (e.g., for relativisation), but not necessarily with grammatical subjecthood. Furthermore, the behaviour of personal pronouns in the hierarchy is idiosyncratic as they are rarely plural and relativised, contrary to high-animacy nouns.},
  isbn = {979-8-89176-251-0},
  file = {/Users/coleman/Zotero/storage/DUB9D6ST/Gregorio et al. - 2025 - The Cross-linguistic Role of Animacy in Grammar Structures.pdf}
}

@article{gyevnar-et-al-2022-communicative,
  title = {Communicative {{Efficiency}} or {{Iconic Learning}}: {{Do Acquisition}} and {{Communicative Pressures Interact}} to {{Shape Colour- Naming Systems}}?},
  shorttitle = {Communicative {{Efficiency}} or {{Iconic Learning}}},
  author = {Gyevnar, Balint and Dagan, Gautier and Haley, Coleman and Guo, Shangmin and Mollica, Frank},
  year = 2022,
  month = nov,
  journal = {Entropy},
  volume = {24},
  number = {11},
  pages = {1542},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e24111542},
  urldate = {2025-11-09},
  abstract = {Language evolution is driven by pressures for simplicity and informativity; however, the timescale on which these pressures operate is debated. Over several generations, learners' biases for simple and informative systems can guide language evolution. Over repeated instances of dyadic communication, the principle of least effort dictates that speakers should bias systems towards simplicity and listeners towards informativity, similarly guiding language evolution. At the same time, it has been argued that learners only provide a bias for simplicity and, thus, language users must provide a bias for informativity. To what extent do languages evolve during acquisition versus use? We address this question by formally defining and investigating the communicative efficiency of acquisition trajectories. We illustrate our approach using colour-naming systems, replicating a communicative efficiency model based on the information bottleneck problem, and an acquisition model based on self-organising maps. We find that to the extent that language is iconic, learning alone is sufficient to shape language evolution. Regarding colour-naming systems specifically, we find that incorporating learning biases into communicative efficiency accounts might explain how speakers and listeners trade off communicative effort.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {colour-naming systems,communicative efficiency,information bottleneck,language evolution},
  file = {/Users/coleman/Zotero/storage/2U7VM265/Gyevnar et al. - 2022 - Communicative Efficiency or Iconic Learning Do Acquisition and Communicative Pressures Interact to.pdf}
}

@article{haiman-1980-iconicity,
  title = {The Iconicity of Grammar: Isomorphism and Motivation},
  shorttitle = {The {{Iconicity}} of {{Grammar}}},
  author = {Haiman, John},
  year = 1980,
  journal = {Language},
  volume = {56},
  number = {3},
  eprint = {414448},
  eprinttype = {jstor},
  pages = {515--540},
  publisher = {Linguistic Society of America},
  issn = {0097-8507},
  doi = {10.2307/414448},
  urldate = {2025-01-28},
  abstract = {Although linguistic signs in isolation are symbolic, the system or grammar which relates them may be diagrammatically iconic in two ways: (a) by isomorphism, a bi-unique correspondence tends to be established between signans and signatum; (b) by motivation, the structure of language directly reflects some aspect of the structure of reality. Isomorphism is so nearly universal that deviations from it require explanation. Motivation, although widespread, establishes a typology of languages, as indicated in Saussure's Cours. The evidence of artificial taboo languages suggests that degree of motivation co-varies inversely with the number of 'prima onomata' in the lexicon.},
  keywords = {thesislit},
  file = {/Users/coleman/Zotero/storage/KCT26EW3/Haiman_1980_The Iconicity of Grammar.pdf}
}

@article{haley-2025-unlocking,
  title = {Unlocking Finite-State Morphological Transducers: {{Derivational}} Networks for {{Inuit-Yupik}} Languages},
  shorttitle = {Unlocking Finite-State Morphological Transducers},
  author = {Haley, Coleman},
  year = 2025,
  month = jun,
  journal = {Society for Computation in Linguistics},
  volume = {8},
  number = {1},
  publisher = {University of Massachusetts Amherst Libraries},
  issn = {2834-1007},
  doi = {10.7275/scil.3172},
  urldate = {2025-11-08},
  abstract = {While derivational morphology is underrepresented in existing computational resources, finite-state morphological transducers (FSMTs) represent a promising untapped source, especially for low-resource, morphologically complex languages. This study presents a method for extracting Universal Derivations-style networks from FSMTs, applying it to Greenlandic and Saint Lawrence Island Yupik: two Inuit-Yupik languages known for their extreme synthesis and agglutination. Using available FSMTs and monolingual corpora, our approach identifies derivationally related forms by analyzing surface-attested words and recursively stripping or modifying morphemes to infer unseen but grammatically implied intermediate forms. The resulting networks include over 53,000 lexemes for SLI Yupik and over 127,000 for Greenlandic, with thousands of non-trivial derivational families and hundreds of unique derivational morphemes. Some individual families contain hundreds of morphemes, highlighting the rich derivational structure of these languages. These results highlight the potential of FSMTs in generating large-scale, empirically-grounded derivational resources for typologically diverse languages.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/NWHADSVU/Haley - 2025 - Unlocking finite-state morphological transducers Derivational networks for Inuit-Yupik languages.pdf}
}

@article{haley-et-al-2023-corpusbased,
  title = {{Corpus-based measures discriminate inflection and derivation cross-linguistically}},
  author = {Haley, Coleman and Ponti, Edoardo M. and Goldwater, Sharon},
  year = 2023,
  month = jun,
  journal = {Society for Computation in Linguistics},
  volume = {6},
  number = {1},
  pages = {403--407},
  publisher = {University of Massachusetts Amherst Libraries},
  issn = {2834-1007},
  doi = {10.7275/z5z0-xx64},
  urldate = {2024-10-15},
  abstract = {Japanese passives are traditionally considered to have two types: direct and indirect passives. However, more recent studies, such as Ishizuka (2012), suggest the two types can be unified un- der the same syntactic movement analysis. Uti- lizing the Balanced Corpus of Contemporary Written Japanese (BCCWJ; Maekawa, 2008; Maekawa et al., 2014), this study aims to in- vestigate how likely different types of passives appear in the naturally occurring texts, espe- cially in relation to markedness-based hierar- chy called Noun Phrase Accessibility Hierar- chy (NPAH; Keenan and Comrie, 1977), and to investigate if true indirect passives occur in contemporary written Japanese.},
  langid = {None},
  file = {/Users/coleman/Zotero/storage/GUFM7UIM/Haley et al_2023_Corpus-based measures discriminate inflection and derivation.pdf}
}

@article{haley-et-al-2024-corpusbased,
  title = {Corpus-Based Measures Discriminate Inflection and Derivation Cross-Linguistically},
  author = {Haley, Coleman and Ponti, Edoardo M. and Goldwater, Sharon},
  year = 2024,
  month = dec,
  journal = {Journal of Language Modelling},
  volume = {12},
  number = {2},
  pages = {477--529},
  issn = {2299-8470},
  doi = {10.15398/jlm.v12i2.351},
  urldate = {2025-10-23},
  abstract = {In morphology, a distinction is commonly drawn between inflection and derivation. However, a precise definition of this distinction which reflects the way it manifests across languages remains elusive within linguistic theory, typically being based on subjective tests. In this study, we present 4 quantitative measures which use the statistics of a raw text corpus in a language to estimate to what extent a given morphological construction changes the form and distribution of lexemes. In particular, we measure both the average and the variance of this change across lexemes. Crucially, distributional information captures syntactic and semantic properties and can be operationalised by word embeddings. Based on a sample of 26 languages, we find that we can reconstruct 89\textpm 1\% of the classification of constructions into inflection and derivation in UniMorph using our 4 measures, providing large-scale cross-linguistic evidence that the concepts of inflection and derivation are associated with measurable signatures in terms of form and distribution that behave consistently across a variety of languages. We also use our measures to identify in a quantitative way whether categories of inflection which have been considered noncanonical in the linguistic literature, such as inherent inflection or transpositions, appear so in terms of properties of their form and distribution. We find that while combining multiple measures reduces the amount of overlap between inflectional and derivational constructions, there are still many constructions near the model's decision boundary between the two categories. This indicates a gradient, rather than categorical, distinction.},
  copyright = {Copyright (c) 2024 Coleman Haley, Edoardo M. Ponti, Sharon Goldwater},
  langid = {english},
  keywords = {derivation,distributional semantics,inflection,morphology,typology},
  file = {/Users/coleman/Zotero/storage/UTMY8YV6/Haley et al. - 2024 - Corpus-based measures discriminate inflection and derivation cross-linguistically.pdf}
}

@article{harris-1954-distributional,
  title = {Distributional Structure},
  author = {Harris, Zellig},
  year = 1954,
  journal = {Word-journal of The International Linguistic Association},
  volume = {10},
  number = {23},
  pages = {146--162}
}

@incollection{hartmann-et-al-2016-identifying,
  title = {Identifying Semantic Role Clusters and Alignment Types via Microrole Coexpression Tendencies},
  booktitle = {Advances in {{Research}} on {{Semantic Roles}}},
  author = {Hartmann, Iren and Haspelmath, Martin and Cysouw, Michael},
  editor = {Kittil{\"a}, Seppo and Z{\'u}{\~n}iga, Fernando},
  year = 2016,
  month = jun,
  pages = {27--49},
  publisher = {John Benjamins Publishing Company},
  address = {Amsterdam, Netherlands},
  url = {https://www.degruyterbrill.com/document/doi/10.1075/bct.88.02har/html?lang=en&srsltid=AfmBOopP-vHPRyneZvs3nBT7wXrjrf7RXuCPJvcKgBZRl8itkcikLHde},
  urldate = {2025-11-09},
  isbn = {978-90-272-6679-8},
  langid = {english}
}

@inproceedings{hartmann-et-al-2019-comparing,
  title = {Comparing {{Unsupervised Word Translation Methods Step}} by {{Step}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32: {{Annual Conference}} on {{Neural Information Processing Systems}} 2019 ({{NeurIPS}} 2019)},
  author = {Hartmann, Mareike and Kementchedjhieva, Yova and S{\o}gaard, Anders},
  editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and {\noopsort{buc}}{d'Alch{\'e}-Buc}, Florence and Fox, Emily B. and Garnett, Roman},
  year = 2019,
  pages = {6031--6041},
  address = {Vancouver, BC, Canada},
  url = {https://proceedings.neurips.cc/paper/2019/hash/d15426b9c324676610fbb01360473ed8-Abstract.html},
  urldate = {2025-11-08},
  file = {/Users/coleman/Zotero/storage/4UE3GNJT/Hartmann et al. - 2019 - Comparing Unsupervised Word Translation Methods Step by Step.pdf}
}

@article{haspelmath-1996-wordclass,
  title = {Word-Class Changing Inflection and Morphological Theory},
  author = {Haspelmath, Martin},
  year = 1996,
  month = jan,
  journal = {Yearbook of morphology 1995, edited by Geert Booij and Jaap van Marle},
  pages = {43--66},
  publisher = {Kluwer},
  address = {Dordrecht},
  doi = {10.5281/zenodo.228046},
  urldate = {2025-10-22},
  abstract = {This paper notes that word-class-changing markers can be inflectional, so that the property of being word-class-changing cannot be a property of derivational morphology in general. The consequences for morphological theory are discussed.},
  keywords = {inflection derivation transposition word class morphological theory},
  file = {/Users/coleman/Zotero/storage/XELLFV92/Haspelmath - 1996 - Word-class changing inflection and morphological theory.pdf}
}

@article{haspelmath-1996-wordclasschanging,
  title = {Word-Class-Changing Inflection and Morphological Theory},
  author = {Haspelmath, Martin},
  editor = {Booij, Geert and {\noopsort{marle}}{van Marle}, Jaap},
  year = 1996,
  journal = {Yearbook of morphology 1995},
  pages = {43--66},
  publisher = {Springer}
}

@incollection{haspelmath-1996-wordclasschanginga,
  title = {Word-Class-Changing Inflection and Morphological Theory},
  booktitle = {Yearbook of {{Morphology}} 1995},
  author = {Haspelmath, Martin},
  editor = {Booij, Geert and {\noopsort{marle}}{van Marle}, Jaap},
  year = 1996,
  pages = {43--66},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-017-3716-6_3},
  urldate = {2025-10-22},
  abstract = {One of the most common claims made about the difference between inflection and derivation in the morphological literature is that derivational affixes change the word-class of their base, while inflectional affixes do not change the word-class. In this paper I argue that this view is wrong, and that important insights about the nature of inflection and derivation are lost if word-class-changing inflection is not recognized. In \S 2, I present a number of examples of word-class-changing inflection, and in \S 3--5 I discuss several potential objections to my analysis. I show that the cases in question can be regarded neither as word-class-changing derivation (\S 3--4) nor as non-word-class-changing inflection (\S 5), and that a description in terms of feature neutralization is not a general solution (\S 6). In \S 7 I argue that to account for the syntactic properties of words, two types of word-class have to be distinguished: lexeme word-class and word-form word-class. \S 8 discusses some problems that arise in the formal representation of this proposal in constituent-structure trees and observes that Tesni\`ere's dependency grammar provides an interesting perspective. Finally, \S 9 discusses the universal correlation between inflection and preservation of internal syntax, and derivation and the non-preservation of internal syntax.},
  isbn = {978-94-017-3716-6},
  langid = {english},
  keywords = {Derivational Morphology,Inflectional Form,Relative Clause,Relative Pronoun,Syntactic Property}
}

@book{haspelmath-1997-indefinite,
  title = {Indefinite {{Pronouns}}},
  author = {Haspelmath, Martin},
  year = 1997,
  publisher = {Oxford University Press},
  address = {Oxford, UK},
  doi = {10.1093/oso/9780198235606.001.0001},
  urldate = {2025-11-09},
  abstract = {Most of the world's languages have indefinite pronouns, that is, expressions such as someone, anything, and nowhere. This book presents an encyclopaedic investigation of indefinite pronouns in the languages of the world, mapping out the range of variation in their functional and formative properties. It shows that cross-linguistic diversity is severely constrained by a set of implicational universals and by a number of unrestricted universals. Topics include formal and functional types of indefinite pronoun, theoretical approaches to the functions of indefinite pronouns, the grammaticalization of indefinite pronouns, and negative indefinite pronouns.},
  isbn = {978-0-19-829963-9 978-0-19-823560-6},
  langid = {english},
  keywords = {cross-linguistic diversity,functions of indefinite pronouns,grammaticalization,implicational universal,indefinite pronoun,language,negative indefinite pronoun,syntax and morphology,thema EDItEUR::C Language and Linguistics::CF Linguistics::CFK Grammar,unrestricted universal},
  annotation = {Accepted: 2018-01-18 23:55},
  file = {/Users/coleman/Zotero/storage/2GSSQZBI/Haspelmath - 1997 - Indefinite Pronouns.pdf}
}

@incollection{haspelmath-2003-geometry,
  title = {The Geometry of Grammatical Meaning: {{Semantic}} Maps and Cross-Linguistic Comparison},
  booktitle = {The New Psychology of Language},
  author = {Haspelmath, Martin},
  editor = {Tomasello, Michael},
  year = 2003,
  volume = {2},
  pages = {211--242},
  publisher = {Lawrence Erlbaum Associates},
  address = {Mahwah, NJ, USA},
  file = {/Users/coleman/Zotero/storage/X42FUXTF/Haspelmath - 2003 - The geometry of grammatical meaning Semantic maps and cross-linguistic comparison.pdf}
}

@article{haspelmath-2007-preestablished,
  title = {Pre-Established Categories Don't Exist: {{Consequences}} for Language Description and Typology},
  author = {Haspelmath, Martin},
  year = 2007,
  journal = {Linguistic Typology},
  volume = {11},
  number = {1},
  pages = {119--132},
  doi = {10.1515/LINGTY.2007.011},
  urldate = {2024-05-14}
}

@article{haspelmath-2008-reply,
  title = {Reply to {{Haiman}} and {{Croft}}},
  author = {Haspelmath, Martin},
  year = 2008,
  month = feb,
  volume = {19},
  number = {1},
  pages = {59--66},
  publisher = {De Gruyter Mouton},
  issn = {1613-3641},
  doi = {10.1515/COG.2008.004},
  urldate = {2025-10-28},
  abstract = {I am grateful to John Haiman and William Croft for their penetrating critiques of my claims and for the interesting challenges that they provide for them. This offers me a chance to clarify and elaborate on some of the central points of my article. This is an important debate, because iconicity and frequency are central explanatoty concepts in functional and cognitive linguistics. Even if we do not succeed in resolving the issues, our understanding will be enhanced by this discussion.},
  chapter = {Cognitive Linguistics},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/SN955B3Z/Haspelmath - 2008 - Reply to Haiman and Croft.pdf}
}

@article{haspelmath-2010-comparative,
  title = {Comparative Concepts and Descriptive Categories in Crosslinguistic Studies},
  author = {Haspelmath, Martin},
  year = 2010,
  journal = {Language},
  volume = {86},
  number = {3},
  eprint = {40961695},
  eprinttype = {jstor},
  pages = {663--687},
  publisher = {Linguistic Society of America},
  issn = {0097-8507},
  url = {https://www.jstor.org/stable/40961695},
  urldate = {2024-10-07},
  abstract = {In this discussion note, I argue that we need to distinguish carefully between descriptive categories, that is, categories of particular languages, and comparative concepts, which are used for crosslinguistic comparison and are specifically created by typologists for the purposes of comparison. Descriptive formal categories cannot be equated across languages because the criteria for category assignment are different from language to language. This old structuralist insight (called CATEGORIAL PARTICULARISM) has recently been emphasized again by several linguists, but the idea that linguists need to identify 'crosslinguistic categories' before they can compare languages is still widespread, especially (but not only) in generative linguistics. Instead, what we have to do (and normally do in practice) is to create comparative concepts that allow us to identify comparable phenomena across languages and to formulate crosslinguistic generalizations. Comparative concepts have to be universally applicable, so they can only be based on other universally applicable concepts: conceptual-semantic concepts, general formal concepts, and other comparative concepts. Comparative concepts are not always purely semantically based concepts, but outside of phonology they usually contain a semantic component. The fact that typologists compare languages in terms of a separate set of concepts that is not taxonomically superordinate to descriptive linguistic categories means that typology and language-particular analysis are more independent of each other than is often thought.},
  file = {/Users/coleman/Zotero/storage/BZBNDRV3/Haspelmath_2010_Comparative concepts and descriptive categories in crosslinguistic studies.pdf}
}

@article{haspelmath-2012-how,
  title = {How to Compare Major Word-Classes across the World's Languages},
  author = {Haspelmath, Martin},
  year = 2012,
  month = feb,
  journal = {UCLA Working Papers in Linguistics},
  volume = {17},
  pages = {109--130},
  doi = {10.5281/ZENODO.3678496},
  urldate = {2024-05-15},
  abstract = {In this paper, I argue that major word-classes, such as nouns, verbs and adjectives, cannot be compared across languages by asking questions such as "Does language X have a noun-verb distinction?". Such questions are routinely asked by linguists (functionalists and generativists alike), but these are the wrong questions (cf. Croft 2000), because they make presuppositions which are not valid.},
  copyright = {Creative Commons Attribution 4.0 International, Open Access}
}

@article{haspelmath-2019-indexing,
  title = {Indexing and Flagging, and Head and Dependent Marking},
  author = {Haspelmath, Martin},
  year = 2019,
  journal = {Te Reo},
  volume = {62},
  number = {1},
  pages = {93--115},
  url = {https://nzlingsoc.org/journal_article/indexing-and-flagging-and-head-and-dependent-marking/},
  urldate = {2025-11-09},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/RMIWLPUW/indexing-and-flagging-and-head-and-dependent-marking.html}
}

@misc{haspelmath-2020-why,
  type = {Billet},
  title = {Why Flags Are Bound Forms: {{A}} Discussion with {{Bill Croft}}},
  shorttitle = {Why Flags Are Bound Forms},
  author = {Haspelmath, Martin},
  year = 2020,
  month = jan,
  journal = {Diversity Linguistics Comment},
  issn = {2197-7518},
  doi = {10.58079/nsvc},
  urldate = {2025-11-09},
  abstract = {A flag is a cover term for an adposition or a case-marker, as I explain in my recent 2019 paper on flagging and indexing (in the journal Te Reo, run by the Linguistic Society of New Zealand). All comparative linguists \dots{} Continue reading {$\rightarrow$}},
  langid = {american},
  file = {/Users/coleman/Zotero/storage/NRL7QKP6/1990.html}
}

@incollection{haspelmath-2021-standardization,
  title = {Towards Standardization of Morphosyntactic Terminology for General Linguistics},
  booktitle = {Linguistic {{Categories}}, {{Language Description}} and {{Linguistic Typology}}},
  author = {Haspelmath, Martin},
  editor = {Alfieri, Luca and Arcodia, Giorgio Francesco and Ramat, Paolo},
  year = 2021,
  month = jun,
  pages = {35--58},
  publisher = {John Benjamins Publishing Company},
  address = {Amsterdam, Netherlands},
  url = {https://www.degruyterbrill.com/document/doi/10.1075/tsl.132.02has/html},
  urldate = {2025-10-26},
  isbn = {978-90-272-5994-3},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/CX3X5BFY/Haspelmath - 2021 - Towards standardization of morphosyntactic terminology for general linguistics.pdf}
}

@misc{haspelmath-2022-what,
  type = {Billet},
  title = {What Is ``Phonological Fusion''? {{A}} Plea for Clear Concepts Such as Boundness or Welding},
  shorttitle = {What Is ``Phonological Fusion''?},
  author = {Haspelmath, Martin},
  year = 2022,
  month = oct,
  journal = {Diversity Linguistics Comment},
  issn = {2197-7518},
  doi = {10.58079/nswu},
  urldate = {2025-10-22},
  abstract = {Many linguists have the intuition that affixes are ``attached'' to their bases, and we often informally use metaphorical terms such as ``fused'', ``cohering'' and ``tightly linked''. There is nothing to be said against metaphors in technical terminology, but do these \dots{} Continue reading {$\rightarrow$}},
  langid = {american}
}

@article{haspelmath-2024-inflection,
  title = {Inflection and Derivation as Traditional Comparative Concepts},
  author = {Haspelmath, Martin},
  year = 2024,
  journal = {Linguistics},
  volume = {62},
  number = {1},
  pages = {43--77},
  doi = {doi:10.1515/ling-2022-0086},
  urldate = {2024-04-24},
  file = {/Users/coleman/Zotero/storage/VV74Q6MJ/Haspelmath - 2024 - Inflection and derivation as traditional comparative concepts.pdf}
}

@article{haspelmath-why,
  title = {Why the Functional-Head Theory in Syntax Is Probably Wrong},
  author = {Haspelmath, Martin},
  journal = {draft},
  url = {https://www.academia.edu/51098638/Why_the_functional_head_theory_in_syntax_is_probably_wrong},
  urldate = {2025-10-07},
  abstract = {This paper examines the functional-head theory in syntax, i.e. the claim that grammatical meanings are represented as heads in syntax, in much the same way as contentive heads (verbs and nouns). It starts out from the presumption that the},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/HEHCED7N/Why_the_functional_head_theory_in_syntax_is_probably_wrong.html}
}

@inproceedings{hathout-et-al-2014-glaff,
  title = {{{GL\`AFF}}, a Large Versatile {{French}} Lexicon},
  booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation ({{LREC}}'14)},
  author = {Hathout, Nabil and Sajous, Franck and Calderone, Basilio},
  year = 2014,
  month = may,
  pages = {1007--1012},
  publisher = {European Language Resources Association (ELRA)},
  address = {Reykjav\'ik, Iceland},
  url = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/58_Paper.pdf},
  abstract = {This paper introduces GLAFF, a large-scale versatile French lexicon extracted from Wiktionary, the collaborative online dictionary. GLAFF contains, for each entry, inflectional features and phonemic transcriptions. It distinguishes itself from the other available French lexicons by its size, its potential for constant updating and its copylefted license. We explain how we have built GLAFF and compare it to other known resources in terms of coverage and quality of the phonemic transcriptions. We show that its size and quality are strong assets that could allow GLAFF to become a reference lexicon for French NLP and linguistics. Moreover, other derived lexicons can easily be based on GLAFF to satisfy specific needs of various fields such as psycholinguistics.}
}

@inproceedings{he-et-al-2018-unsupervised,
  title = {Unsupervised Learning of Syntactic Structure with Invertible Neural Projections},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  author = {He, Junxian and Neubig, Graham and {Berg-Kirkpatrick}, Taylor},
  year = 2018,
  pages = {1292--1302},
  publisher = {Association for Computational Linguistics},
  address = {Brussels, Belgium},
  doi = {10.18653/v1/D18-1160},
  abstract = {Unsupervised learning of syntactic structure is typically performed using generative models with discrete latent variables and multinomial parameters. In most cases, these models have not leveraged continuous word representations. In this work, we propose a novel generative model that jointly learns discrete syntactic structure and continuous word representations in an unsupervised fashion by cascading an invertible neural network with a structured generative prior. We show that the invertibility condition allows for efficient exact inference and marginal likelihood computation in our model so long as the prior is well-behaved. In experiments we instantiate our approach with both Markov and tree-structured priors, evaluating on two tasks: part-of-speech (POS) induction, and unsupervised dependency parsing without gold POS annotation. On the Penn Treebank, our Markov-structured model surpasses state-of-the-art results on POS induction. Similarly, we find that our tree-structured model achieves state-of-the-art performance on unsupervised dependency parsing for the difficult training condition where neither gold POS annotation nor punctuation-based constraints are available.}
}

@article{heilman-et-al-1976-nature,
  title = {The {{Nature}} of {{Comprehension Errors}} in {{Broca}}'s, {{Conduction}} and {{Wernicke}}'s {{Aphasics}}},
  author = {Heilman, Kenneth M. and Scholes, Robert J.},
  year = 1976,
  month = sep,
  journal = {Cortex},
  volume = {12},
  number = {3},
  pages = {258--265},
  issn = {0010-9452},
  doi = {10.1016/S0010-9452(76)80007-X},
  urldate = {2025-11-07},
  abstract = {The purpose of the study was to ascertain if Broca's aphasics have a comprehension defect which is dependent on syntactic relationships, to ascertain how this comprehension defect, if present, is different from that seen in Wernicke's and conduction aphasias. Twenty-six aphasic patients (nine Broca's, eight conduction, nine Wernicke's) and eight controls were given a test which helped differentiate comprehension errors caused by syntactic incompetence from those caused by lexical incompetence. Wernicke's aphasics made significantly more lexical errors than each of the other groups. There were no significant differences between the lexical errors made by the other groups (Broca's, conduction, and control. There were no significant differences between Broca's and conduction aphasics, however, both these groups made more syntactic errors than. the controls.},
  file = {/Users/coleman/Zotero/storage/KUV2GAFL/Heilman and Scholes - 1976 - The Nature of Comprehension Errors in Broca's, Conduction and Wernicke's Aphasics.pdf;/Users/coleman/Zotero/storage/ZRPUPZRW/S001094527680007X.html}
}

@book{helmholtz-1875-sensations,
  title = {On the Sensations of Tone as a Physiological Basis for the Theory of Music},
  author = {Helmholtz, Hermann L.F.},
  year = 1875,
  publisher = {{Longmans, Green, and Co.}},
  address = {London, UK}
}

@article{hengeveld-et-al-2010-implicational,
  title = {An Implicational Map of Parts of Speech},
  author = {Hengeveld, Kees and Van Lier, Eva},
  year = 2010,
  journal = {Linguistic Discovery},
  volume = {8},
  number = {1},
  issn = {1537-0852},
  doi = {10.1349/PS1.1537-0852.A.348},
  urldate = {2025-09-05},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/JCBHQ6Y2/Hengeveld and Van Lier - 2010 - An implicational map of parts of speech.pdf}
}

@book{heny-1983-linguistic,
  title = {Linguistic {{Categories}}: {{Volume One}}: {{Categories}}},
  shorttitle = {Linguistic {{Categories}}},
  author = {Heny, F.},
  year = 1983,
  series = {Studies in {{Linguistics}} and {{Philosophy Ser}}},
  number = {v.19},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  abstract = {Based on Papers Presented at the Fourth Groningen Round Table, Held in July 1980 and Organized by the Institute for General Linguistics of Groningen University},
  collaborator = {Richards, B.},
  isbn = {978-94-009-6989-6},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/5LFBG3BJ/Heny - 1983 - Linguistic Categories Volume One Categories.pdf}
}

@article{hermann-1894-phonophotographische,
  title = {{Phonophotographische Untersuchungen}},
  author = {Hermann, L.},
  year = 1894,
  month = oct,
  journal = {Archiv f\"ur die gesamte Physiologie des Menschen und der Tiere},
  volume = {58},
  number = {5},
  pages = {264--279},
  issn = {1432-2013},
  doi = {10.1007/BF01662480},
  urldate = {2025-11-09},
  langid = {ngerman},
  file = {/Users/coleman/Zotero/storage/6SLLCPHV/Hermann - 1894 - Phonophotographische Untersuchungen.pdf}
}

@inproceedings{hessel-et-al-2018-quantifying,
  title = {Quantifying the {{Visual Concreteness}} of {{Words}} and {{Topics}} in {{Multimodal Datasets}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  author = {Hessel, Jack and Mimno, David and Lee, Lillian},
  editor = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
  year = 2018,
  month = jun,
  pages = {2194--2205},
  publisher = {Association for Computational Linguistics},
  address = {New Orleans, Louisiana},
  doi = {10.18653/v1/N18-1199},
  urldate = {2024-07-23},
  abstract = {Multimodal machine learning algorithms aim to learn visual-textual correspondences. Previous work suggests that concepts with concrete visual manifestations may be easier to learn than concepts with abstract ones. We give an algorithm for automatically computing the visual concreteness of words and topics within multimodal datasets. We apply the approach in four settings, ranging from image captions to images/text scraped from historical books. In addition to enabling explorations of concepts in multimodal datasets, our concreteness scores predict the capacity of machine learning algorithms to learn textual/visual relationships. We find that 1) concrete concepts are indeed easier to learn; 2) the large number of algorithms we consider have similar failure cases; 3) the precise positive relationship between concreteness and performance varies between datasets. We conclude with recommendations for using concreteness scores to facilitate future multimodal research.},
  file = {/Users/coleman/Zotero/storage/MLIR9D3V/Hessel et al_2018_Quantifying the Visual Concreteness of Words and Topics in Multimodal Datasets.pdf}
}

@inproceedings{hewitt-et-al-2019-structural,
  title = {A Structural Probe for Finding Syntax in Word Representations},
  booktitle = {Proceedings of the 2019 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long and Short Papers)},
  author = {Hewitt, John and Manning, Christopher D.},
  year = 2019,
  month = jun,
  pages = {4129--4138},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1419},
  abstract = {Recent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network's word representation space. The probe identifies a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models' vector geometry.}
}

@incollection{hockett-1966-problem,
  title = {The Problem of Universals in Language},
  booktitle = {Universals of Language},
  author = {Hockett, Charles F.},
  editor = {Greenberg, Joseph H.},
  year = 1966,
  edition = {2nd},
  pages = {73--113},
  publisher = {M.I.T Press},
  address = {Cambridge, Massachusetts, USA},
  isbn = {978-0-262-57008-4},
  langid = {english}
}

@article{hollmann-2021-nouniness,
  title = {The `Nouniness' of Attributive Adjectives and `Verbiness' of Predicative Adjectives: Evidence from Phonology},
  shorttitle = {The `Nouniness' of Attributive Adjectives and `Verbiness' of Predicative Adjectives},
  author = {Hollmann, Willem B.},
  year = 2021,
  month = jun,
  journal = {English Language \& Linguistics},
  volume = {25},
  number = {2},
  pages = {257--279},
  issn = {1360-6743, 1469-4379},
  doi = {10.1017/S1360674320000015},
  urldate = {2025-08-09},
  abstract = {This article investigates prototypically attributive versus predicative adjectives in English in terms of the phonological properties that have been associated especially with nouns versus verbs in a substantial body of psycholinguistic research (e.g. Kelly 1992) -- often ignored in theoretical linguistic work on word classes. Inspired by Berg's (2000, 2009) `cross-level harmony constraint', the hypothesis I test is that prototypically attributive adjectives not only align more with nouns than with verbs syntactically, semantically and pragmatically, but also phonologically -- and likewise for prototypically predicative adjectives and verbs. I analyse the phonological structure of frequent adjectives from the Corpus of Contemporary American English (COCA), and show that the data do indeed support the hypothesis. Berg's `cross-level harmony constraint' may thus apply not only to the entire word classes noun, verb and adjective, but also to these two adjectival subclasses. I discuss several theoretical issues that emerge. The facts are most readily accommodated in a usage-based model, such as Radical Construction Grammar (Croft 2001), where these adjectives are seen as forming two distinct but overlapping classes. Drawing also on recent research by Boyd \& Goldberg (2011) and Hao (2015), I explore the possible nature and emergence of these classes in some detail.},
  langid = {english},
  keywords = {acquisition,adjectives,cognitive,convergent,phonology,Radical Construction Grammar,word classes},
  file = {/Users/coleman/Zotero/storage/DFK5YTGG/Hollmann - 2021 - The nouniness of attributive adjectives and verbiness of predicative adjectives evidence from p.pdf}
}

@misc{hosseini-et-al-2024-universality,
  title = {Universality of Representation in Biological and Artificial Neural Networks},
  author = {Hosseini, Eghbal and Casto, Colton and Zaslavsky, Noga and Conwell, Colin and Richardson, Mark and Fedorenko, Evelina},
  year = 2024,
  month = dec,
  primaryclass = {New Results},
  pages = {2024.12.26.629294},
  publisher = {bioRxiv},
  doi = {10.1101/2024.12.26.629294},
  urldate = {2025-11-08},
  abstract = {Many artificial neural networks (ANNs) trained with ecologically plausible objectives on naturalistic data align with behavior and neural representations in biological systems. Here, we show that this alignment is a consequence of convergence onto the same representations by high-performing ANNs and by brains. We developed a method to identify stimuli that systematically vary the degree of inter-model representation agreement. Across language and vision, we then showed that stimuli from high-and low-agreement sets predictably modulated model-to-brain alignment. We also examined which stimulus features distinguish high-from low-agreement sentences and images. Our results establish representation universality as a core component in the model-to-brain alignment and provide a new approach for using ANNs to uncover the structure of biological representations and computations.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {\copyright{} 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/I5HQB6W8/Hosseini et al. - 2024 - Universality of representation in biological and artificial neural networks.pdf}
}

@article{hsieh-2019-distinguishing,
  title = {Distinguishing Nouns and Verbs: {{A Tagalog}} Case Study},
  shorttitle = {Distinguishing Nouns and Verbs},
  author = {Hsieh, Henrison},
  year = 2019,
  month = may,
  journal = {Natural Language \& Linguistic Theory},
  volume = {37},
  number = {2},
  pages = {523--569},
  issn = {0167-806X, 1573-0859},
  doi = {10.1007/s11049-018-9422-3},
  urldate = {2024-10-07},
  langid = {english}
}

@article{hu-et-al-2017-comparison,
  title = {A Comparison of Methods for Estimating the Determinant of High-Dimensional Covariance Matrix},
  author = {Hu, Zongliang and Dong, Kai and Dai, Wenlin and Tong, Tiejun},
  year = 2017,
  journal = {The International Journal of Biostatistics},
  volume = {13},
  number = {2},
  doi = {doi:10.1515/ijb-2017-0013},
  urldate = {2023-02-15},
  note = {Article 20170013}
}

@inproceedings{hu-et-al-2024-findings,
  title = {Findings of the {{Second BabyLM Challenge}}: {{Sample-Efficient Pretraining}} on {{Developmentally Plausible Corpora}}},
  shorttitle = {Findings of the {{Second BabyLM Challenge}}},
  booktitle = {The 2nd {{BabyLM Challenge}} at the 28th {{Conference}} on {{Computational Natural Language Learning}}},
  author = {Hu, Michael Y. and Mueller, Aaron and Ross, Candace and Williams, Adina and Linzen, Tal and Zhuang, Chengxu and Cotterell, Ryan and Choshen, Leshem and Warstadt, Alex and Wilcox, Ethan Gotlieb},
  editor = {Hu, Michael Y. and Mueller, Aaron and Ross, Candace and Williams, Adina and Linzen, Tal and Zhuang, Chengxu and Choshen, Leshem and Cotterell, Ryan and Warstadt, Alex and Wilcox, Ethan Gotlieb},
  year = 2024,
  month = nov,
  pages = {1--21},
  publisher = {Association for Computational Linguistics},
  address = {Miami, FL, USA},
  url = {https://aclanthology.org/2024.conll-babylm.1/},
  urldate = {2025-11-08},
  abstract = {The BabyLM Challenge is a community effort to close the data-efficiency gap between human and computational language learners. Participants compete to optimize language model training on a fixed language data budget of 100 million words or less. This year, we released improved text corpora, as well as a vision-and-language corpus to facilitate research into cognitively plausible vision language models. Submissions were compared on evaluation tasks targeting grammatical ability, (visual) question answering, pragmatic abilities, and grounding, among other abilities. Participants could submit to a 10M-word text-only track, a 100M-word text-only track, and/or a 100M-word and image multimodal track. From 31 submissions employing diverse methods, a hybrid causal-masked language model architecture outperformed other approaches. No submissions outperformed the baselines in the multimodal track. In follow-up analyses, we found a strong relationship between training FLOPs and average performance across tasks, and that the best-performing submissions proposed changes to the training data, training objective, and model architecture. This year's BabyLM Challenge shows that there is still significant room for innovation in this setting, in particular for image-text modeling, but community-driven research can yield actionable insights about effective strategies for small-scale language modeling.},
  file = {/Users/coleman/Zotero/storage/2AYARP3G/Hu et al. - 2024 - Findings of the Second BabyLM Challenge Sample-Efficient Pretraining on Developmentally Plausible C.pdf}
}

@misc{huang-et-al-2021-disentangling,
  title = {Disentangling {{Semantics}} and {{Syntax}} in {{Sentence Embeddings}} with {{Pre-trained Language Models}}},
  author = {Huang, James Y. and Huang, Kuan-Hao and Chang, Kai-Wei},
  year = 2021,
  month = apr,
  number = {arXiv:2104.05115},
  eprint = {2104.05115},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.05115},
  urldate = {2025-09-20},
  abstract = {Pre-trained language models have achieved huge success on a wide range of NLP tasks. However, contextual representations from pre-trained models contain entangled semantic and syntactic information, and therefore cannot be directly used to derive useful semantic sentence embeddings for some tasks. Paraphrase pairs offer an effective way of learning the distinction between semantics and syntax, as they naturally share semantics and often vary in syntax. In this work, we present ParaBART, a semantic sentence embedding model that learns to disentangle semantics and syntax in sentence embeddings obtained by pre-trained language models. ParaBART is trained to perform syntax-guided paraphrasing, based on a source sentence that shares semantics with the target paraphrase, and a parse tree that specifies the target syntax. In this way, ParaBART learns disentangled semantic and syntactic representations from their respective inputs with separate encoders. Experiments in English show that ParaBART outperforms state-of-the-art sentence embedding models on unsupervised semantic similarity tasks. Additionally, we show that our approach can effectively remove syntactic information from semantic sentence embeddings, leading to better robustness against syntactic variation on downstream semantic tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/coleman/Zotero/storage/CTUCQEPT/Huang et al. - 2021 - Disentangling Semantics and Syntax in Sentence Embeddings with Pre-trained Language Models.pdf}
}

@inproceedings{huh-et-al-2024-position,
  title = {Position: {{The Platonic}} Representation Hypothesis},
  shorttitle = {Position},
  booktitle = {Proceedings of the 41st {{International Conference}} on {{Machine Learning}}},
  author = {Huh, Minyoung and Cheung, Brian and Wang, Tongzhou and Isola, Phillip},
  year = 2024,
  month = jul,
  pages = {20617--20642},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v235/huh24a.html},
  urldate = {2025-11-04},
  abstract = {We argue that representations in AI models, particularly deep networks, are converging. First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned. Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way. We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato's concept of an ideal reality. We term such a representation the platonic representation and discuss several possible selective pressures toward it. Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/RBDLQYMG/Huh et al. - 2024 - Position The Platonic Representation Hypothesis.pdf}
}

@inproceedings{hwang-et-al-2017-double,
  title = {Double {{Trouble}}: {{The Problem}} of {{Construal}} in {{Semantic Annotation}} of {{Adpositions}}},
  shorttitle = {Double {{Trouble}}},
  booktitle = {Proceedings of the 6th {{Joint Conference}} on {{Lexical}} and {{Computational Semantics}} (*{{SEM}} 2017)},
  author = {Hwang, Jena D. and Bhatia, Archna and Han, Na-Rae and O'Gorman, Tim and Srikumar, Vivek and Schneider, Nathan},
  editor = {Ide, Nancy and Herbelot, Aur{\'e}lie and M{\`a}rquez, Llu{\'i}s},
  year = 2017,
  month = aug,
  pages = {178--188},
  publisher = {Association for Computational Linguistics},
  address = {Vancouver, Canada},
  doi = {10.18653/v1/S17-1022},
  urldate = {2025-08-09},
  abstract = {We consider the semantics of prepositions, revisiting a broad-coverage annotation scheme used for annotating all 4,250 preposition tokens in a 55,000 word corpus of English. Attempts to apply the scheme to adpositions and case markers in other languages, as well as some problematic cases in English, have led us to reconsider the assumption that an adposition's lexical contribution is equivalent to the role/relation that it mediates. Our proposal is to embrace the potential for construal in adposition use, expressing such phenomena directly at the token level to manage complexity and avoid sense proliferation. We suggest a framework to represent both the scene role and the adposition's lexical function so they can be annotated at scale---supporting automatic, statistical processing of domain-general language---and discuss how this representation would allow for a simpler inventory of labels.},
  file = {/Users/coleman/Zotero/storage/VWE3NT4M/Hwang et al. - 2017 - Double Trouble The Problem of Construal in Semantic Annotation of Adpositions.pdf}
}

@inproceedings{imel-et-al-2022-modal,
  title = {Modal Semantic Universals Optimize the Simplicity/Informativeness Trade-Off},
  booktitle = {Proceedings of the 34th {{Semantics}} and {{Linguistic Theory Conference}}},
  author = {Imel, Nathaniel and {Steinert-Threlkeld}, Shane},
  editor = {Zhang, Yao and Zhao, Fengyue and Cho, Youngdong and Wu, Yifan},
  year = 2022,
  address = {Rochester, New York, USA}
}

@book{jakobson-1941-child,
  title = {Child {{Language}}, {{Aphasia}} and {{Phonological Universals}}},
  author = {Jakobson, Roman},
  origdate = {1941},
  year = 1941,
  publisher = {De Gruyter Mouton},
  address = {Berlin, Germany},
  doi = {10.1515/9783111353562},
  urldate = {2025-11-09},
  abstract = {Child Language, Aphasia and Phonological Universals by Roman Jakobson was published on July 24, 2014 by De Gruyter Mouton.},
  isbn = {978-3-11-135356-2},
  langid = {english},
  keywords = {Aphasia,Children -- Language},
  file = {/Users/coleman/Zotero/storage/R3E25ZHK/Jakobson - 2014 - Child Language, Aphasia and Phonological Universals.pdf}
}

@inproceedings{jha-et-al-2025-harnessing,
  title = {Harnessing the Universal Geometry of Embeddings},
  booktitle = {The {{Thirty-ninth Annual Conference}} on {{Neural Information Processing Systems}}},
  author = {Jha, Rishi Dev and Zhang, Collin and Shmatikov, Vitaly and Morris, John Xavier},
  year = 2025,
  month = oct,
  url = {https://openreview.net/forum?id=jiCLUPq5xv&referrer=%5Bthe%20profile%20of%20Rishi%20Dev%20Jha%5D(%2Fprofile%3Fid%3D~Rishi_Dev_Jha1)},
  urldate = {2025-11-04},
  abstract = {We introduce the first method for translating text embeddings from one vector space to another without any paired data, encoders, or predefined sets of matches. Our unsupervised approach translates any embedding to and from a universal latent representation (i.e., a universal semantic structure conjectured by the Platonic Representation Hypothesis). Our translations achieve high cosine similarity across model pairs with different architectures, parameter counts, and training datasets. The ability to translate unknown embeddings into a different space while preserving their geometry has serious implications for the security of vector databases. An adversary with access only to embedding vectors can extract sensitive information about the underlying documents, sufficient for classification and attribute inference.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/SBVHTQHI/Jha et al. - 2025 - Harnessing the Universal Geometry of Embeddings.pdf}
}

@misc{jiang-et-al-2025-vision,
  title = {Vision {{Transformers Don}}'t {{Need Trained Registers}}},
  author = {Jiang, Nick and Dravid, Amil and Efros, Alexei and Gandelsman, Yossi},
  year = 2025,
  month = oct,
  number = {arXiv:2506.08010},
  eprint = {2506.08010},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.08010},
  urldate = {2025-11-08},
  abstract = {We investigate the mechanism underlying a previously identified phenomenon in Vision Transformers - the emergence of high-norm tokens that lead to noisy attention maps (Darcet et al., 2024). We observe that in multiple models (e.g., CLIP, DINOv2), a sparse set of neurons is responsible for concentrating high-norm activations on outlier tokens, leading to irregular attention patterns and degrading downstream visual processing. While the existing solution for removing these outliers involves retraining models from scratch with additional learned register tokens, we use our findings to create a training-free approach to mitigate these artifacts. By shifting the high-norm activations from our discovered register neurons into an additional untrained token, we can mimic the effect of register tokens on a model already trained without registers. We demonstrate that our method produces cleaner attention and feature maps, enhances performance over base models across multiple downstream visual tasks, and achieves results comparable to models explicitly trained with register tokens. We then extend test-time registers to off-the-shelf vision-language models, yielding cleaner attention-based, text-to-image attribution. Finally, we outline a simple mathematical model that reflects the observed behavior of register neurons and high norm tokens. Our results suggest that test-time registers effectively take on the role of register tokens at test-time, offering a training-free solution for any pre-trained model released without them.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/coleman/Zotero/storage/EJRRAUUY/Jiang et al. - 2025 - Vision Transformers Don't Need Trained Registers.pdf;/Users/coleman/Zotero/storage/X8TTTXRF/2506.html}
}

@inproceedings{johnson-et-al-2015-image,
  title = {Image Retrieval Using Scene Graphs},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Johnson, Justin and Krishna, Ranjay and Stark, Michael and Li, Li-Jia and Shamma, David A. and Bernstein, Michael S. and {Fei-Fei}, Li},
  year = 2015,
  month = jun,
  pages = {3668--3678},
  publisher = {IEEE},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2015.7298990},
  urldate = {2025-11-09},
  abstract = {This paper develops a novel framework for semantic image retrieval based on the notion of a scene graph. Our scene graphs represent objects (``man'', ``boat''), attributes of objects (``boat is white'') and relationships between objects (``man standing on boat''). We use these scene graphs as queries to retrieve semantically related images. To this end, we design a conditional random field model that reasons about possible groundings of scene graphs to test images. The likelihoods of these groundings are used as ranking scores for retrieval. We introduce a novel dataset of 5,000 human-generated scene graphs grounded to images and use this dataset to evaluate our method for image retrieval. In particular, we evaluate retrieval using full scene graphs and small scene subgraphs, and show that our method outperforms retrieval methods that use only objects or low-level image features. In addition, we show that our full model can be used to improve object localization compared to baseline methods.},
  keywords = {Boats,Computational modeling,Context,Grounding,Image retrieval,Semantics,Visualization},
  file = {/Users/coleman/Zotero/storage/RFQFKEK3/7298990.html}
}

@article{joos-1950-description,
  title = {Description of Language Design},
  author = {Joos, Martin},
  year = 1950,
  month = nov,
  journal = {The Journal of the Acoustical Society of America},
  volume = {22},
  number = {6},
  pages = {701--707},
  issn = {0001-4966},
  doi = {10.1121/1.1906674},
  urldate = {2025-11-04},
  abstract = {Physicists describe speech with continuous mathematics, such as Fourier analysis or the autocorrelation function. Linguists describe language instead, using a discontinuous or discrete mathematics called ``linguistics.'' The nature of this odd calculus is outlined and justified here. It treats speech communication as having a telegraphic structure. (Non-linguists normally fail to orient themselves in this field because they treat speech as analogous to telephony.) The telegraph-code structure of language is examined from top to bottom, and at each of its several levels of complexity (compared to the two levels of Morse code) its structure is shown to be defined by possibilities and impossibilities of combination among the units of that level. Above the highest level we find, instead of such absolute restrictions, conditional probabilities of occurrence: this is the semantic field, outside linguistics, where sociologists can work. Below the lowest level we find, instead of such absolute restrictions, conditional probabilities of phonetic quality: this is the phonetic field, outside linguistics, where physicists can work. Thus linguistics is peculiar among mathematical systems in that it abuts upon reality in two places instead of one. This statement is equivalent to defining a language as a symbolic system; that is, as a code.},
  file = {/Users/coleman/Zotero/storage/3SM4E5BK/1.html}
}

@article{jr.-1951-kolmogorovsmirnov,
  title = {The {{Kolmogorov-Smirnov}} Test for Goodness of Fit},
  author = {Jr., Frank J. Massey},
  year = 1951,
  journal = {Journal of the American Statistical Association},
  volume = {46},
  number = {253},
  eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1951.10500769},
  pages = {68--78},
  publisher = {Taylor \& Francis},
  doi = {10.1080/01621459.1951.10500769}
}

@inproceedings{kallini-et-al-2024-mission,
  title = {Mission: {{Impossible Language Models}}},
  shorttitle = {Mission},
  booktitle = {Proceedings of the 62nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Kallini, Julie and Papadimitriou, Isabel and Futrell, Richard and Mahowald, Kyle and Potts, Christopher},
  editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
  year = 2024,
  month = aug,
  pages = {14691--14714},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.787},
  urldate = {2025-11-09},
  abstract = {Chomsky and others have very directly claimed that large language models (LLMs) are equally capable of learning languages that are possible and impossible for humans to learn. However, there is very little published experimental evidence to support such a claim. Here, we develop a set of synthetic impossible languages of differing complexity, each designed by systematically altering English data with unnatural word orders and grammar rules. These languages lie on an impossibility continuum: at one end are languages that are inherently impossible, such as random and irreversible shuffles of English words, and on the other, languages that may not be intuitively impossible but are often considered so in linguistics, particularly those with rules based on counting word positions. We report on a wide range of evaluations to assess the capacity of GPT-2 small models to learn these uncontroversially impossible languages, and crucially, we perform these assessments at various stages throughout training to compare the learning process for each language. Our core finding is that GPT-2 struggles to learn impossible languages when compared to English as a control, challenging the core claim. More importantly, we hope our approach opens up a productive line of inquiry in which different LLM architectures are tested on a variety of impossible languages in an effort to learn more about how LLMs can be used as tools for these cognitive and typological investigations.},
  file = {/Users/coleman/Zotero/storage/3GM9E5TW/Kallini et al. - 2024 - Mission Impossible Language Models.pdf}
}

@article{karpathy-et-al-2017-deep,
  title = {Deep Visual-Semantic Alignments for Generating Image Descriptions},
  author = {Karpathy, Andrej and Li, Fei-Fei},
  year = 2017,
  month = apr,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {39},
  number = {4},
  pages = {664--676},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2016.2598339},
  urldate = {2025-11-01},
  abstract = {We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks (RNN) over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions outperform retrieval baselines on both full images and on a new dataset of region-level annotations. Finally, we conduct large-scale analysis of our RNN language model on the Visual Genome dataset of 4.1 million captions and highlight the differences between image and region-level caption statistics.},
  keywords = {Analytical models,Context,deep neural networks,Image captioning,Image segmentation,language model,Natural languages,recurrent neural network,Recurrent neural networks,visual-semantic embeddings,Visualization},
  file = {/Users/coleman/Zotero/storage/KBLICNRB/Karpathy and Fei-Fei - 2017 - Deep Visual-Semantic Alignments for Generating Image Descriptions.pdf}
}

@inproceedings{kasthuri-et-al-2017-plis,
  title = {{{PLIS}}: {{Proposed}} Language Independent Stemmer for Information Retrieval Systems Using Dynamic Programming},
  booktitle = {2017 World Congress on Computing and Communication Technologies ({{WCCCT}})},
  author = {Kasthuri, M. and Kumar, S. Britto Ramesh and Khaddaj, Souheil},
  year = 2017,
  pages = {132--135},
  doi = {10.1109/WCCCT.2016.39}
}

@article{kauf-et-al-2024-lexicalsemantic,
  title = {Lexical-Semantic Content, Not Syntactic Structure, Is the Main Contributor to {{ANN-brain}} Similarity of {{fMRI}} Responses in the Language Network},
  author = {Kauf, Carina and Tuckute, Greta and Levy, Roger and Andreas, Jacob and Fedorenko, Evelina},
  year = 2024,
  month = apr,
  journal = {Neurobiology of Language},
  volume = {5},
  number = {1},
  pages = {7--42},
  issn = {2641-4368},
  doi = {10.1162/nol_a_00116},
  urldate = {2025-11-04},
  abstract = {Representations from artificial neural network (ANN) language models have been shown to predict human brain activity in the language network. To understand what aspects of linguistic stimuli contribute to ANN-to-brain similarity, we used an fMRI data set of responses to n = 627 naturalistic English sentences (Pereira et al., 2018) and systematically manipulated the stimuli for which ANN representations were extracted. In particular, we (i) perturbed sentences' word order, (ii) removed different subsets of words, or (iii) replaced sentences with other sentences of varying semantic similarity. We found that the lexical-semantic content of the sentence (largely carried by content words) rather than the sentence's syntactic form (conveyed via word order or function words) is primarily responsible for the ANN-to-brain similarity. In follow-up analyses, we found that perturbation manipulations that adversely affect brain predictivity also lead to more divergent representations in the ANN's embedding space and decrease the ANN's ability to predict upcoming tokens in those stimuli. Further, results are robust as to whether the mapping model is trained on intact or perturbed stimuli and whether the ANN sentence representations are conditioned on the same linguistic context that humans saw. The critical result---that lexical-semantic content is the main contributor to the similarity between ANN representations and neural ones---aligns with the idea that the goal of the human language system is to extract meaning from linguistic strings. Finally, this work highlights the strength of systematic experimental manipulations for evaluating how close we are to accurate and generalizable models of the human language network.},
  file = {/Users/coleman/Zotero/storage/6ACP5KE5/Kauf et al. - 2024 - Lexical-Semantic Content, Not Syntactic Structure, Is the Main Contributor to ANN-Brain Similarity o.pdf;/Users/coleman/Zotero/storage/5LY6Y6FQ/nol_a_00116.html}
}

@article{kaufman-2009-austronesian,
  title = {Austronesian {{Nominalism}} and Its Consequences: {{A Tagalog}} Case Study},
  author = {Kaufman, Daniel},
  year = 2009,
  journal = {Theoretical Linguistics},
  volume = {35},
  number = {1},
  pages = {1--49},
  doi = {10.1515/THLI.2009.001},
  urldate = {2024-05-15}
}

@article{kemp-et-al-2012-kinship,
  title = {Kinship Categories across Languages Reflect General Communicative Principles},
  author = {Kemp, Charles and Regier, Terry},
  year = 2012,
  month = may,
  journal = {Science},
  volume = {336},
  number = {6084},
  pages = {1049--1054},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.1218811},
  urldate = {2025-11-06},
  abstract = {Languages vary in their systems of kinship categories, but the scope of possible variation appears to be constrained. Previous accounts of kin classification have often emphasized constraints that are specific to the domain of kinship and are not derived from general principles. Here, we propose an account that is founded on two domain-general principles: Good systems of categories are simple, and they enable informative communication. We show computationally that kin classification systems in the world's languages achieve a near-optimal trade-off between these two competing principles. We also show that our account explains several specific constraints on kin classification proposed previously. Because the principles of simplicity and informativeness are also relevant to other semantic domains, the trade-off between them may provide a domain-general foundation for variation in category systems across languages.}
}

@article{kermes-et-al-average,
  title = {Average Surprisal of Parts-of-Speech},
  author = {Kermes, Hannah and Teich, Elke},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/RGPQTBGA/Kermes and Teich - Average surprisal of parts-of-speech.pdf}
}

@article{kermes-et-al-averagea,
  title = {Average Surprisal of Parts-of-Speech},
  author = {Kermes, Hannah and Teich, Elke},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/LKAEED8H/Kermes and Teich - Average surprisal of parts-of-speech.pdf}
}

@inproceedings{khetarpal-et-al-2009-spatial,
  title = {Spatial Terms Reflect Near-Optimal Spatial Categories},
  booktitle = {Proceedings of the {{Annual Meeting}} of the {{Cognitive Science Society}}},
  author = {Khetarpal, Naveen and Majid, Asifa and Regier, Terry},
  year = 2009,
  volume = {31},
  publisher = {Cognitive Science Society},
  address = {Amsterdam, Netherlands},
  url = {https://escholarship.org/uc/item/41n1k53g},
  urldate = {2025-10-07},
  abstract = {Author(s): Khetarpal, Naveen; Majid, Asifa; Regier, Terry},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/B9AFZ8DB/Khetarpal et al. - 2009 - Spatial terms reflect near-optimal spatial categories.pdf}
}

@article{khetarpal-et-al-spatial,
  title = {Spatial Terms across Languages Support Near-Optimal Communication: {{Evidence}} from {{Peruvian Amazonia}}, and Computational Analyses},
  author = {Khetarpal, Naveen and Neveu, Grace and Majid, Asifa and Michael, Lev and Regier, Terry},
  abstract = {Why do languages have the categories they do? It has been argued that spatial terms in the world's languages reflect categories that support highly informative communication, and that this accounts for the spatial categories found across languages. However, this proposal has been tested against only nine languages, and in a limited fashion. Here, we consider two new languages: Maijki, an under-documented language of Peruvian Amazonia, and English. We analyze spatial data from these two new languages and the original nine, using thorough and theoretically targeted computational tests. The results support the hypothesis that spatial terms across dissimilar languages enable near-optimally informative communication, over an influential competing hypothesis.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/H9IR34SD/Khetarpal et al. - Spatial terms across languages support near-optimal communication Evidence from Peruvian Amazonia,.pdf}
}

@article{kim-2002-does,
  title = {Does {{Korean}} Have Adjectives},
  author = {Kim, Min-Joo},
  year = 2002,
  journal = {MIT Working Papers in Linguistics},
  volume = {43},
  pages = {71--89},
  url = {http://www.webpages.ttu.edu/minjkim/KimHUMIT02.pdf}
}

@article{kim-et-al-2019-predicting,
  title = {Predicting the {{Argumenthood}} of {{English Prepositional Phrases}}},
  author = {Kim, Najoung and Rawlins, Kyle and Durme, Benjamin Van and Smolensky, Paul},
  year = 2019,
  month = jul,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  number = {01},
  pages = {6578--6585},
  issn = {2374-3468},
  doi = {10.1609/aaai.v33i01.33016578},
  urldate = {2025-09-09},
  abstract = {Distinguishing between arguments and adjuncts of a verb is a longstanding, nontrivial problem. In natural language processing, argumenthood information is important in tasks such as semantic role labeling (SRL) and prepositional phrase (PP) attachment disambiguation. In theoretical linguistics, many diagnostic tests for argumenthood exist but they often yield conflicting and potentially gradient results. This is especially the case for syntactically oblique items such as PPs. We propose two PP argumenthood prediction tasks branching from these two motivations: (1) binary argumentadjunct classification of PPs in VerbNet, and (2) gradient argumenthood prediction using human judgments as gold standard, and report results from prediction models that use pretrained word embeddings and other linguistically informed features. Our best results on each task are (1) acc. = 0.955, F1 = 0.954 (ELMo+BiLSTM) and (2) Pearson's r = 0.624 (word2vec+MLP). Furthermore, we demonstrate the utility of argumenthood prediction in improving sentence representations via performance gains on SRL when a sentence encoder is pretrained with our tasks.},
  copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/HFEFFN3W/Kim et al. - 2019 - Predicting the Argumenthood of English Prepositional Phrases.pdf}
}

@inproceedings{kim-et-al-2021-testing,
  title = {Testing for {{Grammatical Category Abstraction}} in {{Neural Language Models}}},
  booktitle = {Proceedings of the {{Society}} for {{Computation}} in {{Linguistics}} 2021},
  author = {Kim, Najoung and Smolensky, Paul},
  editor = {Ettinger, Allyson and Pavlick, Ellie and Prickett, Brandon},
  year = 2021,
  month = feb,
  pages = {467--470},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  url = {https://aclanthology.org/2021.scil-1.59/},
  urldate = {2025-10-07},
  file = {/Users/coleman/Zotero/storage/6KQQF2ZI/Kim and Smolensky - 2021 - Testing for Grammatical Category Abstraction in Neural Language Models.pdf}
}

@inproceedings{kingma-et-al-2015-adam,
  title = {Adam: {{A}} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {{ICLR}} 2015, San Diego, {{CA}}, {{USA}}, May 7-9, 2015, Conference Track Proceedings},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  editor = {Bengio, Yoshua and LeCun, Yann},
  year = 2015,
  url = {http://arxiv.org/abs/1412.6980},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200}
}

@book{kirby-1999-function,
  title = {Function, Selection, and Innateness: {{The}} Emergence of Language Universals},
  shorttitle = {Function, {{Selection}}, and {{Innateness}}},
  author = {Kirby, Simon},
  year = 1999,
  month = apr,
  publisher = {Oxford University Press},
  address = {Oxford, UK},
  doi = {10.1093/oso/9780198238119.001.0001},
  urldate = {2025-11-05},
  abstract = {This book explores issues at the core of modern linguistics and cognitive science.  Why are all languages similar in some ways and in others utterly different? Why do languages change and change variably? How did the human capacity for language evolve, and how far did it do so as an innate ability? Simon Kirby looks at these questions from a broad perspective, arguing that they can (indeed must) be studied together.  The author begins by examining how far the universal properties of language may be explained by examining the way it is used, and how far by the way it is structured. He then considers what insights may be gained by combining functional and formal approaches. In doing so he develops a way of treating language as an adaptive system, in which its communicative and formal roles are both crucial and complementary.  In order to test the effectiveness of competing theories and explanations, Simon Kirby develops computational models to show what universals emerge given a particular theory of language use or acquisition. He presents here both the methodology and the results.  Function, Selection, and Innateness is important for its argument, its methodology, and its conclusions. It is a powerful demonstration of the value of looking at language as an adaptive system and goes to the heart of current debates on the evolution and nature of language.},
  isbn = {978-0-19-823811-9}
}

@article{kirkici-et-al-2013-inflection,
  title = {Inflection and Derivation in Native and Non-Native Language Processing: {{Masked}} Priming Experiments on {{Turkish}}},
  author = {K{\i}rk{\i}c{\i}, Bilal and Clahsen, Harald},
  year = 2013,
  journal = {Bilingualism: Language and Cognition},
  volume = {16},
  number = {4},
  pages = {776--791},
  publisher = {Cambridge University Press},
  doi = {10.1017/S1366728912000648}
}

@inproceedings{klubicka-et-al-2023-idioms,
  title = {Idioms, {{Probing}} and {{Dangerous Things}}: {{Towards Structural Probing}} for {{Idiomaticity}} in {{Vector Space}}},
  shorttitle = {Idioms, {{Probing}} and {{Dangerous Things}}},
  booktitle = {Proceedings of the 19th {{Workshop}} on {{Multiword Expressions}} ({{MWE}} 2023)},
  author = {Klubi{\v c}ka, Filip and Nedumpozhimana, Vasudevan and Kelleher, John},
  editor = {Bhatia, Archna and Evang, Kilian and Garcia, Marcos and Giouli, Voula and Han, Lifeng and Taslimipoor, Shiva},
  year = 2023,
  month = may,
  pages = {45--57},
  publisher = {Association for Computational Linguistics},
  address = {Dubrovnik, Croatia},
  doi = {10.18653/v1/2023.mwe-1.8},
  urldate = {2025-11-09},
  abstract = {The goal of this paper is to learn more about how idiomatic information is structurally encoded in embeddings, using a structural probing method. We repurpose an existing English verbal multi-word expression (MWE) dataset to suit the probing framework and perform a comparative probing study of static (GloVe) and contextual (BERT) embeddings. Our experiments indicate that both encode some idiomatic information to varying degrees, but yield conflicting evidence as to whether idiomaticity is encoded in the vector norm, leaving this an open question. We also identify some limitations of the used dataset and highlight important directions for future work in improving its suitability for a probing analysis.},
  file = {/Users/coleman/Zotero/storage/NZYYGUJG/Klubika et al. - 2023 - Idioms, Probing and Dangerous Things Towards Structural Probing for Idiomaticity in Vector Space.pdf}
}

@article{konig-2006-marked,
  title = {Marked Nominative in {{Africa}}},
  author = {K{\"o}nig, Christa},
  year = 2006,
  journal = {Studies in Language},
  volume = {30},
  number = {4},
  pages = {655--732},
  publisher = {John Benjamins}
}

@inproceedings{koper-et-al-2016-automatically,
  title = {Automatically {{Generated Affective Norms}} of {{Abstractness}}, {{Arousal}}, {{Imageability}} and {{Valence}} for 350 000 {{German Lemmas}}},
  booktitle = {Proceedings of the {{Tenth International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}}'16)},
  author = {K{\"o}per, Maximilian and {Schulte im Walde}, Sabine},
  editor = {Calzolari, Nicoletta and Choukri, Khalid and Declerck, Thierry and Goggi, Sara and Grobelnik, Marko and Maegaard, Bente and Mariani, Joseph and Mazo, Helene and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
  year = 2016,
  month = may,
  pages = {2595--2598},
  publisher = {European Language Resources Association (ELRA)},
  address = {Portoro\v z, Slovenia},
  url = {https://aclanthology.org/L16-1413},
  urldate = {2024-10-07},
  abstract = {This paper presents a collection of 350,000 German lemmatised words, rated on four psycholinguistic affective attributes. All ratings were obtained via a supervised learning algorithm that can automatically calculate a numerical rating of a word. We applied this algorithm to abstractness, arousal, imageability and valence. Comparison with human ratings reveals high correlation across all rating types. The full resource is publically available at: http://www.ims.uni-stuttgart.de/data/affective\_norms/},
  file = {/Users/coleman/Zotero/storage/BGTJC34D/Kper_Schulte im Walde_2016_Automatically Generated Affective Norms of Abstractness, Arousal, Imageability.pdf}
}

@misc{kozlowski-et-al-2025-semantic,
  title = {Semantic {{Structure}} in {{Large Language Model Embeddings}}},
  author = {Kozlowski, Austin C. and Dai, Callin and Boutyline, Andrei},
  year = 2025,
  month = aug,
  number = {arXiv:2508.10003},
  eprint = {2508.10003},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2508.10003},
  urldate = {2025-11-04},
  abstract = {Psychological research consistently finds that human ratings of words across diverse semantic scales can be reduced to a low-dimensional form with relatively little information loss. We find that the semantic associations encoded in the embedding matrices of large language models (LLMs) exhibit a similar structure. We show that the projections of words on semantic directions defined by antonym pairs (e.g. kind - cruel) correlate highly with human ratings, and further find that these projections effectively reduce to a 3-dimensional subspace within LLM embeddings, closely resembling the patterns derived from human survey responses. Moreover, we find that shifting tokens along one semantic direction causes off-target effects on geometrically aligned features proportional to their cosine similarity. These findings suggest that semantic features are entangled within LLMs similarly to how they are interconnected in human language, and a great deal of semantic information, despite its apparent complexity, is surprisingly low-dimensional. Furthermore, accounting for this semantic structure may prove essential for avoiding unintended consequences when steering features.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/coleman/Zotero/storage/QPVAMQCT/Kozlowski et al. - 2025 - Semantic Structure in Large Language Model Embeddings.pdf;/Users/coleman/Zotero/storage/GU78XFDP/2508.html}
}

@article{kratzer-stagelevel,
  title = {Stage-{{Level}} and {{Individual-Level Predicates}}},
  author = {Kratzer, Angelika},
  file = {/Users/coleman/Zotero/storage/EKB3T7DT/Kratzer - Stage-Level and Individual-Level Predicates.pdf}
}

@article{kyjanek-et-al-2020-universal,
  title = {Universal {{Derivations}} 1.0, a Growing Collection of Harmonised Word-Formation Resources},
  author = {Kyj{\'a}nek, Luk{\'a}{\v s} and {\v Z}abokrtsk{\'y}, Zden{\v e}k and {\v S}ev{\v c}{\'i}kov{\'a}, Magda and Vidra, Jon{\'a}{\v s}},
  year = 2020,
  journal = {The Prague Bulletin of Mathematical Linguistics},
  volume = {2},
  number = {115},
  pages = {333--348},
  publisher = {Karolinum Press},
  address = {Prague, Czech Republic}
}

@inproceedings{kyjanek-et-al-2025-theoretical,
  title = {Theoretical Stances Shape Empirical Generalizations on Inflection vs. Derivation: Quantitative Evidence from {{Czech}}},
  booktitle = {Word-{{Formation Theories VII}} \& {{Typology}} and {{Universals}} in {{Word-Formation V}}},
  author = {Kyj{\'a}nek, Luk{\'a}{\v s} and Bonami, Olivier},
  year = 2025,
  pages = {25--26},
  publisher = {Pavol Jozef \v Saf\'arik University},
  address = {Ko\v sice, Slovakia},
  annotation = {book of abstracts}
}

@book{lakoff-1987-women,
  title = {Women, {{Fire}}, and {{Dangerous Things}}:  {{What Categories Reveal}} about the {{Mind}}},
  shorttitle = {Women, Fire, and Dangerous Things},
  author = {Lakoff, George},
  year = 1987,
  series = {Women, Fire, and Dangerous Things:  {{What}} Categories Reveal about the Mind},
  publisher = {University of Chicago Press},
  address = {Chicago, Illinois, US},
  abstract = {This book attempts to bring together some of the evidence for the view that reason is embodied and imaginative---in particular, the evidence that comes from the study of the way people categorize. Conceptual systems are organized in terms of categories, and most if not all of our thought involves those categories. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  isbn = {978-0-226-46803-7},
  langid = {english},
  keywords = {Cognition,Concept Formation,Human Information Storage,Philosophies,Psycholinguistics,Reasoning,Taxonomies},
  file = {/Users/coleman/Zotero/storage/I7VIBMFS/1987-97400-000.html}
}

@article{laks-et-al-2022-hebrewnette,
  title = {Hebrewnette---a New Derivational Resource for Non-Concatenative Morphology: {{Principles}}, Design and Implementation},
  author = {Laks, Lior and Namer, Fiammetta},
  year = 2022,
  journal = {The Prague Bulletin of Mathematical Linguistics},
  volume = {118},
  pages = {25--53}
}

@article{landau2016,
  title = {Update on ``What'' and ``Where'' in Spatial Language: A New Division of Labor for Spatial Terms},
  author = {Landau, Barbara},
  year = 2017,
  journal = {Cognitive Science},
  volume = {41},
  number = {S2},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12410},
  pages = {321--350},
  doi = {10.1111/cogs.12410},
  abstract = {Abstract In this article, I revisit Landau and Jackendoff's () paper, ``What and where in spatial language and spatial cognition,'' proposing a friendly amendment and reformulation. The original paper emphasized the distinct geometries that are engaged when objects are represented as members of object kinds (named by count nouns), versus when they are represented as figure and ground in spatial expressions (i.e., play the role of arguments of spatial prepositions). We provided empirical and theoretical arguments for the link between these distinct representations in spatial language and their accompanying nonlinguistic neural representations, emphasizing the ``what'' and ``where'' systems of the visual system. In the present paper, I propose a second division of labor between two classes of spatial prepositions in English that appear to be quite distinct. One class includes prepositions such as in and on, whose core meanings engage force-dynamic, functional relationships between objects, with geometry only a marginal player. The second class includes prepositions such as above/below and right/left, whose core meanings engage geometry, with force-dynamic relationships a passing or irrelevant variable. The insight that objects' force-dynamic relationships matter to spatial terms' uses is not new; but thinking of these terms as a distinct set within spatial language has theoretical and empirical consequences that are new. I propose three such consequences, rooted in the fact that geometric knowledge is highly constrained and early-emerging in life, while force-dynamic knowledge of objects and their interactions is relatively unconstrained and needs to be learned piecemeal over a lengthy timeline. First, the two classes will engage different learning problems, with different developmental trajectories for both first and second language learners; second, the classes will naturally lead to different degrees of cross-linguistic variation; and third, they may be rooted in different neural representations.},
  keywords = {Cross-linguistic studies,Development,Force-dynamics,Geometry,Linguistics,Spatial cognition,Spatial language,Spatial prepositions}
}

@book{langacker-1987-foundations,
  title = {Foundations of Cognitive Grammar},
  author = {Langacker, Ronald W.},
  year = 1987,
  publisher = {Stanford University Press},
  address = {Stanford, California},
  isbn = {0-8047-1261-1},
  langid = {english},
  keywords = {Cognitive grammar}
}

@incollection{larasati-et-al-2011-indonesian,
  title = {Indonesian Morphology Tool ({{MorphInd}}): {{Towards}} an {{Indonesian}} Corpus},
  booktitle = {Systems and Frameworks for Computational Morphology},
  author = {Larasati, Septina Dian and Kubo{\v n}, Vladislav and Zeman, Daniel},
  editor = {Mahlow, Cerstin and Piotrowski, Michael},
  year = 2011,
  pages = {119--129},
  publisher = {Springer},
  address = {Berlin/Heidelberg, Germany},
  doi = {10.1007/978-3-642-23138-4_8}
}

@article{lass-1984-vowel,
  title = {Vowel System Universals and Typology: Prologue to Theory},
  shorttitle = {Vowel System Universals and Typology},
  author = {Lass, Roger},
  year = 1984,
  month = may,
  journal = {Phonology},
  volume = {1},
  pages = {75--111},
  issn = {2059-6286, 0265-8062},
  doi = {10.1017/S0952675700000300},
  urldate = {2025-10-07},
  abstract = {Considering all the work done on vowel system typology and universals in the past half-century (Trubetzkoy 1929, 1939; Hockett Sedlak 1969; Crothers 1978), my title may seem rather arrogant. There are after all theories of vocalic organisation about, or at least models and taxonomies; there are even attempts to explain why certain implicational universals seem to hold (from Jakobson's [laws of solidarity] (1968) to the more sophisticated treatments in Liljencrants \& Lindblom 1972; Kim 1973; etc.).},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/V3G8Y2ZH/Lass - 1984 - Vowel system universals and typology prologue to theory.pdf}
}

@article{laudanna-et-al-1992-processing,
  title = {Processing Inflectional and Derivational Morphology},
  author = {Laudanna, Alessandro and Badecker, William and Caramazza, Alfonso},
  year = 1992,
  journal = {Journal of Memory and Language},
  volume = {31},
  number = {3},
  pages = {333--348},
  publisher = {Elsevier}
}

@article{leminen-et-al-2019-morphological,
  title = {Morphological Processing in the Brain: {{The}} Good (Inflection), the Bad (Derivation) and the Ugly (Compounding)},
  shorttitle = {Morphological Processing in the Brain},
  author = {Leminen, Alina and Smolka, Eva and Du{\~n}abeitia, Jon A. and Pliatsikas, Christos},
  year = 2019,
  month = jul,
  journal = {Cortex},
  series = {Structure in Words: The Present and Future of Morphological Processing in a Multidisciplinary Perspective},
  volume = {116},
  pages = {4--44},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2018.08.016},
  urldate = {2025-10-22},
  abstract = {There is considerable behavioral evidence that morphologically complex words such as `tax-able' and `kiss-es' are processed and represented combinatorially. In other words, they are decomposed into their constituents `tax' and `-able' during comprehension (reading or listening), and producing them might also involve on--the--spot combination of these constituents (especially for inflections). However, despite increasing amount of neurocognitive research, the neural mechanisms underlying these processes are still not fully understood. The purpose of this critical review is to offer a comprehensive overview on the state-of-the-art of the research on the neural mechanisms of morphological processing. In order to take into account all types of complex words, we include findings on inflected, derived, and compound words presented both visually and aurally. More specifically, we cover a wide range of electro- and magnetoencephalography (EEG and MEG, respectively) as well as structural/functional magnetic resonance imaging (s/fMRI) studies that focus on morphological processing. We present the findings with respect to the temporal course and localization of morphologically complex word processing. We summarize the observed findings, their interpretations with respect to current psycholinguistic models, and discuss methodological approaches as well as their possible limitations.},
  keywords = {Compounding,Derivation,Inflection,Morphology,Neuroimaging},
  file = {/Users/coleman/Zotero/storage/KGQ46NL6/Leminen et al. - 2019 - Morphological processing in the brain The good (inflection), the bad (derivation) and the ugly (com.pdf;/Users/coleman/Zotero/storage/MHH5TY8M/S0010945218302661.html}
}

@article{levenshtein-1966-binary,
  title = {Binary Codes Capable of Correcting Deletions, Insertions and Reversals},
  author = {Levenshtein, Vladimir},
  year = 1966,
  journal = {Soviet Physics Doklady},
  volume = {10},
  pages = {707}
}

@book{levinson-2003-space,
  title = {Space in {{Language}} and {{Cognition}}: {{Explorations}} in {{Cognitive Diversity}}},
  shorttitle = {Space in {{Language}} and {{Cognition}}},
  author = {Levinson, Stephen C.},
  year = 2003,
  series = {Language {{Culture}} and {{Cognition}}},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9780511613609},
  urldate = {2025-11-08},
  abstract = {Languages differ in how they describe space, and such differences between languages can be used to explore the relation between language and thought. This 2003 book shows that even in a core cognitive domain like spatial thinking, language influences how people think, memorize and reason about spatial relations and directions. After outlining a typology of spatial coordinate systems in language and cognition, it is shown that not all languages use all types, and that non-linguistic cognition mirrors the systems available in the local language. The book reports on collaborative, interdisciplinary research, involving anthropologists, linguists and psychologists, conducted in many languages and cultures around the world, which establishes this robust correlation. The overall results suggest that thinking in the cognitive sciences underestimates the transformative power of language on thinking. The book will be of interest to linguists, psychologists, anthropologists and philosophers, and especially to students of spatial cognition.},
  isbn = {978-0-521-81262-7},
  file = {/Users/coleman/Zotero/storage/P4YC45Y9/D07AD2885A025E00B1C94ED722071D80.html}
}

@inproceedings{levshina-2020-how,
  title = {How Tight Is Your Language? {{A}} Semantic Typology Based on {{Mutual Information}}},
  shorttitle = {How Tight Is Your Language?},
  booktitle = {Proceedings of the 19th {{International Workshop}} on {{Treebanks}} and {{Linguistic Theories}}},
  author = {Levshina, Natalia},
  editor = {Evang, Kilian and Kallmeyer, Laura and Ehren, Rafael and Petitjean, Simon and Seyffarth, Esther and Seddah, Djam{\'e}},
  year = 2020,
  month = oct,
  pages = {70--78},
  publisher = {Association for Computational Linguistics},
  address = {D\"usseldorf, Germany},
  doi = {10.18653/v1/2020.tlt-1.7},
  urldate = {2025-05-14},
  keywords = {sivan},
  file = {/Users/coleman/Zotero/storage/99XFX4H7/Levshina_2020_How tight is your language.pdf}
}

@article{levshina-2022-semantic,
  title = {Semantic Maps of Causation: {{New}} Hybrid Approaches Based on Corpora and Grammar Descriptions},
  shorttitle = {Semantic Maps of Causation},
  author = {Levshina, Natalia},
  year = 2022,
  month = jun,
  journal = {Zeitschrift f\"ur Sprachwissenschaft},
  volume = {41},
  number = {1},
  pages = {179--205},
  publisher = {De Gruyter},
  issn = {1613-3706},
  doi = {10.1515/zfs-2021-2043},
  urldate = {2025-11-10},
  abstract = {The present paper discusses connectivity and proximity maps of causative constructions and combines them with different types of typological data. In the first case study, I show how one can create a connectivity map based on a parallel corpus. This allows us to solve many problems, such as incomplete descriptions, inconsistent terminology and the problem of determining the semantic nodes. The second part focuses on proximity maps based on Multidimensional Scaling and compares the most important semantic distinctions, which are inferred from a parallel corpus of film subtitles and from grammar descriptions. The results suggest that corpus-based maps of tokens are more sensitive to cultural and genre-related differences in the prominence of specific causation scenarios than maps based on constructional types, which are described in reference grammars. The grammar-based maps also reveal a less clear structure, which can be due to incomplete semantic descriptions in grammars. Therefore, each approach has its shortcomings, which researchers need to be aware of.},
  langid = {english},
  keywords = {causation,cluster analysis,graph theory,Multidimensional Scaling,parallel corpus},
  file = {/Users/coleman/Zotero/storage/MC9KWA2V/Levshina - 2022 - Semantic maps of causation New hybrid approaches based on corpora and grammar descriptions.pdf}
}

@article{levy-2008-expectation,
  title = {Expectation-Based Syntactic Comprehension},
  author = {Levy, Roger},
  year = 2008,
  journal = {Cognition},
  volume = {106},
  number = {3},
  pages = {1126--1177},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2007.05.006},
  abstract = {This paper investigates the role of resource allocation as a source of processing difficulty in human sentence comprehension. The paper proposes a simple information-theoretic characterization of processing difficulty as the work incurred by resource reallocation during parallel, incremental, probabilistic disambiguation in sentence comprehension, and demonstrates its equivalence to the theory of Hale [Hale, J. (2001). A probabilistic Earley parser as a psycholinguistic model. In Proceedings of NAACL (Vol. 2, pp. 159--166)], in which the difficulty of a word is proportional to its surprisal (its negative log-probability) in the context within which it appears. This proposal subsumes and clarifies findings that high-constraint contexts can facilitate lexical processing, and connects these findings to well-known models of parallel constraint-based comprehension. In addition, the theory leads to a number of specific predictions about the role of expectation in syntactic comprehension, including the reversal of locality-based difficulty patterns in syntactically constrained contexts, and conditions under which increased ambiguity facilitates processing. The paper examines a range of established results bearing on these predictions, and shows that they are largely consistent with the surprisal theory.},
  keywords = {Frequency,Information theory,Parsing,Prediction,Sentence processing,Syntactic complexity,Syntax,Word order}
}

@inproceedings{levy-et-al-2006-speakers,
  title = {Speakers Optimize Information Density through Syntactic Reduction},
  booktitle = {Advances in {{Neural Information Processing Systems}} 19, {{Proceedings}} of the {{Twentieth Annual Conference}} on {{Neural Information Processing Systems}}, {{Vancouver}}, {{British Columbia}}, {{Canada}}, {{December}} 4-7, 2006},
  author = {Levy, Roger and Jaeger, T. Florian},
  editor = {Sch{\"o}lkopf, Bernhard and Platt, John C. and Hofmann, Thomas},
  year = 2006,
  pages = {849--856},
  publisher = {MIT Press},
  url = {https://proceedings.neurips.cc/paper/2006/hash/c6a01432c8138d46ba39957a8250e027-Abstract.html},
  urldate = {2025-11-09}
}

@inproceedings{li-2025-embedding,
  title = {Embedding Derived Animacy Rankings Offer Insights into the Sources of Grammatical Animacy},
  booktitle = {Proceedings of the 2025 {{Conference}} of the {{Nations}} of the {{Americas Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}} ({{Volume}} 1: {{Long Papers}})},
  author = {Li, Vivian G.},
  editor = {Chiruzzo, Luis and Ritter, Alan and Wang, Lu},
  year = 2025,
  month = apr,
  pages = {1339--1351},
  publisher = {Association for Computational Linguistics},
  address = {Albuquerque, New Mexico, USA},
  doi = {10.18653/v1/2025.naacl-long.62},
  urldate = {2025-10-07},
  abstract = {In this study, we applied the semantic projection approach to animacy, a feature that has not been previously explored using this method. We compared the relative animacy rankings of nouns denoting animals, humans, objects, and first-, second-, and third-person pronouns, as derived from word embeddings, with rankings derived from human behavioral ratings of animacy and from grammatical patterns. Our results support the semantic projection approach as an effective method for deriving proxies of human perception from word embeddings and offer insights into the sources of grammatical animacy.},
  isbn = {979-8-89176-189-6},
  file = {/Users/coleman/Zotero/storage/6P7FBI9F/Li - 2025 - Embedding derived animacy rankings offer insights into the sources of grammatical animacy.pdf}
}

@book{li-et-al-1981-mandarin,
  title = {Mandarin {{Chinese}}: {{A Functional Reference Grammar}}},
  shorttitle = {Mandarin {{Chinese}}},
  author = {Li, Charles N. and Thompson, Sandra},
  year = 1981,
  month = aug,
  publisher = {University of California Press},
  address = {Berkeley and Los Angeles, California, USA},
  doi = {10.1525/9780520352858},
  urldate = {2025-11-09},
  abstract = {This reference grammar provides, for the first time, a description of the grammar of Mandarin Chinese, the official spoken language of China and Taiwan, in functional terms, focusing on the role and meanings of word-level and sentence-level structures in actual conversations.},
  isbn = {978-0-520-35285-8},
  langid = {english},
  keywords = {General,Language Arts & Disciplines,Linguistics},
  file = {/Users/coleman/Zotero/storage/W2SFX7XQ/LiThompson - 1981 - Mandarin Chinese A Functional Reference Grammar.pdf}
}

@incollection{lieber-et-al-2014-universals,
  title = {Universals in {{Derivation}}},
  booktitle = {The {{Oxford Handbook}} of {{Derivational Morphology}}},
  author = {Lieber, Rochelle and {\v S}tekauer, Pavol},
  editor = {Lieber, Rochelle and {\v S}tekauer, Pavol},
  year = 2014,
  month = sep,
  pages = {777--786},
  publisher = {Oxford University Press},
  address = {Oxford, UK},
  doi = {10.1093/oxfordhb/9780199641642.013.0041},
  urldate = {2025-10-22},
  abstract = {This chapter discusses the question of universals in derivational morphology, an issue that has been paid little attention. First, methodological questions are raised, including some of the obstacles in looking for universals of derivation. The next section reviews some of the past proposals for universals of derivation. The final section assesses some of these proposed universals in light of the material assembled in this volume.},
  isbn = {978-0-19-964164-2},
  file = {/Users/coleman/Zotero/storage/M7PY43TC/Lieber and tekauer - 2014 - Universals in Derivation.pdf;/Users/coleman/Zotero/storage/9WYSPLQZ/9780199641642.013.html}
}

@article{lier-2017-typology,
  title = {The Typology of Property Words in {{Oceanic}} Languages},
  author = {{\noopsort{lier}}van Lier, Eva},
  year = 2017,
  month = nov,
  journal = {Linguistics},
  volume = {55},
  number = {6},
  pages = {1237--1280},
  publisher = {De Gruyter Mouton},
  issn = {1613-396X},
  doi = {10.1515/ling-2017-0027},
  urldate = {2025-09-05},
  abstract = {This paper describes the morphosyntactic behavior of different semantic types of property words in a balanced sample of 36 Oceanic languages. After a brief general introduction to the functional typology of property words, I first discuss diversity in Oceanic property word classes from a family-internal perspective. In the second part of the paper, Oceanic property words are placed in a world-wide typological perspective. Specifically, I test their behavior with regard to two implicational universals proposed in the literature, concerning the relation between the encoding of predicative property words, the presence of grammatical tense, and locus of marking at the clause level. In typological studies, the Oceanic language family has been claimed to display verbal predicative property words, to lack tense, and to be head- or zero-marking, with marginal exceptions. This paper shows that, even though such an overall profile can be discerned, Oceanic property words exhibit more variation than is acknowledged in crosslinguistic research. Moreover, my findings for property word classes are fitted into a larger picture of lexical categorization in Oceanic languages.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  keywords = {adjectives,locus of marking,Oceanic languages,property words,tense},
  file = {/Users/coleman/Zotero/storage/EFAKJQ4N/Lier - 2017 - The typology of property words in Oceanic languages.pdf}
}

@article{liljencrants-et-al-1972-numerical,
  title = {Numerical Simulation of Vowel Quality Systems: {{The}} Role of Perceptual Contrast},
  shorttitle = {Numerical {{Simulation}} of {{Vowel Quality Systems}}},
  author = {Liljencrants, Johan and Lindblom, Bj{\"o}rn},
  year = 1972,
  month = dec,
  journal = {Language},
  volume = {48},
  number = {4},
  pages = {839--862},
  issn = {00978507},
  doi = {10.2307/411991},
  urldate = {2024-10-07},
  jstor = {411991}
}

@inproceedings{lin-et-al-2014-microsoft,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  shorttitle = {Microsoft {{COCO}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2014},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C. Lawrence},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  year = 2014,
  pages = {740--755},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-10602-1_48},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  isbn = {978-3-319-10602-1},
  langid = {english},
  keywords = {Common Object,Object Category,Object Detection,Object Instance,Scene Understanding},
  file = {/Users/coleman/Zotero/storage/43KCFAI3/Lin et al. - 2014 - Microsoft COCO Common Objects in Context.pdf}
}

@article{lin-et-al-2022-word,
  title = {Word Imageability Is Associated with Expressive Vocabulary in Children with Autism Spectrum Disorder},
  author = {Lin, Kimberly R. and Wisman Weil, Lisa and Thurm, Audrey and Lord, Catherine and Luyster, Rhiannon J.},
  year = 2022,
  month = mar,
  journal = {Autism \& Developmental Language Impairments},
  volume = {7},
  issn = {2396-9415},
  doi = {10.1177/23969415221085827},
  urldate = {2024-07-23},
  abstract = {Background \& aims Throughout typical development, children prioritize different perceptual, social, and linguistic cues to learn words. The earliest acquired words are often those that are perceptually salient and highly imageable. Imageability, the ease in which a word evokes a mental image, is a strong predictor for word age of acquisition in typically developing (TD) children, independent of other lexicosemantic features such as word frequency. However, little is known about the effects of imageability in children with autism spectrum disorder (ASD), who tend to have differences in linguistic processing and delayed language acquisition compared to their TD peers. This study explores the extent to which imageability and word frequency are associated with early noun and verb acquisition in children with ASD. Methods Secondary analyses were conducted on previously collected data of 156 children (78 TD, 78 ASD) matched on sex and parent-reported language level. Total expressive vocabulary, as measured by the MacArthur Bates Communicative Development Inventory (MB-CDI), included 123 words (78 nouns, 45 verbs) that overlapped with previously published imageability ratings and word input frequencies. A two-step hierarchical linear regression was used to examine the relationship between word input frequency, imageability, and total expressive vocabulary. An F-test was then used to assess the unique contribution of imageability on total expressive vocabulary when controlling for word input frequency. Results In both the TD and ASD groups, imageability uniquely explained a portion of the variance in total expressive vocabulary size, independent of word input frequency. Notably, imageability was significantly associated with noun vocabulary and verb vocabulary size alone, with imageability explaining a greater portion of the variance in total nouns produced than in total verbs produced. Conclusions Imageability was identified as a significant lexicosemantic feature for describing expressive vocabulary size in children with ASD. Consistent with literature on TD children, children with ASD who have small vocabularies primarily produce words that are highly imageable. Children who are more proficient word learners with larger vocabularies produce words that are less imageable, indicating a potential shift away from reliance on perceptual-based language processing. This was consistent across both noun and verb vocabularies. Implications Our findings contribute to a growing body of literature describing early word learning in children with ASD and provide a basis for exploring the use of multisensory language learning strategies.},
  note = {Article 23969415221085827},
  file = {/Users/coleman/Zotero/storage/4HU9DYXG/Lin et al_2022_Word imageability is associated with expressive vocabulary in children with.pdf}
}

@article{lindblom-et-al-1971-acoustical,
  title = {Acoustical Consequences of Lip, Tongue, Jaw, and Larynx Movement},
  author = {Lindblom, B. E. and Sundberg, J. E.},
  year = 1971,
  month = oct,
  journal = {The Journal of the Acoustical Society of America},
  volume = {50},
  number = {4},
  pages = {1166--1179},
  issn = {0001-4966},
  doi = {10.1121/1.1912750},
  langid = {english},
  pmid = {5117649},
  keywords = {Acoustics,Humans,Jaw,Larynx,Lip,Tongue}
}

@inproceedings{lindsey-et-al-2025-biology,
  title = {On the Biology of a {{Large Language Model}}},
  booktitle = {Transformer {{Circuits Thread}}},
  author = {Lindsey, Jack and Gurnee, Wes and Ameisen, Emmanuel and Chen, Brian and Pearce, Adam and Turner, Nicholas L. and Citro, Craig and Abrahams, David and Carter, Shan and Hosmer, Basil and Marcus, Jonathan and Sklar, Michael and Templeton, Adly and Bricken, Trenton and McDougall, Callum and Cunningham, Hoagy and Henighan, Thomas and Jermyn, Adam and Jones, Andy and Persic, Andrew and Qi, Zhenyi and Thompson, T. Ben and Zimmerman, Sam and Rivoire, Kelley and Conerly, Thomas and Olah, Chris and Batson, Joshua},
  year = 2025,
  month = mar,
  url = {https://transformer-circuits.pub/2025/attribution-graphs/biology.html},
  urldate = {2025-11-04},
  abstract = {We investigate the internal mechanisms used by Claude 3.5 Haiku --- Anthropic's lightweight production model --- in a variety of contexts, using our circuit tracing methodology.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/28IU7L3H/biology.html}
}

@inproceedings{liu-et-al-2021-lexical,
  title = {Lexical {{Semantic Recognition}}},
  booktitle = {Proceedings of the 17th {{Workshop}} on {{Multiword Expressions}} ({{MWE}} 2021)},
  author = {Liu, Nelson F. and Hershcovich, Daniel and Kranzlein, Michael and Schneider, Nathan},
  editor = {Cook, Paul and Mitrovi{\'c}, Jelena and Escart{\'i}n, Carla Parra and Vaidya, Ashwini and Osenova, Petya and Taslimipoor, Shiva and Ramisch, Carlos},
  year = 2021,
  month = aug,
  pages = {49--56},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.mwe-1.6},
  urldate = {2025-08-14},
  abstract = {In lexical semantics, full-sentence segmentation and segment labeling of various phenomena are generally treated separately, despite their interdependence. We hypothesize that a unified lexical semantic recognition task is an effective way to encapsulate previously disparate styles of annotation, including multiword expression identification / classification and supersense tagging. Using the STREUSLE corpus, we train a neural CRF sequence tagger and evaluate its performance along various axes of annotation. As the label set generalizes that of previous tasks (PARSEME, DiMSUM), we additionally evaluate how well the model generalizes to those test sets, finding that it approaches or surpasses existing models despite training only on STREUSLE. Our work also establishes baseline models and evaluation metrics for integrated and accurate modeling of lexical semantics, facilitating future work in this area.},
  file = {/Users/coleman/Zotero/storage/XNSERLLT/Liu et al. - 2021 - Lexical Semantic Recognition.pdf}
}

@inproceedings{ljubesic-et-al-2018-predicting,
  title = {Predicting {{Concreteness}} and {{Imageability}} of {{Words Within}} and {{Across Languages}} via {{Word Embeddings}}},
  booktitle = {Proceedings of the {{Third Workshop}} on {{Representation Learning}} for {{NLP}}},
  author = {Ljube{\v s}i{\'c}, Nikola and Fi{\v s}er, Darja and {Peti-Stanti{\'c}}, Anita},
  editor = {Augenstein, Isabelle and Cao, Kris and He, He and Hill, Felix and Gella, Spandana and Kiros, Jamie and Mei, Hongyuan and Misra, Dipendra},
  year = 2018,
  month = jul,
  pages = {217--222},
  publisher = {Association for Computational Linguistics},
  address = {Melbourne, Australia},
  doi = {10.18653/v1/W18-3028},
  urldate = {2024-10-07},
  abstract = {The notions of concreteness and imageability, traditionally important in psycholinguistics, are gaining significance in semantic-oriented natural language processing tasks. In this paper we investigate the predictability of these two concepts via supervised learning, using word embeddings as explanatory variables. We perform predictions both within and across languages by exploiting collections of cross-lingual embeddings aligned to a single vector space. We show that the notions of concreteness and imageability are highly predictable both within and across languages, with a moderate loss of up to 20\% in correlation when predicting across languages. We further show that the cross-lingual transfer via word embeddings is more efficient than the simple transfer via bilingual dictionaries.},
  file = {/Users/coleman/Zotero/storage/NWICZTAZ/Ljubei et al_2018_Predicting Concreteness and Imageability of Words Within and Across Languages.pdf}
}

@article{lynott-et-al-2020-lancaster,
  title = {The {{Lancaster Sensorimotor Norms}}: Multidimensional Measures of Perceptual and Action Strength for 40,000 {{English}} Words},
  shorttitle = {The {{Lancaster Sensorimotor Norms}}},
  author = {Lynott, Dermot and Connell, Louise and Brysbaert, Marc and Brand, James and Carney, James},
  year = 2020,
  month = jun,
  journal = {Behavior Research Methods},
  volume = {52},
  number = {3},
  pages = {1271--1291},
  issn = {1554-3528},
  doi = {10.3758/s13428-019-01316-z},
  urldate = {2024-10-15},
  abstract = {Sensorimotor information plays a fundamental role in cognition. However, the existing materials that measure the sensorimotor basis of word meanings and concepts have been restricted in terms of their sample size and breadth of sensorimotor experience. Here we present norms of sensorimotor strength for 39,707 concepts across six perceptual modalities (touch, hearing, smell, taste, vision, and interoception) and five action effectors (mouth/throat, hand/arm, foot/leg, head excluding mouth/throat, and torso), gathered from a total of 3,500 individual participants using Amazon's Mechanical Turk platform. The Lancaster Sensorimotor Norms are unique and innovative in a number of respects: They represent the largest-ever set of semantic norms for English, at 40,000 words \texttimes{} 11 dimensions (plus several informative cross-dimensional variables), they extend perceptual strength norming to the new modality of interoception, and they include the first norming of action strength across separate bodily effectors. In the first study, we describe the data collection procedures, provide summary descriptives of the dataset, and interpret the relations observed between sensorimotor dimensions. We then report two further studies, in which we (1) extracted an optimal single-variable composite of the 11-dimension sensorimotor profile (Minkowski 3 strength) and (2) demonstrated the utility of both perceptual and action strength in facilitating lexical decision times and accuracy in two separate datasets. These norms provide a valuable resource to researchers in diverse areas, including psycholinguistics, grounded cognition, cognitive semantics, knowledge representation, machine learning, and big-data approaches to the analysis of language and conceptual representations. The data are accessible via the Open Science Framework (http://osf.io/7emr6/) and an interactive web application (https://www.lancaster.ac.uk/psychology/lsnorms/).},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/QJCRP2YL/Lynott et al_2020_The Lancaster Sensorimotor Norms.pdf}
}

@inproceedings{machonis-2018-linguistic,
  title = {Linguistic {{Resources}} for {{Phrasal Verb Identification}}},
  booktitle = {Proceedings of the {{First Workshop}} on {{Linguistic Resources}} for {{Natural Language Processing}}},
  author = {Machonis, Peter},
  editor = {Machonis, Peter and Barreiro, Anabela and Kocijan, Kristina and Silberztein, Max},
  year = 2018,
  month = aug,
  pages = {18--27},
  publisher = {Association for Computational Linguistics},
  address = {Santa Fe, New Mexico, USA},
  url = {https://aclanthology.org/W18-3804/},
  urldate = {2025-09-20},
  abstract = {This paper shows how a Lexicon-Grammar dictionary of English phrasal verbs (PV) can be transformed into an electronic dictionary, and with the help of multiple grammars, dictionaries, and filters within the linguistic development environment, NooJ, how to accurately identify PV in large corpora. The NooJ program is an alternative to statistical methods commonly used in NLP: all PV are listed in a dictionary and then located by means of a PV grammar in both continuous and discontinuous format. Results are then refined with a series of dictionaries, disambiguating grammars, and other linguistics recourses. The main advantage of such a program is that all PV can be identified in any corpus. The only drawback is that PV not listed in the dictionary (e.g., archaic forms, recent neologisms) are not identified; however, new PV can easily be added to the electronic dictionary, which is freely available to all.},
  file = {/Users/coleman/Zotero/storage/E5CTPK9P/Machonis - 2018 - Linguistic Resources for Phrasal Verb Identification.pdf}
}

@article{mackay-1978-derivational,
  title = {Derivational Rules and the Internal Lexicon},
  author = {MacKay, Donald G.},
  year = 1978,
  journal = {Journal of verbal learning and verbal behavior},
  volume = {17},
  number = {1},
  pages = {61--71},
  publisher = {Elsevier}
}

@inproceedings{malouf-et-al-2020-lexical,
  title = {Lexical Databases for Computational Analyses: {{A}} Linguistic Perspective},
  booktitle = {Proceedings of the Society for Computation in Linguistics 2020},
  author = {Malouf, Robert and Ackerman, Farrell and Semenuks, Arturs},
  editor = {Ettinger, Allyson and Jarosz, Gaja and Pater, Joe},
  year = 2020,
  month = jan,
  pages = {446--456},
  publisher = {Association for Computational Linguistics},
  address = {New York, New York, USA},
  url = {https://aclanthology.org/2020.scil-1.52}
}

@article{manzini-et-al-lexical,
  title = {On the {{Lexical}}/{{Functional Divide}}: {{The Case}} of {{Negation}}},
  shorttitle = {On the {{Lexical}}/{{Functional Divide}}},
  author = {Manzini, M. Rita and Savoia, Leonardo M.},
  url = {https://academic.oup.com/book/32617/chapter/270490642},
  urldate = {2025-09-06},
  abstract = {Abstract. This chapter challenges the distinction between functional and lexical items, and argues that what is usually claimed to fill the head or the spe},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/84ZLZS8Q/270490642.html}
}

@book{martinheidegger-1916-kategorien,
  title = {{Die Kategorien- und Bedeutungslehre des Duns Scotus}},
  author = {{Martin Heidegger}},
  year = 1916,
  url = {http://archive.org/details/Bedeutungslehre},
  urldate = {2025-09-06},
  abstract = {T\"ubingen : Mohr, 1916 Teilw. zugl.: Freiburg i.Br., Univ., Habil.-Schr., 1915},
  copyright = {http://creativecommons.org/publicdomain/mark/1.0/},
  langid = {german},
  keywords = {Philosophy}
}

@article{matthei-et-al-1989-postaccess,
  title = {Postaccess Processes in the Open vs. Closed Class Distinction},
  author = {Matthei, Edward H. and Kean, Mary-Louise},
  year = 1989,
  month = feb,
  journal = {Brain and Language},
  volume = {36},
  number = {2},
  pages = {163--180},
  issn = {0093-934X},
  doi = {10.1016/0093-934X(89)90059-X},
  urldate = {2025-10-07},
  abstract = {We present the results of two auditory lexical decision experiments in which we attempted to replicate findings originally presented in Bradley (1978, Computational distinctions of vocabulary type, Ph.D. dissertation, MIT). The results obtained by Bradley were used as evidence for a processing distinction between the open and the closed class vocabularies; this distinction was then used as part of an explanation for agrammatism in the comprehension and production of Broca's aphasics. In our first experiment we failed to replicate Bradley's result of frequency insensitivity in the closed class. Our second experiment, however, replicates Bradley's finding that closed class based nonwords (e.g., thanage) fail to induce interference effects in nonword decisions. We argue that our results, together with the various other reported failures to replicate Bradley's frequency insensitivity result, indicate that the open and closed classes may play distinct roles in postaccess phenomena involving the processing of morphological information but that such studies cannot address the question of whether the open vs. closed class distinction plays a role in syntactic processing.},
  file = {/Users/coleman/Zotero/storage/7RYVC35L/Matthei and Kean - 1989 - Postaccess processes in the open vs. closed class distinction.pdf;/Users/coleman/Zotero/storage/DCZ2MLBV/0093934X8990059X.html}
}

@inproceedings{matthews-et-al-2025-disentangling,
  title = {Disentangling Language Change: Sparse Autoencoders Quantify the Semantic Evolution of Indigeneity in {{French}}},
  shorttitle = {Disentangling Language Change},
  booktitle = {Proceedings of the 2025 {{Conference}} of the {{Nations}} of the {{Americas Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}} ({{Volume}} 1: {{Long Papers}})},
  author = {Matthews, Jacob A. and Dubreuil, Laurent and Terhmina, Imane and Sun, Yunci and Wilkens, Matthew and {\noopsort{schijndel}}{van Schijndel}, Marten},
  editor = {Chiruzzo, Luis and Ritter, Alan and Wang, Lu},
  year = 2025,
  month = apr,
  pages = {11208--11222},
  publisher = {Association for Computational Linguistics},
  address = {Albuquerque, New Mexico, USA},
  doi = {10.18653/v1/2025.naacl-long.559},
  urldate = {2025-11-06},
  abstract = {This study presents a novel approach to analyzing historical language change, focusing on the evolving semantics of the French term ``indig\`ene(s)'' (``indigenous'') between 1825 and 1950. While existing approaches to measuring semantic change with contextual word embeddings (CWE) rely primarily on similarity measures or clustering, these methods may not be suitable for highly imbalanced datasets, and pose challenges for interpretation. For this reason, we propose an interpretable, feature-level approach to analyzing language change, which we use to trace the semantic evolution of ``indig\`ene(s)'' over a 125-year period. Following recent work on sequence embeddings (O'Neill et al., 2024), we use k-sparse autoencoders (k-SAE) (Makhzani and Frey, 2013) to interpret over 210,000 CWEs generated using sentences sourced from the French National Library. We demonstrate that k-SAEs can learn interpretable features from CWEs, as well as how differences in feature activations across time periods reveal highly specific aspects of language change. In addition, we show that diachronic change in feature activation frequency reflects the evolution of French colonial legal structures during the 19th and 20th centuries.},
  isbn = {979-8-89176-189-6},
  file = {/Users/coleman/Zotero/storage/GJE39PV9/Matthews et al. - 2025 - Disentangling language change sparse autoencoders quantify the semantic evolution of indigeneity in.pdf}
}

@article{matzig-2009-spared,
  title = {Spared Syntax and Impaired Spell-out: The Case of Prepositions in {{Broca}}'s and Anomic Aphasia},
  author = {M{\"a}tzig, S},
  year = 2009,
  abstract = {The present study deals with the impairment of prepositions, a somewhat neglected topic in aphasia research. It is the first to investigate the availability of all types of prepositions (i.e., spatial, temporal, other meaningful, subcategorized, syntactic prepositions, and particles) in a variety of comprehension and production tasks in one anomic aphasic and four Broca's aphasic patients and healthy speakers. While the availability of spatial, temporal, or subcategorized prepositions has been investigated, other preposition types have never been studied before.},
  langid = {english},
  keywords = {adpositions},
  file = {/Users/coleman/Zotero/storage/WCRFU4ZJ/Mtzig - Spared syntax and impaired spell-out the case of prepositions in Broca's and anomic aphasia.pdf}
}

@article{matzig-et-al-2010-spared,
  title = {Spared Syntax and Impaired Spell-out: {{The}} Case of Prepositions},
  shorttitle = {Spared Syntax and Impaired Spell-Out},
  author = {M{\"a}tzig, Simone and Druks, Judit and Neeleman, Ad and Craig, Gordon},
  year = 2010,
  month = jul,
  journal = {Journal of Neurolinguistics},
  volume = {23},
  number = {4},
  pages = {354--382},
  issn = {0911-6044},
  doi = {10.1016/j.jneuroling.2010.02.002},
  urldate = {2025-11-06},
  abstract = {The objective of the study was to identify the factors that determine the preservation/impairment of prepositions in aphasia. Five parameters derived from previous research (Bennis et~al., 1983, Friederici, 1982, Grodzinsky, 1988, Kean, 1977, Kean, 1979, Kreindler and Mih\~ailescu, 1970) were examined in a sentence completion task and three types of grammaticality judgement tasks using four subcategories of prepositions with 18 preposition tokens in a large number of test sentences. Prepositions were found impaired in both Broca's and anomic aphasia. Most of the parameters could not account for the data, and some data were in the opposite direction to the predicted. No disproportionate impairments of meaningless prepositions were found and prepositions with syntactic function were best preserved in the majority of patients. Patients made predominately within-category substitution errors. The results are interpreted as evidence for preserved syntactic knowledge about prepositions. It is suggested that a deficit at the post syntactic level of (late) spell-out is the underlying reason for the preposition deficit.},
  keywords = {Agrammatism,Aphasia,Grammatical morphemes,Prepositions,Spell-out},
  file = {/Users/coleman/Zotero/storage/KQ4LRHHE/Mtzig et al. - 2010 - Spared syntax and impaired spell-out The case of prepositions.pdf;/Users/coleman/Zotero/storage/4NR5YSGR/S0911604410000254.html}
}

@inproceedings{mccarthy-et-al-2003-detecting,
  title = {Detecting a {{Continuum}} of {{Compositionality}} in {{Phrasal Verbs}}},
  booktitle = {Proceedings of the {{ACL}} 2003 {{Workshop}} on {{Multiword Expressions}}: {{Analysis}}, {{Acquisition}} and {{Treatment}}},
  author = {McCarthy, Diana and Keller, Bill and Carroll, John},
  year = 2003,
  month = jul,
  pages = {73--80},
  publisher = {Association for Computational Linguistics},
  address = {Sapporo, Japan},
  doi = {10.3115/1119282.1119292},
  urldate = {2025-09-20},
  file = {/Users/coleman/Zotero/storage/PRKCDIPZ/McCarthy et al. - 2003 - Detecting a Continuum of Compositionality in Phrasal Verbs.pdf}
}

@inproceedings{mccarthy-et-al-2020-unimorph,
  title = {{{UniMorph}} 3.0: {{Universal Morphology}}},
  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
  author = {McCarthy, Arya D. and Kirov, Christo and Grella, Matteo and Nidhi, Amrit and Xia, Patrick and Gorman, Kyle and Vylomova, Ekaterina and Mielke, Sabrina J. and Nicolai, Garrett and Silfverberg, Miikka and Arkhangelskiy, Timofey and Krizhanovsky, Nataly and Krizhanovsky, Andrew and Klyachko, Elena and Sorokin, Alexey and Mansfield, John and Ern{\v s}treits, Valts and Pinter, Yuval and Jacobs, Cassandra L. and Cotterell, Ryan and Hulden, Mans and Yarowsky, David},
  year = 2020,
  month = may,
  pages = {3922--3931},
  publisher = {European Language Resources Association},
  address = {Marseille, France},
  url = {https://aclanthology.org/2020.lrec-1.483},
  abstract = {The Universal Morphology (UniMorph) project is a collaborative effort providing broad-coverage instantiated normalized morphological paradigms for hundreds of diverse world languages. The project comprises two major thrusts: a language-independent feature schema for rich morphological annotation and a type-level resource of annotated data in diverse languages realizing that schema. We have implemented several improvements to the extraction pipeline which creates most of our data, so that it is both more complete and more correct. We have added 66 new languages, as well as new parts of speech for 12 languages. We have also amended the schema in several ways. Finally, we present three new community tools: two to validate data for resource creators, and one to make morphological data available from the command line. UniMorph is based at the Center for Language and Speech Processing (CLSP) at Johns Hopkins University in Baltimore, Maryland. This paper details advances made to the schema, tooling, and dissemination of project resources since the UniMorph 2.0 release described at LREC 2018.},
  isbn = {979-10-95546-34-4},
  langid = {english}
}

@inproceedings{mcdonald-et-al-2013-universal,
  title = {Universal {{Dependency Annotation}} for {{Multilingual Parsing}}},
  booktitle = {Proceedings of the 51st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {McDonald, Ryan and Nivre, Joakim and {Quirmbach-Brundage}, Yvonne and Goldberg, Yoav and Das, Dipanjan and Ganchev, Kuzman and Hall, Keith and Petrov, Slav and Zhang, Hao and T{\"a}ckstr{\"o}m, Oscar and Bedini, Claudia and Bertomeu Castell{\'o}, N{\'u}ria and Lee, Jungmee},
  editor = {Schuetze, Hinrich and Fung, Pascale and Poesio, Massimo},
  year = 2013,
  month = aug,
  pages = {92--97},
  publisher = {Association for Computational Linguistics},
  address = {Sofia, Bulgaria},
  url = {https://aclanthology.org/P13-2017/},
  urldate = {2025-11-09},
  file = {/Users/coleman/Zotero/storage/DXWLC5LL/McDonald et al. - 2013 - Universal Dependency Annotation for Multilingual Parsing.pdf}
}

@inproceedings{mcgahay-2025-modeling,
  title = {Modeling {{Vowel System Typology Using Iterated Confusion Minimization}}},
  booktitle = {Proceedings of {{Interspeech}} 2025},
  author = {McGahay, John},
  year = 2025,
  pages = {2955--2959},
  publisher = {International Speech Communication Association},
  address = {Rotterdam, Netherlands},
  doi = {10.21437/Interspeech.2025-2192},
  urldate = {2025-11-09},
  langid = {english}
}

@article{miestamo-et-al-2016-sampling,
  title = {Sampling for Variety},
  author = {Miestamo, Matti and Bakker, Dik and Arppe, Antti},
  year = 2016,
  month = oct,
  journal = {Linguistic Typology},
  volume = {20},
  number = {2},
  pages = {233--296},
  publisher = {De Gruyter Mouton},
  issn = {1613-415X},
  doi = {10.1515/lingty-2016-0006},
  urldate = {2025-11-07},
  abstract = {Variety sampling aims at capturing as much of the world's linguistic variety as possible. The article discusses and compares two sampling methods designed for variety sampling: the Diversity Value method, in which sample languages are picked according to the diversity found in family trees, and the Genus-Macroarea method, in which genealogical stratification is primarily based on genera and areal stratification pays attention to the proportional representation of the genealogical diversity of macroareas. The pros and cons of the methods are discussed, some additional features are introduced to the Genus-Macroarea method, and the ability of both methods to capture crosslinguistic variety is tested with computerized simulations drawing on data in The world atlas of language structures database.},
  langid = {english},
  keywords = {genealogical classification,genus,macroarea,methodology,sampling,variety sampling},
  file = {/Users/coleman/Zotero/storage/YAIIQH7T/Miestamo et al. - 2016 - Sampling for variety.pdf}
}

@inproceedings{mikolov-et-al-2013-distributed,
  title = {Distributed Representations of Words and Phrases and Their Compositionality},
  booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
  author = {Mikolov, Tom{\'a}{\v s} and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = 2013,
  series = {{{NIPS}}'13},
  pages = {3111--3119},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.}
}

@inproceedings{mikolov-et-al-2013-efficient,
  title = {Efficient Estimation of Word Representations in Vector Space},
  booktitle = {1st {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2013, {{Scottsdale}}, {{Arizona}}, {{USA}}, {{May}} 2-4, 2013, {{Workshop Track Proceedings}}},
  author = {Mikolov, Tom{\'a}{\v s} and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  editor = {Bengio, Yoshua and LeCun, Yann},
  year = 2013,
  url = {http://arxiv.org/abs/1301.3781},
  urldate = {2025-11-04}
}

@incollection{minda-et-al-2011-prototype,
  title = {Prototype Models of Categorization: Basic Formulation, Predictions, and Limitations},
  shorttitle = {Prototype Models of Categorization},
  booktitle = {Formal {{Approaches}} in {{Categorization}}},
  author = {Minda, John Paul and Smith, J. David},
  editor = {Wills, Andy J. and Pothos, Emmanuel M.},
  year = 2011,
  pages = {40--64},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9780511921322.003},
  urldate = {2025-09-20},
  abstract = {SummaryThe prototype model has had a long history in cognitive psychology, and prototype theory posed an early challenge to the classical view of concepts. Prototype models assume that categories are represented by a summary representation of a category (i.e., a prototype) that might represent information about the most common features, the average feature values, or even the ideal features of a category. Prototype models assume that classification decisions are made on the basis of how similar an object is to a category prototype. This chapter presents a formal description of the model, the motivation and theoretical history of the model, as well as several simulations that illustrate the model's properties. In general, the prototype model is well suited to explain the learning of many visual categories (e.g. dot patterns) and categories with a strong family-resemblance structure.Prototype models of categorization: basic formulation, predictions, and limitationsCategories are fundamental to cognition, and the ability to learn and use categories is present in all humans and animals. An important theoretical account of categorization is the prototype view (Homa \&amp; Cultice, 1984; Homa et al., 1973; Minda \&amp; Smith, 2001, 2002; Posner \&amp; Keele, 1968; J. D. Smith \&amp; Minda, 1998, 2000, 2001; J. D. Smith, Redford, \&amp; Haas, 2008). The prototype view assumes that a category of things in the world (objects, animals, shapes, etc.) can be represented in the mind by a prototype.},
  isbn = {978-0-511-92132-2},
  file = {/Users/coleman/Zotero/storage/LMCGJ67T/Minda and Smith - 2011 - Prototype models of categorization basic formulation, predictions, and limitations.pdf}
}

@inproceedings{minixhofer-et-al-2024-zeroshot,
  title = {Zero-Shot Tokenizer Transfer},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Minixhofer, Benjamin and Ponti, Edoardo M. and Vuli{\'c}, Ivan},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = 2024,
  volume = {37},
  pages = {46791--46818},
  publisher = {Curran Associates, Inc.},
  doi = {10.52202/079017-1484}
}

@inproceedings{misra-et-al-2024-language,
  title = {Language {{Models Learn Rare Phenomena}} from {{Less Rare Phenomena}}: {{The Case}} of the {{Missing AANNs}}},
  shorttitle = {Language {{Models Learn Rare Phenomena}} from {{Less Rare Phenomena}}},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Misra, Kanishka and Mahowald, Kyle},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = 2024,
  month = nov,
  pages = {913--929},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.53},
  urldate = {2025-11-08},
  abstract = {Language models learn rare syntactic phenomena, but the extent to which this is attributable to generalization vs. memorization is a major open question. To that end, we iteratively trained transformer language models on systematically manipulated corpora which were human-scale in size, and then evaluated their learning of a rare grammatical phenomenon: the English Article+Adjective+Numeral+Noun (AANN) construction (``a beautiful five days''). We compared how well this construction was learned on the default corpus relative to a counterfactual corpus in which AANN sentences were removed. We found that AANNs were still learned better than systematically perturbed variants of the construction. Using additional counterfactual corpora, we suggest that this learning occurs through generalization from related constructions (e.g., ``a few days''). An additional experiment showed that this learning is enhanced when there is more variability in the input. Taken together, our results provide an existence proof that LMs can learn rare grammatical phenomena by generalization from less rare phenomena. Data and code: https://github.com/kanishkamisra/aannalysis.},
  file = {/Users/coleman/Zotero/storage/E4IBIU9K/Misra and Mahowald - 2024 - Language Models Learn Rare Phenomena from Less Rare Phenomena The Case of the Missing AANNs.pdf}
}

@book{miyamoto-1999-light,
  title = {The Light Verb Construction in {{Japanese}}: The Role of the Verbal Noun},
  shorttitle = {The Light Verb Construction in {{Japanese}}},
  author = {Miyamoto, Tadao},
  year = 1999,
  series = {Linguistik Aktuell, v. 29},
  edition = {1st ed.},
  publisher = {J. Benjamins Pub.},
  address = {Philadelphia, PA},
  doi = {10.1075/la.29},
  abstract = {This study deals with the so-called Light Verb Construction in Japanese, which consists of the verb "suru" 'do' and an accusative ("o") marked verbal noun (VN). There have been unresolved debates on the role of "suru": whether "suru" in "VN-o suru" functions as a light or heavy verb. The previous studies attempt to disambiguate "VN-o suru" formations by relying solely on examining whether "suru" can be thematically light or not. This study argues that the ambiguity does not stem from the 'weight' of "suru" but from its accusative phrase: whether it is headed by a thematic (complex event) VN or non-thematic (simple event) VN. Using a principles and parameters approach and employing ideas from conceptual semantics and theories of aspect, this study demonstrates that the characterization of "VN-o suru" formations arises not from the dichotic behavior of "suru" but from the dichotic behavior of the accusative phrase.},
  isbn = {9786612160080},
  langid = {english},
  keywords = {Japanese language,Noun phrase,Verb}
}

@article{mollica-et-al-2021-forms,
  title = {The Forms and Meanings of Grammatical Markers Support Efficient Communication},
  author = {Mollica, Francis and Bacon, Geoff and Zaslavsky, Noga and Xu, Yang and Regier, Terry and Kemp, Charles},
  year = 2021,
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {49},
  pages = {e2025993118},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2025993118},
  urldate = {2025-11-08},
  abstract = {Functionalist accounts of language suggest that forms are paired with meanings in ways that support efficient communication. Previous work on grammatical marking suggests that word forms have lengths that enable efficient production, and work on the semantic typology of the lexicon suggests that word meanings represent efficient partitions of semantic space. Here we establish a theoretical link between these two lines of work and present an information-theoretic analysis that captures how communicative pressures influence both form and meaning. We apply our approach to the grammatical features of number, tense, and evidentiality and show that the approach explains both which systems of feature values are attested across languages and the relative lengths of the forms for those feature values. Our approach shows that general information-theoretic principles can capture variation in both form and meaning across languages.},
  file = {/Users/coleman/Zotero/storage/B5GCJESC/Mollica et al. - 2021 - The forms and meanings of grammatical markers support efficient communication.pdf}
}

@incollection{mollica-et-al-2025-informationtheoretic,
  title = {Information-Theoretic and Machine Learning Methods for Semantic Categorization},
  booktitle = {The {{Oxford Handbook}} of {{Approaches}} to {{Language Evolution}}},
  author = {Mollica, Francis and Zaslavsky, Noga},
  editor = {Raviv, Limor and Boeckx, Cedric},
  year = 2025,
  month = may,
  pages = {423--440},
  publisher = {Oxford University Press},
  address = {Oxford, UK},
  doi = {10.1093/oxfordhb/9780192886491.013.21},
  urldate = {2025-11-09},
  abstract = {Of the incredible amount of structure in the world, a small subset is privileged enough to be carved into semantic categories, i.e. partitions that are encoded and transmitted via language. What makes a useful partitioning? Why do different cultures sometimes have different partitions? Characterizing the constrained variation and universals in semantic categorization remains a foundational question for cognitive science. One prominent approach argues that semantic categories are shaped by pressure to communicate efficiently, captured by a trade-off between cognitive economy and communicative accuracy. This idea has recently been formulated and tested using tools from machine learning and information theory and, in particular, the Information Bottleneck principle (Tishby et al. 1999; Zaslavsky et al. 2018). Here, we review the framework and its empirical evidence across languages and semantic domains, and contextualize it with respect to alternative formulations of efficient communication. We demonstrate the application of this framework with the colour domain.},
  isbn = {978-0-19-288649-1},
  file = {/Users/coleman/Zotero/storage/4M6TSKV5/9780192886491.013.html}
}

@article{morita-2010-internal,
  title = {The Internal Structures of Adjectives in {{Japanese}}},
  author = {Morita, Chigusa},
  year = 2010,
  journal = {Linguistic research: working papers in English linguistics},
  volume = {26},
  pages = {105--117}
}

@article{narasimhan-et-al-2015-unsupervised,
  title = {An Unsupervised Method for Uncovering Morphological Chains},
  author = {Narasimhan, Karthik and Barzilay, Regina and Jaakkola, Tommi},
  year = 2015,
  month = dec,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {3},
  pages = {157--167}
}

@article{neville-et-al-1992-fractionating,
  title = {Fractionating Language: {{Different}} Neural Subsystems with Different Sensitive Periods},
  shorttitle = {Fractionating {{Language}}},
  author = {Neville, Helen J. and Mills, Debra L. and Lawson, Donald S.},
  year = 1992,
  month = may,
  journal = {Cerebral Cortex},
  volume = {2},
  number = {3},
  pages = {244--258},
  issn = {1047-3211},
  doi = {10.1093/cercor/2.3.244},
  urldate = {2025-11-07},
  abstract = {Theoretical considerations and psycholinguistic studies have alternately provided criticism and support for the proposal that semantic and grammatical functions are distinct subprocesses within the language domain. Neurobiological evidence concerning this hypothesis was sought by (1) comparing, in normal adults, event-related brain potentials (ERPs) elicited by words that provide primarily semantic information (open class) and grammatical information (closed class) and (2) comparing the effects of the altered early language experience of congenitaily deaf subjects on ERPs to open and closed class words. In normal-hearing adults, the different word types elicited qualitatively different ERPs that were compatible with the hypothesized different roles of the word classes in language processing. In addition, where as ERP indices of semantic processing were virtually identical in deaf and hearing subjects, those linked to grammatical processes were markedly different in deaf and hearing subjects. The results suggest that nonidentical neural systems with different developmental vulnerabilities mediate these different aspects of language. More generally, these results provide neurobiological support for the distinction between semantic and gram-matical functions.},
  file = {/Users/coleman/Zotero/storage/PH968KJP/Neville et al. - 1992 - Fractionating Language Different Neural Subsystems with Different Sensitive Periods.pdf;/Users/coleman/Zotero/storage/977LM3UL/2.3.html}
}

@incollection{newmeyer-1999-discrete,
  title = {The Discrete Nature {{Of}} Syntactic Categories: {{Against}} a Prototype-Based Account},
  booktitle = {The {{Nature}} and {{Function}} of {{Syntactic Categories}}},
  author = {Newmeyer, Frederick J.},
  year = 1999,
  month = jan,
  pages = {221--250},
  publisher = {Brill},
  address = {Leiden, Netherlands},
  doi = {10.1163/9781849500098_009},
  urldate = {2025-09-12},
  langid = {english},
  keywords = {Languages and Linguistics,Morphology & Syntax},
  file = {/Users/coleman/Zotero/storage/2CFDCP4G/Newmeyer - 1999 - The Discrete Nature Of Syntactic Categories Against A Prototype-Based Account.pdf}
}

@article{newmeyer-2007-linguistic,
  title = {Linguistic Typology Requires Crosslinguistic Formal Categories},
  author = {Newmeyer, Frederick J},
  year = 2007,
  series = {Linguistic {{Typology}}},
  volume = {11},
  number = {1},
  pages = {133--157},
  doi = {10.1515/LINGTY.2007.012},
  urldate = {2024-05-14}
}

@article{newmeyer-2010-comparative,
  title = {On Comparative Concepts and Descriptive Categories: {{A}} Reply to {{Haspelmath}}},
  shorttitle = {On Comparative Concepts and Descriptive Categories},
  author = {Newmeyer, Frederick J.},
  year = 2010,
  journal = {Language},
  volume = {86},
  number = {3},
  eprint = {40961696},
  eprinttype = {jstor},
  pages = {688--695},
  publisher = {Linguistic Society of America},
  issn = {0097-8507},
  url = {https://www.jstor.org/stable/40961696},
  urldate = {2025-09-20},
  file = {/Users/coleman/Zotero/storage/H6YDB3N4/Newmeyer - 2010 - On comparative concepts and descriptive categories A reply to Haspelmath.pdf}
}

@article{nishiyama-japanese,
  title = {Japanese {{Verbal Morphology}} in {{Coordination}}},
  author = {Nishiyama, Kunio},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/RLXGZGXD/Nishiyama - Japanese Verbal Morphology in Coordination.pdf}
}

@inproceedings{nivre-2025-constructions,
  title = {Constructions and {{Strategies}} in {{Universal Dependencies}}},
  booktitle = {Proceedings of the {{Joint}} 25th {{Nordic Conference}} on {{Computational Linguistics}} and 11th {{Baltic Conference}} on {{Human Language Technologies}} ({{NoDaLiDa}}/{{Baltic-HLT}} 2025)},
  author = {Nivre, Joakim},
  editor = {Johansson, Richard and Stymne, Sara},
  year = 2025,
  month = mar,
  pages = {419--423},
  publisher = {University of Tartu Library},
  address = {Tallinn, Estonia},
  url = {https://aclanthology.org/2025.nodalida-1.45/},
  urldate = {2025-11-09},
  abstract = {Is the framework of Universal Dependencies (UD) compatible with findings from linguistic typology? One way to find out is to investigate whether UD can adequately represent constructions of the world's languages, as described in William Croft's recent book Morphosyntax. This paper discusses how such an investigation could be carried out and why it would be useful.},
  isbn = {978-9908-53-109-0},
  file = {/Users/coleman/Zotero/storage/JSEBRCI4/Nivre - 2025 - Constructions and Strategies in Universal Dependencies.pdf}
}

@inproceedings{nivre-croft-2025-reference,
  title = {Reference and Modification in {{Universal Dependencies}}},
  booktitle = {Proceedings of the Eighth Workshop on Universal Dependencies ({{UDW}}, {{SyntaxFest}} 2025)},
  author = {Nivre, Joakim and Croft, William},
  editor = {Bouma, Gosse and {\c C}{\"o}ltekin, {\c C}a{\u g}r{\i}},
  year = 2025,
  month = aug,
  pages = {1--10},
  publisher = {Association for Computational Linguistics},
  address = {Ljubljana, Slovenia},
  url = {https://aclanthology.org/2025.udw-1.1/},
  abstract = {Is the framework of Universal Dependencies (UD) compatible with findings from linguistic typology? To address this question, we need to systematically review how UD represents linguistic constructions in the world's languages, and how it handles the range of morphosyntactic variation attested in linguistic typology. In this paper, we start this review by discussing reference and modification constructions. The review shows that, although UD can represent all major constructions in this area, there are a number of cases where UD categories do not align systematically with a typological classification of constructions, and where constructional similarity is therefore not transparent across languages. We also identify limitations in the representation of certain morphosyntactic strategies, notably indexation and linkers. To overcome these limitations, we propose a number of revisions that may be considered for future versions of UD.},
  isbn = {979-8-89176-292-3}
}

@inproceedings{oh-et-al-2024-leading,
  title = {Leading Whitespaces of Language Models' Subword Vocabulary Pose a Confound for Calculating Word Probabilities},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Oh, Byung-Doh and Schuler, William},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = 2024,
  month = nov,
  pages = {3464--3472},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.202},
  urldate = {2025-09-12},
  file = {/Users/coleman/Zotero/storage/NW9PPHSQ/Oh and Schuler - 2024 - Leading Whitespaces of Language Models' Subword Vocabulary Pose a Confound for Calculating Word Prob.pdf}
}

@article{ohala-et-al-1977-story,
  title = {The {{Story}} of [w]: {{An Exercise}} in the {{Phonetic Explanation}} for {{Sound Patterns}}},
  shorttitle = {The {{Story}} of [w]},
  author = {Ohala, John J. and Lorentz, James},
  year = 1977,
  journal = {Proceedings of the Annual Meeting of the Berkeley Linguistics Society},
  volume = {3},
  number = {3},
  url = {https://escholarship.org/uc/item/5wj32788},
  urldate = {2025-09-26},
  abstract = {Author(s): Ohala, John J; Lorentz, James},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/865VGIJL/Ohala and Lorentz - 1977 - The Story of [w] An Exercise in the Phonetic Explanation for Sound Patterns.pdf}
}

@article{oshima-et-al-2019-gradability,
  title = {Gradability, Scale Structure, and the Division of Labor between Nouns and Adjectives: {{The}} Case of {{Japanese}}},
  shorttitle = {Gradability, Scale Structure, and the Division of Labor between Nouns and Adjectives},
  author = {Oshima, David Y. and Akita, Kimi and Sano, Shin-ichiro},
  year = 2019,
  month = mar,
  journal = {Glossa: a journal of general linguistics},
  volume = {4},
  number = {1},
  publisher = {Open Library of Humanities},
  issn = {2397-1835},
  doi = {10.5334/gjgl.737},
  urldate = {2025-09-12},
  abstract = {Japanese has three major ``adjective-like'' word classes, which roughly correspond to ``adjectives'', ``adjectival nouns'', and ``precopular nouns'' in Martin's (1975) A Reference Grammar of Japanese. This work explores how the three classes contrast semantically, paying special attention to the notion of gradability. Their scale-structural characteristics, in comparison with the English adjective class, will be examined, aiming to contribute to a better understanding of how languages may contrast in terms of (i) how different kinds of stative predicates divide the labor in encoding different kinds of state concepts, and (ii) how the niche of their noun class (as a major part-of-speech) is delimited. The major findings include (i) that ``adjectives'' and ``adjectival nouns'' have a strong tendency to encode relative gradable concepts, (ii) that ``precopular nouns'' tend to be nongradable, and (iii) none of the three Japanese classes is closely tied to the feature of absolute gradability.},
  copyright = {Copyright: \copyright{} 2019 The Author(s). This is an open-access article distributed under the terms of the Creative Commons Attribution 4.0 International License (CC-BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. See http://creativecommons.org/licenses/by/4.0/.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/P922DLLE/Oshima et al. - 2019 - Gradability, scale structure, and the division of labor between nouns and adjectives The case of Ja.pdf}
}

@inproceedings{ostling-2015-word,
  title = {Word {{Order Typology}} through {{Multilingual Word Alignment}}},
  booktitle = {Proceedings of the 53rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 7th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 2: {{Short Papers}})},
  author = {{\"O}stling, Robert},
  editor = {Zong, Chengqing and Strube, Michael},
  year = 2015,
  month = jul,
  pages = {205--211},
  publisher = {Association for Computational Linguistics},
  address = {Beijing, China},
  doi = {10.3115/v1/P15-2034},
  urldate = {2025-05-14},
  file = {/Users/coleman/Zotero/storage/2PQZQG4Z/stling_2015_Word Order Typology through Multilingual Word Alignment.pdf}
}

@inproceedings{palmer-2025-keynote,
  title = {Keynote},
  author = {Palmer, Alexis},
  year = 2025,
  month = may,
  address = {The Fifth Workshop on NLP for Indigenous Languages of  the Americas.}
}

@misc{papadimitriou-et-al-2021-deep,
  title = {Deep Subjecthood: {{Higher-order}} Grammatical Features in Multilingual {{BERT}}},
  shorttitle = {Deep {{Subjecthood}}},
  author = {Papadimitriou, Isabel and Chi, Ethan A. and Futrell, Richard and Mahowald, Kyle},
  year = 2021,
  month = jan,
  number = {arXiv:2101.11043},
  eprint = {2101.11043},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2101.11043},
  urldate = {2025-10-07},
  abstract = {We investigate how Multilingual BERT (mBERT) encodes grammar by examining how the high-order grammatical feature of morphosyntactic alignment (how different languages define what counts as a "subject") is manifested across the embedding spaces of different languages. To understand if and how morphosyntactic alignment affects contextual embedding spaces, we train classifiers to recover the subjecthood of mBERT embeddings in transitive sentences (which do not contain overt information about morphosyntactic alignment) and then evaluate them zero-shot on intransitive sentences (where subjecthood classification depends on alignment), within and across languages. We find that the resulting classifier distributions reflect the morphosyntactic alignment of their training languages. Our results demonstrate that mBERT representations are influenced by high-level grammatical features that are not manifested in any one input sentence, and that this is robust across languages. Further examining the characteristics that our classifiers rely on, we find that features such as passive voice, animacy and case strongly correlate with classification decisions, suggesting that mBERT does not encode subjecthood purely syntactically, but that subjecthood embedding is continuous and dependent on semantic and discourse factors, as is proposed in much of the functional linguistics literature. Together, these results provide insight into how grammatical features manifest in contextual embedding spaces, at a level of abstraction not covered by previous work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/coleman/Zotero/storage/R762Y2A4/Papadimitriou et al. - 2021 - Deep Subjecthood Higher-Order Grammatical Features in Multilingual BERT.pdf;/Users/coleman/Zotero/storage/JURFASQD/2101.html}
}

@inproceedings{papadimitriou-et-al-2023-multilingual,
  title = {Multilingual {{BERT}} Has an Accent: {{Evaluating English}} Influences on Fluency in Multilingual Models},
  shorttitle = {Multilingual {{BERT}} Has an Accent},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EACL}} 2023},
  author = {Papadimitriou, Isabel and Lopez, Kezia and Jurafsky, Dan},
  editor = {Vlachos, Andreas and Augenstein, Isabelle},
  year = 2023,
  month = may,
  pages = {1194--1200},
  publisher = {Association for Computational Linguistics},
  address = {Dubrovnik, Croatia},
  doi = {10.18653/v1/2023.findings-eacl.89},
  urldate = {2025-11-08},
  abstract = {While multilingual language models can improve NLP performance on low-resource languages by leveraging higher-resource languages, they also reduce average performance on all languages (the `curse of multilinguality'). Here we show another problem with multilingual models: grammatical structures in higher-resource languages bleed into lower-resource languages, a phenomenon we call grammatical structure bias. We show this bias via a novel method for comparing the fluency of multilingual models to the fluency of monolingual Spanish and Greek models: testing their preference for two carefully-chosen variable grammatical structures (optional pronoun-drop in Spanish and optional Subject-Verb ordering in Greek). We find that multilingual BERT is biased toward the English-like setting (explicit pronouns and Subject-Verb-Object ordering) as compared to our monolingual control language model. With our case studies, we hope to bring to light the fine-grained ways in which multilingual models can be biased, and encourage more linguistically-aware fluency evaluation.},
  file = {/Users/coleman/Zotero/storage/4NHEEHLF/Papadimitriou et al. - 2023 - Multilingual BERT has an accent Evaluating English influences on fluency in multilingual models.pdf}
}

@inproceedings{pawley-2006-where,
  title = {Where Have All the Verbs Gone? {{Remarks}} on the Organisation of Languages with Small, Closed Verb Classes},
  booktitle = {11th Biennial Rice University Linguistics Symposium},
  author = {Pawley, Andrew K.},
  year = 2006,
  publisher = {Rice University},
  address = {Houston, Texas, USA},
  url = {http://www.ruf.rice.edu/~lingsymp/Pawley_paper.pdf}
}

@inproceedings{pennington-et-al-2014-glove,
  title = {{{GloVe}}: {{Global}} Vectors for Word Representation},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  year = 2014,
  month = oct,
  pages = {1532--1543},
  publisher = {Association for Computational Linguistics},
  address = {Doha, Qatar},
  doi = {10.3115/v1/D14-1162}
}

@article{perlmutter-1988-split,
  title = {The Split Morphology Hypothesis: {{Evidence}} from {{Yiddish}}},
  author = {Perlmutter, David},
  year = 1988,
  journal = {Theoretical morphology},
  pages = {79--100},
  publisher = {Academic Press San Diego, CA}
}

@incollection{peyraube-et-al-2023-east,
  title = {East {{Asian Early Linguistic Traditions}}: {{China}}; {{Korea}} and {{Japan}}},
  shorttitle = {East {{Asian Early Linguistic Traditions}}},
  booktitle = {The {{Cambridge History}} of {{Linguistics}}},
  author = {Peyraube, Alain and Chappell, Hilary and Vovin, Alexander},
  editor = {Joseph, John E. and Waugh, Linda R. and {Monville-Burston}, Monique},
  year = 2023,
  pages = {54--76},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/9780511842788.006},
  urldate = {2025-11-08},
  abstract = {Part 1 (by Peyraube and Chappell): In China reflections on language date back to the fifth century BCE. Interest in language was first metaphysical in nature, focusing on the correspondence between reality and names. Later there were studies about phonology, lexical dialectology, and the prosody of rhymes. Chinese scholars concentrated on the writing system, and the classification and semantic value of the Chinese characters (dictionaries of rhymes). From the twelfth to the thirteenth century CE, grammatical analyses developed, dealing with ``empty particles.'' Later, philological work compared vernacular registers and older forms of the language (Classical Chinese). Remarkably, there was no interest in China regarding works on the Chinese language composed by western missionaries and sinologists from the sixteenth century onward. These grammars, which describe various dialects and registers, first followed European models. Only in the nineteenth century did better reasoned descriptions of Chinese appear.(by Vovin): This is concerned with the adaptation of the Chinese script to Korean and then of the Korean script to Japanese; it also considers the development of an alphabetic writing system in Korea. The philological Japanese tradition (lexicography and commentaries on Old Japanese texts) is also discussed.},
  isbn = {978-0-521-84990-6},
  keywords = {Adaptation of Chinese script in Korea and Japan,Chinese characters,Chinese grammars by missionaries and sinologists,Chinese lexicography,Chinese philological tradition,Chinese rhetoric and stylistics,Development of Korean and Japanese scripts,Grammatical ("empty") particles,Japanese philological tradition},
  file = {/Users/coleman/Zotero/storage/NFERRB24/07C43FC57D8211E50FDA851ECEB3A0F4.html}
}

@article{piantadosi-et-al-2011-word,
  title = {Word Lengths Are Optimized for Efficient Communication},
  author = {Piantadosi, Steven T. and Tily, Harry and Gibson, Edward},
  year = 2011,
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {108},
  number = {9},
  pages = {3526--3529},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1012551108},
  urldate = {2025-10-27},
  abstract = {We demonstrate a substantial improvement on one of the most celebrated empirical laws in the study of language, Zipf's 75-y-old theory that word le...},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/LT472T92/Piantadosi et al. - 2011 - Word lengths are optimized for efficient communication.pdf}
}

@misc{piantadosi-et-al-2022-meaning,
  title = {Meaning without Reference in Large Language Models},
  author = {Piantadosi, Steven T. and Hill, Felix},
  year = 2022,
  month = aug,
  number = {arXiv:2208.02957},
  eprint = {2208.02957},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.02957},
  urldate = {2025-09-06},
  abstract = {The widespread success of large language models (LLMs) has been met with skepticism that they possess anything like human concepts or meanings. Contrary to claims that LLMs possess no meaning whatsoever, we argue that they likely capture important aspects of meaning, and moreover work in a way that approximates a compelling account of human cognition in which meaning arises from conceptual role. Because conceptual role is defined by the relationships between internal representational states, meaning cannot be determined from a model's architecture, training data, or objective function, but only by examination of how its internal states relate to each other. This approach may clarify why and how LLMs are so successful and suggest how they can be made more human-like.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/coleman/Zotero/storage/QHGEGX48/Piantadosi and Hill - 2022 - Meaning without reference in large language models.pdf;/Users/coleman/Zotero/storage/TWJ69IV9/2208.html}
}

@inproceedings{pimentel-et-al-2024-how,
  title = {How to Compute the Probability of a Word},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Pimentel, Tiago and Meister, Clara},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = 2024,
  month = nov,
  pages = {18358--18375},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.1020},
  urldate = {2025-09-12},
  abstract = {Language models (LMs) estimate a probability distribution over strings in a natural language; these distributions are crucial for computing perplexity and surprisal in linguistics research. While we are usually concerned with measuring these values for words, most LMs operate over subwords. Despite seemingly straightforward, accurately computing probabilities over one unit given probabilities over the other requires care. Indeed, we show here that many recent linguistic studies have been incorrectly computing these values. This paper derives the correct methods for computing word probabilities, highlighting issues when relying on language models that use beginning-of-word (bow)-marking tokenisers, e.g., the GPT family. Empirically, we show that correcting the widespread bug in probability computations affects measured outcomes in sentence comprehension and lexical optimisation analyses.},
  file = {/Users/coleman/Zotero/storage/AGIZMWQK/Pimentel and Meister - 2024 - How to Compute the Probability of a Word.pdf}
}

@manual{pinheiro-et-al-2025-nlme,
  type = {Manual},
  title = {Nlme: {{Linear}} and Nonlinear Mixed Effects Models},
  author = {Pinheiro, Jos{\'e} and Bates, Douglas and {R Core Team}},
  year = 2025,
  doi = {10.32614/CRAN.package.nlme}
}

@incollection{plank-1994-inflection,
  title = {Inflection and Derivation},
  booktitle = {The Encyclopedia of Language and Linguistics},
  author = {Plank, Frans},
  year = 1994,
  pages = {1671--1679},
  publisher = {{Elsevier Science and Technology}},
  address = {Amsterdam, Netherlands}
}

@article{plank-2017-extent,
  title = {Extent and Limits of Linguistic Diversity as the Remit of Typology -- but through Constraints on What Is Diversity Limited?},
  author = {Plank, Frans},
  year = 2017,
  journal = {Linguistic Typology},
  volume = {21},
  number = {2017},
  pages = {43--68},
  doi = {doi:10.1515/lingty-2017-1004},
  urldate = {2024-05-14}
}

@inproceedings{ploeger-et-al-2024-what,
  title = {What Is ``{{Typological Diversity}}'' in {{NLP}}?},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Ploeger, Esther and Poelman, Wessel and {\noopsort{lhoneux}}{de Lhoneux}, Miryam and Bjerva, Johannes},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = 2024,
  month = nov,
  pages = {5681--5700},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.326},
  urldate = {2025-11-07},
  abstract = {The NLP research community has devoted increased attention to languages beyond English, resulting in considerable improvements for multilingual NLP. However, these improvements only apply to a small subset of the world's languages. An increasing number of papers aspires to enhance generalizable multilingual performance across languages. To this end, linguistic typology is commonly used to motivate language selection, on the basis that a broad typological sample ought to imply generalization across a broad range of languages. These selections are often described as being typologically diverse. In this meta-analysis, we systematically investigate NLP research that includes claims regarding typological diversity. We find there are no set definitions or criteria for such claims. We introduce metrics to approximate the diversity of resulting language samples along several axes and find that the results vary considerably across papers. Crucially, we show that skewed language selection can lead to overestimated multilingual performance. We recommend future work to include an operationalization of typological diversity that empirically justifies the diversity of language samples. To help facilitate this, we release the code for our diversity measures.},
  file = {/Users/coleman/Zotero/storage/IG7UADBV/Ploeger et al. - 2024 - What is Typological Diversity in NLP.pdf}
}

@article{ploeger-et-al-2025-principled,
  title = {A {{Principled Framework}} for {{Evaluating}} on {{Typologically Diverse Languages}}},
  author = {Ploeger, Esther and Poelman, Wessel and {H{\o}eg-Petersen}, Andreas Holck and Schlichtkrull, Anders and {\noopsort{lhoneux}}{de Lhoneux}, Miryam and Bjerva, Johannes},
  year = 2025,
  month = oct,
  journal = {Computational Linguistics},
  pages = {1--36},
  issn = {0891-2017},
  doi = {10.1162/COLI.a.577},
  urldate = {2025-11-07},
  abstract = {Beyond individual languages, multilingual natural language processing (NLP) research increasingly aims to develop models that perform well across languages generally. However, evaluating these systems on all the world's languages is practically infeasible. To attain generalizability, representative language sampling is essential. Previous work argues that generalizable multilingual evaluation sets should contain languages with diverse typological properties. However, `typologically diverse' language samples have been found to vary considerably in this regard, and popular sampling methods are flawed and inconsistent. We present a language sampling framework for selecting highly typologically diverse languages given a sampling frame, informed by language typology. We compare sampling methods with a range of metrics and find that our systematic methods consistently retrieve more typologically diverse language selections than previous methods in NLP. Moreover, we provide evidence that this affects generalizability in multilingual model evaluation, emphasizing the importance of diverse language sampling in NLP evaluation.},
  file = {/Users/coleman/Zotero/storage/7PEKTLLD/Ploeger et al. - 2025 - A Principled Framework for Evaluating on Typologically Diverse Languages.pdf;/Users/coleman/Zotero/storage/SIFSV4KW/COLI.a.html}
}

@article{poole-2000-nonparametric,
  title = {Nonparametric {{Unfolding}} of {{Binary Choice Data}}},
  author = {Poole, Keith T.},
  year = 2000,
  journal = {Political Analysis},
  volume = {8},
  number = {3},
  pages = {211--237},
  issn = {1047-1987, 1476-4989},
  doi = {10.1093/oxfordjournals.pan.a029814},
  urldate = {2025-11-09},
  abstract = {This paper shows a general nonparametric unfolding technique for maximizing the correct classification of binary choice or two-category data. The motivation for and the primary focus of the unfolding technique are parliamentary roll call voting data. However, the procedures that implement the unfolding also can be applied to the problem of unfolding rank order data as well as analyzing a data set that would normally be the subject of a probit, logit, or linear probability analysis. One aspect of the scaling method greatly improves Manski's ``maximum score estimator'' technique for estimating limited dependent variable models. To unfold binary choice data two subproblems must be solved. First, given a set of chooser or legislator points, a cutting plane must be found such that it divides the legislators/choosers into two sets that reproduce the actual choices as closely as possible. Second, given a set of cutting planes for the binary choices, a point for each chooser or legislator must be found which reproduces the actual choices as closely as possible. Solutions for these two problems are shown in this paper. Monte Carlo tests of the procedure show it to be highly accurate in the presence of voting error and missing data.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/TVMF2KKT/Poole - 2000 - Nonparametric Unfolding of Binary Choice Data.pdf}
}

@book{popper-1934-logic,
  title = {The {{Logic}} of {{Scientific Discovery}}},
  author = {Popper, Karl Raimund},
  editor = {Group, Hutchinson Publishing},
  year = 1934,
  publisher = {Routledge},
  address = {New York},
  file = {/Users/coleman/Zotero/storage/K76IFJ4X/POPTLO-2.html}
}

@misc{prasertsom-et-al-2025-recursive,
  title = {Recursive Numeral Systems Are Highly Regular and Easy to Process},
  author = {Prasertsom, Ponrawee and Silvi, Andrea and Culbertson, Jennifer and Johansson, Moa and Dubhashi, Devdatt and Smith, Kenny},
  year = 2025,
  month = oct,
  number = {arXiv:2510.27049},
  eprint = {2510.27049},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2510.27049},
  urldate = {2025-11-09},
  abstract = {Previous work has argued that recursive numeral systems optimise the trade-off between lexicon size and average morphosyntatic complexity (Deni\textbackslash 'c and Szymanik, 2024). However, showing that only natural-language-like systems optimise this tradeoff has proven elusive, and the existing solution has relied on ad-hoc constraints to rule out unnatural systems (Yang and Regier, 2025). Here, we argue that this issue arises because the proposed trade-off has neglected regularity, a crucial aspect of complexity central to human grammars in general. Drawing on the Minimum Description Length (MDL) approach, we propose that recursive numeral systems are better viewed as efficient with regard to their regularity and processing complexity. We show that our MDL-based measures of regularity and processing complexity better capture the key differences between attested, natural systems and unattested but possible ones, including "optimal" recursive numeral systems from previous work, and that the ad-hoc constraints from previous literature naturally follow from regularity. Our approach highlights the need to incorporate regularity across sets of forms in studies that attempt to measure and explain optimality in language.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Formal Languages and Automata Theory},
  file = {/Users/coleman/Zotero/storage/NJR3J784/Prasertsom et al. - 2025 - Recursive numeral systems are highly regular and easy to process.pdf;/Users/coleman/Zotero/storage/SBNZ5IJH/2510.html}
}

@article{pulvermuller-et-al-1995-electrocortical,
  title = {Electrocortical Distinction of Vocabulary Types},
  author = {Pulverm{\"u}ller, Friedemann and Lutzenberger, Werner and Birbaumer, Niels},
  year = 1995,
  month = may,
  journal = {Electroencephalography and Clinical Neurophysiology},
  volume = {94},
  number = {5},
  pages = {357--370},
  issn = {0013-4694},
  doi = {10.1016/0013-4694(94)00291-R},
  urldate = {2025-11-07},
  abstract = {Psycholinguistic theories propose that words of the 2 major vocabulary classes, content (open-class) and function (closed-class) words, are computationally distinct and have different neuronal generators. This predicts distinct EEG patterns elicited by words of these 2 classes. To test this prediction, content and function words, together with matched pseudowords, were presented in a lexical decision task (where subjects had to decide whether stimuli were meaningful words or not). Evoked potentials were recorded from 17 electrodes 12 of which were located in close vicinity of the perisylvian cortices. Already 160 msec post stimulus onset, substantial differences in activity patterns distinguish the 2 vocabulary classes. A hemisphere by word class interaction revealed interhemispheric differences for function words but not for content words. Potentials evoked by function words were more negative over the left hemisphere compared to the right. These results evidence that brain mechanisms underlying function and content word processing are different. The following explanation of the data is proposed: content words correspond to neuronal assemblies equally distributed over both hemispheres, while assemblies corresponding to function words are strongly lateralized to the left hemisphere and primarily located in the perisylvian region.},
  keywords = {Cell assembly,Content/function word,Language,Lexical decision,N2,N4,Open-class/closed-class word,P3},
  file = {/Users/coleman/Zotero/storage/3RX2DWUJ/Pulvermller et al. - 1995 - Electrocortical distinction of vocabulary types.pdf;/Users/coleman/Zotero/storage/VSPACASJ/001346949400291R.html}
}

@inproceedings{qi-et-al-2020-stanza,
  title = {Stanza: {{A Python Natural Language Processing Toolkit}} for {{Many Human Languages}}},
  shorttitle = {Stanza},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{System Demonstrations}}},
  author = {Qi, Peng and Zhang, Yuhao and Zhang, Yuhui and Bolton, Jason and Manning, Christopher D.},
  editor = {Celikyilmaz, Asli and Wen, Tsung-Hsien},
  year = 2020,
  month = jul,
  pages = {101--108},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-demos.14},
  urldate = {2024-10-07},
  abstract = {We introduce Stanza, an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, and named entity recognition. We have trained Stanza on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested. Additionally, Stanza includes a native Python interface to the widely used Java Stanford CoreNLP software, which further extends its functionality to cover other tasks such as coreference resolution and relation extraction. Source code, documentation, and pretrained models for 66 languages are available at https://stanfordnlp.github.io/stanza/.},
  file = {/Users/coleman/Zotero/storage/EN5LWR42/Qi et al_2020_Stanza.pdf}
}

@inproceedings{qin-et-al-2025-visionandlanguage,
  title = {Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does Not Fundamentally Alter It},
  booktitle = {The {{Thirty-ninth Annual Conference}} on {{Neural Information Processing Systems}}},
  author = {Qin, Yulu and Varghese, Dheeraj and Lindstr{\"o}m, Adam Dahlgren and Donatelli, Lucia and Misra, Kanishka and Kim, Najoung},
  year = 2025,
  month = oct,
  url = {https://openreview.net/forum?id=KXmDTGKwhy},
  urldate = {2025-11-01},
  abstract = {Does vision-and-language (VL) training change the linguistic representations of language models in meaningful ways? In terms of downstream task performance on text-only tasks, most results in the literature have shown marginal differences. In this work, we start from the hypothesis that the domain in which VL training could have a significant effect is lexical-conceptual knowledge, in particular its taxonomic organization. Through comparing minimal pairs of text-only LMs and their VL-trained counterparts, we first show that the VL models often outperform their text-only counterparts on a text-only question-answering task that requires taxonomic understanding of concepts mentioned in the questions. Using an array of targeted behavioral and representational analyses, we show that the LMs and VLMs do not differ significantly in terms of their taxonomic knowledge itself, but they differ in how they represent questions that contain concepts in a taxonomic relation vs. a non-taxonomic relation. This implies that the taxonomic knowledge itself does not change substantially through additional VL training, but VL training does improve the deployment of this knowledge in the context of a specific task, even when the presentation of the task is purely linguistic.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/BUEZSMDM/Qin et al. - 2025 - Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does Not Fundamentally Alter It.pdf}
}

@article{radanovic-et-al-2016-quantifying,
  title = {Quantifying Semantic Animacy: {{How}} Much Are Words Alive?},
  shorttitle = {Quantifying Semantic Animacy},
  author = {Radanovi{\'c}, Jelena and Westbury, Chris and Milin, Petar},
  year = 2016,
  month = nov,
  journal = {Applied Psycholinguistics},
  volume = {37},
  number = {6},
  pages = {1477--1499},
  issn = {0142-7164, 1469-1817},
  doi = {10.1017/S0142716416000096},
  urldate = {2025-11-09},
  abstract = {The main goal of this study, which comprised two experimental tasks and three normative studies, was to describe the underlying distribution of semantic animacy, with the focus on Serbian and English. Animacy was measured using three normative techniques. The cognitive effects of obtained measures were tested in two experiments conducted in both Serbian and English: a visual lexical decision task and a semantic categorization task. Results suggest that semantic animacy is a graded property. A high correlation between Serbian and English measures suggests that semantic animacy might be language independent, most likely because of its biological grounding. As for its behavioral correlates, animacy does not affect lexical decision times but it does codetermine the categorization speed: the category decision gradually slows as a function of the degree of animacy. These results were consistent across two languages under research scrutiny. We thus conclude that animacy is a continuous aspect of meaning.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/MC8IKKFD/Radanovi et al. - 2016 - Quantifying semantic animacy How much are words alive.pdf}
}

@inproceedings{radford-et-al-2021-learning,
  title = {Learning Transferable Visual Models from Natural Language Supervision},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}, {{ICML}} 2021, 18-24 {{July}} 2021, {{Virtual Event}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  editor = {Meila, Marina and Zhang, Tong},
  year = 2021,
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {139},
  pages = {8748--8763},
  publisher = {PMLR},
  url = {http://proceedings.mlr.press/v139/radford21a.html},
  urldate = {2025-10-31}
}

@article{rakhilina-et-al-2022-lexical,
  title = {Lexical Typology and Semantic Maps: {{Perspectives}} and Challenges},
  shorttitle = {Lexical Typology and Semantic Maps},
  author = {Rakhilina, Ekaterina and Ryzhova, Daria and Badryzlova, Yulia},
  year = 2022,
  month = jun,
  journal = {Zeitschrift f\"ur Sprachwissenschaft},
  volume = {41},
  number = {1},
  pages = {231--262},
  publisher = {De Gruyter},
  issn = {1613-3706},
  doi = {10.1515/zfs-2021-2046},
  urldate = {2025-10-07},
  abstract = {The paper outlines the basics of data collection, analysis and visualization under the frame-based approach to lexical typology and illustrates its methodology using the data of cross-linguistic research on verbs of falling. The framework reveals several challenges to semantic map modelling that usually escape researchers' attention. These are: (1) principles of establishing lexical comparative concepts; (2) the effective ways of visualization for the opposition between direct and figurative meanings of lexical items; (3) the problem of the borderlines between semantic fields, which seem to be very subtle. These problems are discussed in detail in the paper, as well as possible theoretical decisions and semantic modelling techniques that could overcome these bottlenecks.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  keywords = {frame-based approach,lexical typology,metaphor,metonymy,semantic continuity,verbs of falling},
  file = {/Users/coleman/Zotero/storage/I7GJJBIZ/Rakhilina et al. - 2022 - Lexical typology and semantic maps Perspectives and challenges.pdf}
}

@phdthesis{rauhut-2023-quantitative,
  title = {Quantitative {{Aspects}} of the {{Word Class Continuum}} in {{English}}},
  author = {Rauhut, Alexander},
  year = 2023,
  url = {https://www.proquest.com/openview/be61da34685c26eaafa678412407be0e/1.pdf?pq-origsite=gscholar&cbl=2026366&diss=y},
  urldate = {2025-11-09},
  abstract = {Explore millions of resources from scholarly journals, books, newspapers, videos and more, on the ProQuest Platform.},
  langid = {english},
  school = {Feie Universit\"at Berlin},
  file = {/Users/coleman/Zotero/storage/4QT9A2C3/1.html}
}

@article{regier-et-al-2007-color,
  title = {Color Naming Reflects Optimal Partitions of Color Space},
  author = {Regier, Terry and Kay, Paul and Khetarpal, Naveen},
  year = 2007,
  month = jan,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {104},
  number = {4},
  pages = {1436--1441},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.0610341104},
  urldate = {2025-10-07},
  abstract = {The nature of color categories in the world's languages is contested. One major view holds that color categories are organized around universal focal colors, whereas an opposing view holds instead that categories are defined at their boundaries by linguistic convention. Both of these standardly opposed views are challenged by existing data. Here, we argue for a third view based on a proposal by Jameson and D'Andrade [Jameson KA, D'Andrade RG (1997) in Color Categories in Thought and Language, eds Hardin CL, Maffi L (Cambridge Univ Press, Cambridge, U.K.), pp 295--319]: that color naming across languages reflects optimal or near-optimal divisions of an irregularly shaped perceptual color space. We formalize this idea, test it against color-naming data from a broad range of languages and show that it accounts for universal tendencies in color naming while also accommodating some observed cross-language variation.},
  file = {/Users/coleman/Zotero/storage/WHUL83RE/Regier et al. - 2007 - Color naming reflects optimal partitions of color space.pdf}
}

@article{regier-et-al-2013-inferring,
  title = {Inferring Semantic Maps},
  author = {Regier, Terry and Khetarpal, Naveen and Majid, Asifa},
  year = 2013,
  month = jun,
  journal = {Linguistic Typology},
  volume = {17},
  number = {1},
  pages = {89--105},
  publisher = {De Gruyter Mouton},
  issn = {1613-415X},
  doi = {10.1515/lity-2013-0003},
  urldate = {2025-11-04},
  abstract = {Semantic maps are a means of representing universal structure underlying semantic variation. However, no algorithm has existed for inferring a graph-based semantic map from cross-language data. Here, we note that this open problem is formally identical to the known problem of inferring a social network from disease outbreaks. From this identity it follows that semantic map inference is computationally intractable, but that an efficient approximation algorithm for it exists. We demonstrate that this algorithm produces sensible semantic maps from two existing bodies of data. We conclude that universal semantic graph structure can be automatically approximated from cross-language semantic data.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/JEDBPWK8/Regier et al. - 2013 - Inferring semantic maps.pdf}
}

@article{richards-2009-nouns,
  title = {Nouns, Verbs, and Hidden Structure in {{Tagalog}}},
  author = {Richards, Norvin},
  year = 2009,
  month = jul,
  journal = {Theoretical Linguistics},
  volume = {35},
  number = {1},
  pages = {139--152},
  issn = {0301-4428, 1613-4060},
  doi = {10.1515/THLI.2009.008},
  urldate = {2024-10-07},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/B884EHTN/Richards_2009_Nouns, verbs, and hidden structure in Tagalog.pdf}
}

@article{rizzi-2004-inventories,
  title = {From {{Inventories}} to {{Computations}}: {{Open}}/{{Closed Class Items}} and {{Substantive}}/{{Functional Heads}}},
  shorttitle = {From {{Inventories}} to {{Computations}}},
  author = {Rizzi, Luigi},
  year = 2004,
  journal = {Dialectica},
  volume = {58},
  number = {3},
  eprint = {42970849},
  eprinttype = {jstor},
  pages = {437--451},
  publisher = {Wiley},
  issn = {0012-2017},
  url = {https://www.jstor.org/stable/42970849},
  urldate = {2025-10-07},
  abstract = {The distinction between open and closed class items represents a fundamental bifurcation in the mental lexicon. It proved useful to express certain basic generalisations in linguistics and in the study of language acquisition and language pathology. The distinction is too rough tough: it must be refined by paying attention to the computational properties of the two classes and their division of labor in the generation of complex expressions. It will be shown how the distinction is expressed within current linguistic models of the principles and parameters/minimalist type.},
  file = {/Users/coleman/Zotero/storage/URL8FGLK/Rizzi - 2004 - From Inventories to Computations OpenClosed Class Items and SubstantiveFunctional Heads.pdf}
}

@article{rofes-et-al-2018-imageability,
  title = {Imageability Ratings across Languages},
  author = {Rofes, Adri{\`a} and Zakari{\'a}s, Lilla and Ceder, Klaudia and Lind, Marianne and Johansson, Monica Blom and {\noopsort{aguiar}}{de Aguiar}, V{\^a}nia and Bjeki{\'c}, Jovana and Fyndanis, Valantis and Gavarr{\'o}, Anna and Simonsen, Hanne Gram and Sacrist{\'a}n, Carlos Hern{\'a}ndez and Kambanaros, Maria and Kraljevi{\'c}, Jelena Kuva{\v c} and {Mart{\'i}nez-Ferreiro}, Silvia and Mavis, {\.I}lknur and Orellana, Carolina M{\'e}ndez and S{\"o}r, Ingrid and Luk{\'a}cs, {\'A}gnes and Tun{\c c}er, M{\"u}ge and Vuksanovi{\'c}, Jasmina and Ibarrola, Amaia Munarriz and Pourquie, Marie and Varlokosta, Spyridoula and Howard, David},
  year = 2018,
  month = jun,
  journal = {Behavior Research Methods},
  volume = {50},
  number = {3},
  pages = {1187--1197},
  issn = {1554-3528},
  doi = {10.3758/s13428-017-0936-0},
  abstract = {Imageability is a psycholinguistic variable that indicates how well a word gives rise to a mental image or sensory experience. Imageability ratings are used extensively in psycholinguistic, neuropsychological, and aphasiological studies. However, little formal knowledge exists about whether and how these ratings are associated between and within languages. Fifteen imageability databases were cross-correlated using nonparametric statistics. Some of these corresponded to unpublished data collected within a European research network---the Collaboration of Aphasia Trialists (COST IS1208). All but four of the correlations were significant. The average strength of the correlations (rho = .68) and the variance explained (R2 = 46\%) were moderate. This implies that factors other than imageability may explain 54\% of the results. Imageability ratings often correlate across languages. Different possibly interacting factors may explain the moderate strength and variance explained in the correlations: (1) linguistic and cultural factors; (2) intrinsic differences between the databases; (3) range effects; (4) small numbers of words in each database, equivalent words, and participants; and (5) mean age of the participants. The results suggest that imageability ratings may be used cross-linguistically. However, further understanding of the factors explaining the variance in the correlations will be needed before research and practical recommendations can be made.}
}

@mastersthesis{rogers-2016-illustrating,
  title = {Illustrating the Prototype Structures of Parts of Speech: {{A}} Multidimensional Scaling Analysis},
  author = {Rogers, Phillip},
  year = 2016,
  address = {Albuquerque, New Mexico, USA},
  url = {https://digitalrepository.unm.edu/ling_etds/28/},
  langid = {english},
  school = {University of New Mexico},
  file = {/Users/coleman/Zotero/storage/HJW4JBUB/Rogers - Illustrating the prototype structures of parts of speech A multidimensional scaling analysis.pdf}
}

@inproceedings{rosa-et-al-2019-attempting,
  title = {Attempting to Separate Inflection and Derivation Using Vector Space Representations},
  booktitle = {Proceedings of the Second International Workshop on Resources and Tools for Derivational Morphology},
  author = {Rosa, Rudolf and {\v Z}abokrtsk{\'y}, Zden{\v e}k},
  year = 2019,
  month = sep,
  pages = {61--70},
  publisher = {{Charles University, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics}},
  address = {Prague, Czechia},
  url = {https://aclanthology.org/W19-8508}
}

@article{rosa-et-al-2019-unsupervised,
  title = {Unsupervised Lemmatization as Embeddings-Based Word Clustering},
  author = {Rosa, Rudolf and {\v Z}abokrtsk{\'y}, Zden{\v e}k},
  year = 2019,
  journal = {CoRR},
  volume = {abs/1908.08528},
  eprint = {1908.08528},
  url = {http://arxiv.org/abs/1908.08528},
  archiveprefix = {arXiv},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  timestamp = {Sat, 23 Jan 2021 01:11:13 +0100}
}

@article{rosch-1973-natural,
  title = {Natural Categories},
  author = {Rosch, Eleanor H.},
  year = 1973,
  month = may,
  journal = {Cognitive Psychology},
  volume = {4},
  number = {3},
  pages = {328--350},
  issn = {0010-0285},
  doi = {10.1016/0010-0285(73)90017-0},
  urldate = {2025-11-09},
  abstract = {The hypothesis of the study was that the domains of color and form are structured into nonarbitrary, semantic categories which develop around perceptually salient ``natural prototypes.'' Categories which reflected such an organization (where the presumed natural prototypes were central tendencies of the categories) and categories which violated the organization (natural prototypes peripheral) were taught to a total of 162 members of a Stone Age culture which did not initially have hue or geometric-form concepts. In both domains, the presumed ``natural'' categories were consistently easier to learn than the ``distorted'' categories. Even when not central, natural prototype stimuli tended to be more rapidly learned and more often chosen as the most typical example of the category than were other stimuli. Implications for general differences between natural categories and the artificial categories of concept formation research were discussed.},
  file = {/Users/coleman/Zotero/storage/QAWEPVFW/Rosch - 1973 - Natural categories.pdf;/Users/coleman/Zotero/storage/9CII3CZN/0010028573900170.html}
}

@inproceedings{ross-1972-category,
  title = {The Category Squish: {{Endstation Hauptwort}}},
  booktitle = {Proceedings of the Eighth Regional Meeting of the Chicago Linguistic Society},
  author = {Ross, John R.},
  editor = {Peranteau, Paul M. and Levi, Judith N. and Phares, Gloria C.},
  year = 1972,
  pages = {316--328},
  publisher = {Chicago Linguistic Society, University of Chicago},
  address = {Chicago, Illinois, USA}
}

@inproceedings{rozner-et-al-2025-constructions,
  title = {Constructions Are Revealed in Word Distributions},
  booktitle = {Proceedings of the 2025 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Rozner, Joshua and Weissweiler, Leonie and Mahowald, Kyle and Shain, Cory},
  editor = {Christodoulopoulos, Christos and Chakraborty, Tanmoy and Rose, Carolyn and Peng, Violet},
  year = 2025,
  month = nov,
  pages = {2116--2138},
  publisher = {Association for Computational Linguistics},
  address = {Suzhou, China},
  url = {https://aclanthology.org/2025.emnlp-main.108/},
  urldate = {2025-11-04},
  abstract = {Construction grammar posits that constructions, or form-meaning pairings, are acquired through experience with language (the distributional learning hypothesis).But how much information about constructions does this distribution actually contain? Corpus-based analyses provide some answers, but text alone cannot answer counterfactual questions about what caused a particular word to occur.This requires computable models of the distribution over strings---namely, pretrained language models (PLMs).Here, we treat a RoBERTa model as a proxy for this distribution and hypothesize that constructions will be revealed within it as patterns of statistical affinity.We support this hypothesis experimentally: many constructions are robustly distinguished, including (i) hard cases where semantically distinct constructions are superficially similar, as well as (ii) schematic constructions, whose ``slots'' can be filled by abstract word classes.Despite this success, we also provide qualitative evidence that statistical affinity alone may be insufficient to identify all constructions from text.Thus, statistical affinity is likely an important, but partial, signal available to learners.},
  isbn = {979-8-89176-332-6},
  file = {/Users/coleman/Zotero/storage/BU9RIISK/Rozner et al. - 2025 - Constructions are Revealed in Word Distributions.pdf}
}

@book{rumelhart-et-al-1986-parallel,
  title = {Parallel {{Distributed Processing}}, {{Volume}} 1: {{Explorations}} in the {{Microstructure}} of {{Cognition}}: {{Foundations}}},
  shorttitle = {Parallel {{Distributed Processing}}, {{Volume}} 1},
  author = {Rumelhart, David E. and McClelland, James L. and PDP Research Group},
  year = 1986,
  month = jul,
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts, USA},
  doi = {10.7551/mitpress/5236.001.0001},
  urldate = {2025-11-04},
  abstract = {What makes people smarter than computers? These volumes by a pioneering neurocomputing group suggest that the answer lies in the massively parallel archite},
  isbn = {978-0-262-29140-8},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/BWJVGEJ8/Rumelhart et al. - 1986 - Parallel Distributed Processing, Volume 1 Explorations in the Microstructure of Cognition Foundati.pdf}
}

@article{saffran-et-al-1996-statistical,
  title = {Statistical Learning by 8-Month-Old Infants},
  author = {Saffran, Jenny R. and Aslin, Richard N. and Newport, Elissa L.},
  year = 1996,
  journal = {Science},
  volume = {274},
  number = {5294},
  pages = {1926--1928},
  publisher = {American Association for the Advancement of Science}
}

@misc{sanseido-2016,
  title = {{ 2022}},
  author = {Sanseido},
  year = 2016,
  journal = {Sanseido Dictionaries},
  url = {https://dictionary.sanseido-publ.co.jp/shingo/2022/},
  urldate = {2025-11-09},
  abstract = {},
  langid = {jp},
  file = {/Users/coleman/Zotero/storage/4FUVRU6L/2022.html}
}

@inproceedings{saphra-et-al-2024-first,
  title = {First Tragedy, Then Parse: {{History}} Repeats Itself in the New Era of {{Large Language Models}}},
  shorttitle = {First {{Tragedy}}, Then {{Parse}}},
  booktitle = {Proceedings of the 2024 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}} ({{Volume}} 1: {{Long Papers}})},
  author = {Saphra, Naomi and Fleisig, Eve and Cho, Kyunghyun and Lopez, Adam},
  editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
  year = 2024,
  month = jun,
  pages = {2310--2326},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-long.128},
  urldate = {2025-11-04},
  abstract = {Many NLP researchers are experiencing an existential crisis triggered by the astonishing success of ChatGPT and other systems based on large language models (LLMs). After such a disruptive change to our understanding of the field, what is left to do? Taking a historical lens, we look for guidance from the first era of LLMs, which began in 2005 with large n-gram models for machine translation (MT). We identify durable lessons from the first era, and more importantly, we identify evergreen problems where NLP researchers can continue to make meaningful contributions in areas where LLMs are ascendant. We argue that disparities in scale are transient and researchers can work to reduce them; that data, rather than hardware, is still a bottleneck for many applications; that meaningful realistic evaluation is still an open problem; and that there is still room for speculative approaches.},
  file = {/Users/coleman/Zotero/storage/CED7GKJG/Saphra et al. - 2024 - First Tragedy, then Parse History Repeats Itself in the New Era of Large Language Models.pdf}
}

@book{sapir-1921-language,
  title = {Language, an Introduction to the Study of Speech},
  author = {Sapir, Edward},
  year = 1921,
  publisher = {{Harcourt, Brace and Company}},
  address = {New York, New York, USA},
  url = {http://archive.org/details/languageanintrod00sapi},
  urldate = {2025-10-07},
  abstract = {vii, 258 p. 20 cm},
  collaborator = {{University of California Libraries}},
  langid = {english},
  lccn = {SRLF\_UCLA:LAGE-2880137},
  keywords = {Language and languages}
}

@article{schakel-et-al-2015-measuring,
  title = {Measuring Word Significance Using Distributed Representations of Words},
  author = {Schakel, Adriaan M. J. and Wilson, Benjamin J.},
  year = 2015,
  journal = {Computing Research Repository},
  volume = {arXiv:1508.02297},
  url = {http://arxiv.org/abs/1508.02297}
}

@inproceedings{schneider-et-al-2015-hierarchy,
  title = {A {{Hierarchy}} with, of, and for {{Preposition Supersenses}}},
  booktitle = {Proceedings of the 9th {{Linguistic Annotation Workshop}}},
  author = {Schneider, Nathan and Srikumar, Vivek and Hwang, Jena D. and Palmer, Martha},
  editor = {Meyers, Adam and Rehbein, Ines and Zinsmeister, Heike},
  year = 2015,
  month = jun,
  pages = {112--123},
  publisher = {Association for Computational Linguistics},
  address = {Denver, Colorado, USA},
  doi = {10.3115/v1/W15-1612},
  urldate = {2025-09-06},
  file = {/Users/coleman/Zotero/storage/PU2HCBWR/Schneider et al. - 2015 - A Hierarchy with, of, and for Preposition Supersenses.pdf}
}

@inproceedings{schneider-et-al-2018-comprehensive,
  title = {Comprehensive {{Supersense Disambiguation}} of {{English Prepositions}} and {{Possessives}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Schneider, Nathan and Hwang, Jena D. and Srikumar, Vivek and Prange, Jakob and Blodgett, Austin and Moeller, Sarah R. and Stern, Aviram and Bitan, Adi and Abend, Omri},
  editor = {Gurevych, Iryna and Miyao, Yusuke},
  year = 2018,
  month = jul,
  pages = {185--196},
  publisher = {Association for Computational Linguistics},
  address = {Melbourne, Australia},
  doi = {10.18653/v1/P18-1018},
  urldate = {2025-09-06},
  abstract = {Semantic relations are often signaled with prepositional or possessive marking---but extreme polysemy bedevils their analysis and automatic interpretation. We introduce a new annotation scheme, corpus, and task for the disambiguation of prepositions and possessives in English. Unlike previous approaches, our annotations are comprehensive with respect to types and tokens of these markers; use broadly applicable supersense classes rather than fine-grained dictionary definitions; unite prepositions and possessives under the same class inventory; and distinguish between a marker's lexical contribution and the role it marks in the context of a predicate or scene. Strong interannotator agreement rates, as well as encouraging disambiguation results with established supervised methods, speak to the viability of the scheme and task.},
  file = {/Users/coleman/Zotero/storage/4IHZYY2C/Schneider et al. - 2018 - Comprehensive Supersense Disambiguation of English Prepositions and Possessives.pdf}
}

@misc{schneider-et-al-2022-adposition,
  title = {Adposition and {{Case Supersenses}} v2.6: {{Guidelines}} for {{English}}},
  shorttitle = {Adposition and {{Case Supersenses}} v2.6},
  author = {Schneider, Nathan and Hwang, Jena D. and Srikumar, Vivek and Bhatia, Archna and Han, Na-Rae and O'Gorman, Tim and Moeller, Sarah R. and Abend, Omri and Shalev, Adi and Blodgett, Austin and Prange, Jakob},
  year = 2022,
  month = jul,
  number = {arXiv:1704.02134},
  eprint = {1704.02134},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1704.02134},
  urldate = {2025-08-09},
  abstract = {This document offers a detailed linguistic description of SNACS (Semantic Network of Adposition and Case Supersenses; Schneider et al., 2018), an inventory of 52 semantic labels ("supersenses") that characterize the use of adpositions and case markers at a somewhat coarse level of granularity, as demonstrated in the STREUSLE corpus (https://github.com/nert-nlp/streusle/ ; version 4.5 tracks guidelines version 2.6). Though the SNACS inventory aspires to be universal, this document is specific to English; documentation for other languages will be published separately. Version 2 is a revision of the supersense inventory proposed for English by Schneider et al. (2015, 2016) (henceforth "v1"), which in turn was based on previous schemes. The present inventory was developed after extensive review of the v1 corpus annotations for English, plus previously unanalyzed genitive case possessives (Blodgett and Schneider, 2018), as well as consideration of adposition and case phenomena in Hebrew, Hindi, Korean, and German. Hwang et al. (2017) present the theoretical underpinnings of the v2 scheme. Schneider et al. (2018) summarize the scheme, its application to English corpus data, and an automatic disambiguation task. Liu et al. (2021) offer an English Lexical Semantic Recognition tagger that includes SNACS labels in its output. This documentation can also be browsed alongside corpus data on the Xposition website (Gessler et al., 2022): http://www.xposition.org/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/coleman/Zotero/storage/MNPLLARY/Schneider et al. - 2022 - Adposition and Case Supersenses v2.6 Guidelines for English.pdf;/Users/coleman/Zotero/storage/PDQ2LTIU/1704.html}
}

@article{schrimpf-et-al-2021-neural,
  title = {The Neural Architecture of Language: {{Integrative}} Modeling Converges on Predictive Processing},
  shorttitle = {The Neural Architecture of Language},
  author = {Schrimpf, Martin and Blank, Idan Asher and Tuckute, Greta and Kauf, Carina and Hosseini, Eghbal A. and Kanwisher, Nancy and Tenenbaum, Joshua B. and Fedorenko, Evelina},
  year = 2021,
  month = nov,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {45},
  doi = {10.1073/pnas.2105646118},
  urldate = {2025-10-30},
  abstract = {The neuroscience of perception has recently been revolutionized with an integrative modeling approach in which computation, brain function, and behavior are linked across many datasets and many computational models. By revealing trends across models, this approach yields novel insights into cognitive and neural mechanisms in the target domain. We here present a systematic study taking this approach to higher-level cognition: human language processing, our species' signature cognitive skill. We find that the most powerful ``transformer'' models predict nearly 100\% of explainable variance in neural responses to sentences and generalize across different datasets and imaging modalities (functional MRI and electrocorticography). Models' neural fits (``brain score'') and fits to behavioral responses are both strongly correlated with model accuracy on the next-word prediction task (but not other language tasks). Model architecture appears to substantially contribute to neural fit. These results provide computationally explicit evidence that predictive processing fundamentally shapes the language comprehension mechanisms in the human brain.},
  note = {Article e2105646118},
  file = {/Users/coleman/Zotero/storage/EJGNMGSA/Schrimpf et al. - 2021 - The neural architecture of language Integrative modeling converges on predictive processing.pdf}
}

@phdthesis{schultze-berndt-2000-simple,
  title = {Simple and Complex Verbs in {{Jaminjung}}: {{A}} Study of Event Categorisation in an {{Australian}} Language},
  shorttitle = {Simple and Complex Verbs in {{Jaminjung}}},
  author = {{Schultze-Berndt}, Eva},
  year = 2000,
  address = {Nijmegen, Netherlands},
  url = {http://pubman.mpdl.mpg.de/pubman/item/escidoc:2057716},
  urldate = {2024-10-07},
  langid = {english},
  school = {Radboud University}
}

@article{schwartz-et-al-1997-dispersionfocalization,
  title = {The {{Dispersion-Focalization Theory}} of Vowel Systems},
  author = {Schwartz, Jean-Luc and Bo{\"e}, Louis-Jean and Vall{\'e}e, Nathalie and Abry, Christian},
  year = 1997,
  month = jul,
  journal = {Journal of Phonetics},
  volume = {25},
  number = {3},
  pages = {255--286},
  issn = {0095-4470},
  doi = {10.1006/jpho.1997.0043},
  urldate = {2025-11-06},
  abstract = {The Dispersion-Focalization Theory (DFT) attempts to predict vowel systems based on the minimization of an energy function summing two perceptual components: global dispersion, which is based on inter-vowel distances; and local focalization, which is based on intra-vowel spectral salience related to the proximity of formants. The computation takes place in an auditory (formant-based) space, and is controlled by two parameters,{$\lambda$},which sets the respective weight of F1and higher formants in auditory distances, and{$\alpha$},which sets the respective weights of the dispersion and focalization components. In this study, we first sample the acoustic space with 33 ``prototypes'' associated with the major primary articulations for vowels. Then, for a given number of vowels, we define two different tools, ``phase spaces'' for dealing with optimal systems and ``stability '' for characterizing sub-optimal ones. The corresponding predictions are compared with the main trends of vowel systems analyzed in a companion paper (Schwartz, Bo\"e, Vall\'ee \& Abry, this issue). We then derive a region in the ({$\lambda$}, {$\alpha$})space for which theory predictions fit quite well with phonological inventories, with a special concern for two problems, namely peripheral vowels and front rounded vowels. Finally we stress the difficulty of predicting some trends of vowel systems linked to the organization of vowels in series, which could constitute a limit to substance-based predictions of vowel inventories.},
  file = {/Users/coleman/Zotero/storage/47F2ZGRR/S0095447097900437.html}
}

@article{schwartz-et-al-1997-major,
  title = {Major Trends in Vowel System Inventories},
  author = {Schwartz, Jean-Luc and Bo{\"e}, Louis-Jean and Vall{\'e}e, Nathalie and Abry, Christian},
  year = 1997,
  month = jul,
  journal = {Journal of Phonetics},
  volume = {25},
  number = {3},
  pages = {233--253},
  issn = {0095-4470},
  doi = {10.1006/jpho.1997.0044},
  urldate = {2025-10-07},
  abstract = {The search for universal tendencies in the languages of the world is a necessary anchor point for any theoretical approach to phonetics. The present typology of vowel systems aims to provide material for testing substance-based theories, including, among others, the Dispersion-Focalization Theory of vowel systems presented in a companion paper (Schwartz, Bo\"e, Vall\'ee \& Abry, this issue). It is focused on theeconomyof vowel systems, specifically the way a system of vowels functions as a whole, and the way vowels interact within a given system. The UPSID (UCLA Phonological Segment Inventory Database,Maddieson, 1984) inventory is analyzed using an original methodology, with the following main results. 1.Vowel systems first exploit a ``primary'' system of sounds; with more than 9 vowels, there is a clear trend for exploiting at least one new dimension (``secondary '' systems). 2.Primary systems mainly contain 3 to 9 vowels, and secondary systems 1 to 7 vowels, both with a preference for 5 vowels. 3.In both primary and secondary systems, vowels are mainly concentrated at the periphery. For peripheral systems, symmetry (same number of front and back vowels) is the rule; if there is an asymmetry, the number of front vowels is generally greater than the number of back vowels. 4.For ``interior'' vowels in primary systems, central vowels are preferred; among non-central vowels, front rounded vowels are twice as frequent as back unrounded ones. 5.Schwa is the preferred interior vowel, and it does not seem to interact with other sounds within vowel systems.},
  file = {/Users/coleman/Zotero/storage/XSG6GZE8/Schwartz et al. - 1997 - Major trends in vowel system inventories.pdf;/Users/coleman/Zotero/storage/JT77JFRA/S0095447097900449.html}
}

@inproceedings{scivetti-et-al-2025-construction,
  title = {Construction Identification and Disambiguation Using {{BERT}}: {{A}} Case Study of {{NPN}}},
  shorttitle = {Construction {{Identification}} and {{Disambiguation Using BERT}}},
  booktitle = {Proceedings of the 29th {{Conference}} on {{Computational Natural Language Learning}}},
  author = {Scivetti, Wesley and Schneider, Nathan},
  editor = {Boleda, Gemma and Roth, Michael},
  year = 2025,
  month = jul,
  pages = {365--376},
  publisher = {Association for Computational Linguistics},
  address = {Vienna, Austria},
  doi = {10.18653/v1/2025.conll-1.24},
  urldate = {2025-11-04},
  abstract = {Construction Grammar hypothesizes that knowledge of a language consists chiefly of knowledge of form--meaning pairs (``constructions'') that include vocabulary, general grammar rules, and even idiosyncratic patterns. Recent work has shown that transformer language models represent at least some constructional patterns, including ones where the construction is rare overall. In this work, we probe BERT's representation of the form and meaning of a minor construction of English, the NPN (noun--preposition--noun) construction---exhibited in such expressions as face to face and day to day---which is known to be polysemous. We construct a benchmark dataset of semantically annotated corpus instances (including distractors that superficially resemble the construction). With this dataset, we train and evaluate probing classifiers. They achieve decent discrimination of the construction from distractors, as well as sense disambiguation among true instances of the construction, revealing that BERT embeddings carry indications of the construction's semantics.Moreover, artificially permuting the word order of true construction instances causes them to be rejected, indicating sensitivity to matters of form. We conclude that BERT does latently encode at least some knowledge of the NPN construction going beyond a surface syntactic pattern and lexical cues.},
  isbn = {979-8-89176-271-8},
  file = {/Users/coleman/Zotero/storage/BRDFWDKI/Scivetti and Schneider - 2025 - Construction Identification and Disambiguation Using BERT A Case Study of NPN.pdf}
}

@inproceedings{scivetti-et-al-2025-multilingual,
  title = {Multilingual {{Supervision Improves Semantic Disambiguation}} of {{Adpositions}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Computational Linguistics}}},
  author = {Scivetti, Wesley and Levine, Lauren and Schneider, Nathan},
  editor = {Rambow, Owen and Wanner, Leo and Apidianaki, Marianna and {Al-Khalifa}, Hend and Eugenio, Barbara Di and Schockaert, Steven},
  year = 2025,
  month = jan,
  pages = {3655--3669},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, UAE},
  url = {https://aclanthology.org/2025.coling-main.247/},
  urldate = {2025-08-09},
  abstract = {Adpositions display a remarkable amount of ambiguity and flexibility in their meanings, and are used in different ways across languages. We conduct a systematic corpus-based cross-linguistic investigation into the lexical semantics of adpositions, utilizing SNACS (Schneider et al., 2018), an annotation framework with data available in several languages. Our investigation encompasses 5 of these languages: Chinese, English, Gujarati, Hindi, and Japanese. We find substantial distributional differences in adposition semantics, even in comparable corpora. We further train classifiers to disambiguate adpositions in each of our languages. Despite the cross-linguistic differences in adpositional usage, sharing annotated data across languages boosts overall disambiguation performance, leading to the highest published scores on this task for all 5 languages.},
  file = {/Users/coleman/Zotero/storage/DETAEF3I/Scivetti et al. - 2025 - Multilingual Supervision Improves Semantic Disambiguation of Adpositions.pdf}
}

@article{scott-et-al-2019-glasgow,
  title = {The {{Glasgow Norms}}: {{Ratings}} of 5,500 Words on Nine Scales},
  shorttitle = {The {{Glasgow Norms}}},
  author = {Scott, Graham G. and Keitel, Anne and Becirspahic, Marc and Yao, Bo and Sereno, Sara C.},
  year = 2019,
  month = jun,
  journal = {Behavior Research Methods},
  volume = {51},
  number = {3},
  pages = {1258--1270},
  issn = {1554-3528},
  doi = {10.3758/s13428-018-1099-3},
  urldate = {2024-10-15},
  abstract = {The Glasgow Norms are a set of normative ratings for 5,553 English words on nine psycholinguistic dimensions: arousal, valence, dominance, concreteness, imageability, familiarity, age of acquisition, semantic size, and gender association. The Glasgow Norms are unique in several respects. First, the corpus itself is relatively large, while simultaneously providing norms across a substantial number of lexical dimensions. Second, for any given subset of words, the same participants provided ratings across all nine dimensions (33 participants/word, on average). Third, two novel dimensions---semantic size and gender association---are included. Finally, the corpus contains a set of 379 ambiguous words that are presented either alone (e.g., toast) or with information that selects an alternative sense (e.g., toast (bread), toast (speech)). The relationships between the dimensions of the Glasgow Norms were initially investigated by assessing their correlations. In addition, a principal component analysis revealed four main factors, accounting for 82\% of the variance (Visualization, Emotion, Salience, and Exposure). The validity of the Glasgow Norms was established via comparisons of our ratings to 18 different sets of current psycholinguistic norms. The dimension of size was tested with megastudy data, confirming findings from past studies that have explicitly examined this variable. Alternative senses of ambiguous words (i.e., disambiguated forms), when discordant on a given dimension, seemingly led to appropriately distinct ratings. Informal comparisons between the ratings of ambiguous words and of their alternative senses showed different patterns that likely depended on several factors (the number of senses, their relative strengths, and the rating scales themselves). Overall, the Glasgow Norms provide a valuable resource---in particular, for researchers investigating the role of word recognition in language comprehension.},
  langid = {english},
  keywords = {Age of acquisition,Arousal,Concreteness,Dominance,Familiarity,Gender association,Imageability,Psycholinguistic norms,Semantic size,Valence},
  file = {/Users/coleman/Zotero/storage/HACPU8L7/Scott et al_2019_The Glasgow Norms.pdf}
}

@article{segalowitz-et-al-2000-lexical,
  title = {Lexical Access of Function versus Content Words},
  author = {Segalowitz, Sidney J. and Lane, Korri C.},
  year = 2000,
  journal = {Brain and Language},
  volume = {75},
  number = {3},
  pages = {376--389},
  publisher = {Elsevier Science},
  address = {Netherlands},
  issn = {1090-2155},
  doi = {10.1006/brln.2000.2361},
  abstract = {There has been a simmering debate as to whether evidence exists for differential processes of lexical access for function and content words. This has centered around the frequency effect (higher word frequency reducing access times for content words but not function words). Previous work has used the lexical decision paradigm, which has been shown to reflect more than lexical access times. The authors measured naming times for words in sentences read for meaning. The findings confirm that lexical access for function words is indeed faster than for content words as predicted by neurolinguistic theory and electrophysiological evidence, but that this difference can be attributed to word predictability (Cloze value) and word familiarity (log frequency). The authors also show that differences in frequency effect for the 2 word types holds only for the lower frequency words and not at all for the higher frequency words. They discuss the implications of the results for neurolinguistic theory. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  keywords = {Lexical Access,Naming,Sentence Comprehension,Word Recognition,Words (Phonetic Units)},
  file = {/Users/coleman/Zotero/storage/SFR5GF2C/2000-14258-004.html}
}

@article{seymour-2022-color,
  title = {Color Inconstancy in {{CIELAB}}: {{A}} Red Herring?},
  shorttitle = {Color Inconstancy in {{CIELAB}}},
  author = {Seymour, John},
  year = 2022,
  journal = {Color Research \& Application},
  volume = {47},
  number = {4},
  pages = {900--919},
  issn = {1520-6378},
  doi = {10.1002/col.22782},
  urldate = {2025-11-09},
  abstract = {Color constancy is something we take for granted. An apple appears red despite monumental changes in the intensity and spectral balance of illumination. There are many components to the human visual system that make this illusion possible. Nonetheless, the system is imperfect. Objects will change apparent color under different illumination. The CIELAB color space allows for the computation of color values under any illuminant. CIELAB values will, in general, change as the illuminant changes. It would be natural to assume that these changes in CIELAB values would agree with the apparent color change that a human would experience. But the chromatic adaptation built into CIELAB is known to be less than optimal due to an imperfect emulation of how the adaptation process is performed in the human visual system. As a result, at least part of the illuminant color change implied by the CIELAB values is an artifact of this imperfect emulation. This paper investigates the magnitude of this artifact by comparing color changes implied by CIELAB with color changes in a color system that mimics CIELAB, except that it more closely emulates the human visual system. The difference between the two estimates of color change is assumed to be an artifact of CIELAB's poor emulation of the chromatic adaptation process. As color manufacturers are dealing with a wider spectrum of illumination sources, they may wish to consider the issue of color constancy; what colors and what formulations of colors exhibit better color constancy? Can CIELAB be used to quantify this? Or are color spaces that are based on LMS rather than XYZ more appropriate?},
  copyright = {\copyright{} 2022 The Author. Color Research and Application published by Wiley Periodicals LLC.},
  langid = {english},
  keywords = {CIELAB,color inconstancy,LMS},
  file = {/Users/coleman/Zotero/storage/X8WW4GW3/Seymour - 2022 - Color inconstancy in CIELAB A red herring.pdf;/Users/coleman/Zotero/storage/8F643YG8/col.html}
}

@article{shannon-1948-mathematical,
  title = {A Mathematical Theory of Communication},
  author = {Shannon, Claude},
  year = 1948,
  month = jul,
  journal = {Bell System Technical Journal},
  volume = {27},
  number = {3},
  pages = {379--423},
  publisher = {John Wiley \& Sons, Ltd},
  issn = {1538-7305},
  doi = {10.1002/j.1538-7305.1948.tb01338.x},
  urldate = {2025-10-27},
  abstract = {Click on the article title to read more.},
  langid = {english}
}

@inproceedings{sikos-et-al-2019-frame,
  title = {Frame {{Identification}} as {{Categorization}}: {{Exemplars}} vs {{Prototypes}} in {{Embeddingland}}},
  shorttitle = {Frame {{Identification}} as {{Categorization}}},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{Computational Semantics}} - {{Long Papers}}},
  author = {Sikos, Jennifer and Pad{\'o}, Sebastian},
  editor = {Dobnik, Simon and Chatzikyriakidis, Stergios and Demberg, Vera},
  year = 2019,
  month = may,
  pages = {295--306},
  publisher = {Association for Computational Linguistics},
  address = {Gothenburg, Sweden},
  doi = {10.18653/v1/W19-0425},
  urldate = {2025-09-20},
  abstract = {Categorization is a central capability of human cognition, and a number of theories have been developed to account for properties of categorization. Even though many tasks in semantics also involve categorization of some kind, theories of categorization do not play a major role in contemporary research in computational linguistics. This paper follows the idea that embedding-based models of semantics lend themselves well to being formulated in terms of classical categorization theories. The benefit is a space of model families that enables (a) the formulation of hypotheses about the impact of major design decisions, and (b) a transparent assessment of these decisions. We instantiate this idea on the task of frame-semantic frame identification. We define four models that cross two design variables: (a) the choice of prototype vs. exemplar categorization, corresponding to different degrees of generalization applied to the input; and (b) the presence vs. absence of a fine-tuning step, corresponding to generic vs. task-adaptive categorization. We find that for frame identification, generalization and task-adaptive categorization both yield substantial benefits. Our prototype-based, fine-tuned model, which combines the best choices for these variables, establishes a new state of the art in frame identification.},
  file = {/Users/coleman/Zotero/storage/DD5UMHZN/Sikos and Pad - 2019 - Frame Identification as Categorization Exemplars vs Prototypes in Embeddingland.pdf}
}

@incollection{silverstein-1986-hierarchy,
  title = {Hierarchy of Features and Ergativity},
  booktitle = {Features and Projections},
  author = {Silverstein, Michael},
  year = 1986,
  pages = {163--232},
  publisher = {De Gruyter Mouton},
  address = {Berlin, Germany},
  doi = {doi:10.1515/9783110871661-008},
  urldate = {2023-10-13},
  isbn = {978-3-11-087166-1}
}

@incollection{simone-et-al-2014-light,
  title = {On {{Light Nouns}}},
  booktitle = {Word {{Classes}}: {{Nature}}, Typology and Representations},
  author = {Simone, Raffaele and Masini, Francesca},
  editor = {Simone, Raffaele and Masini, Francesca},
  year = 2014,
  month = sep,
  pages = {51--74},
  publisher = {John Benjamins Publishing Company},
  url = {https://www.degruyterbrill.com/document/doi/10.1075/cilt.332.04sim/html},
  urldate = {2025-10-07},
  isbn = {978-90-272-6976-8},
  langid = {english}
}

@book{simone-et-al-2014-word,
  title = {Word {{Classes}}: {{Nature}}, {{Typology}} and {{Representations}}},
  shorttitle = {Word {{Classes}}},
  author = {Simone, Raffaele and Masini, Francesca},
  year = 2014,
  publisher = {John Benjamins Publishing Company},
  address = {Amsterdam/Philadelphia, NETHERLANDS, THE},
  url = {http://ebookcentral.proquest.com/lib/ed/detail.action?docID=1784085},
  urldate = {2025-09-09},
  isbn = {978-90-272-6976-8},
  keywords = {Grammar Comparative and general -- Grammatical catagories.,Parts of speech.,Typology (Linguistics),Word (Linguistics)},
  file = {/Users/coleman/Zotero/storage/SC2B9A4S/detail.html}
}

@article{smith-levy-2013,
  title = {The Effect of Word Predictability on Reading Time Is Logarithmic},
  author = {Smith, Nathaniel J. and Levy, Roger},
  year = 2013,
  journal = {Cognition},
  volume = {128},
  number = {3},
  pages = {302--319},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2013.02.013},
  abstract = {It is well known that real-time human language processing is highly incremental and context-driven, and that the strength of a comprehender's expectation for each word encountered is a key determinant of the difficulty of integrating that word into the preceding context. In reading, this differential difficulty is largely manifested in the amount of time taken to read each word. While numerous studies over the past thirty years have shown expectation-based effects on reading times driven by lexical, syntactic, semantic, pragmatic, and other information sources, there has been little progress in establishing the quantitative relationship between expectation (or prediction) and reading times. Here, by combining a state-of-the-art computational language model, two large behavioral data-sets, and non-parametric statistical techniques, we establish for the first time the quantitative form of this relationship, finding that it is logarithmic over six orders of magnitude in estimated predictability. This result is problematic for a number of established models of eye movement control in reading, but lends partial support to an optimal perceptual discrimination account of word recognition. We also present a novel model in which language processing is highly incremental well below the level of the individual word, and show that it predicts both the shape and time-course of this effect. At a more general level, this result provides challenges for both anticipatory processing and semantic integration accounts of lexical predictability effects. And finally, this result provides evidence that comprehenders are highly sensitive to relative differences in predictability -- even for differences between highly unpredictable words -- and thus helps bring theoretical unity to our understanding of the role of prediction at multiple levels of linguistic structure in real-time language comprehension.},
  keywords = {Expectation,Information theory,Probabilistic models of cognition,Psycholinguistics,Reading}
}

@book{sohn-1999-korean,
  title = {The {{Korean}} Language},
  author = {Sohn, Ho-Min},
  year = 1999,
  series = {Cambridge {{Language Surveys}}},
  publisher = {Cambridge University Press},
  address = {Cambridge, UK}
}

@book{spencer-2013-lexical,
  title = {Lexical Relatedness},
  author = {Spencer, Andrew},
  year = 2013,
  publisher = {Oxford University Press},
  address = {Oxford, UK}
}

@article{spreen-et-al-1966-parameters,
  title = {Parameters of Abstraction, Meaningfulness, and Pronunciability for 329 Nouns},
  author = {Spreen, Otfried and Schulz, Rudolph W.},
  year = 1966,
  month = oct,
  journal = {Journal of Verbal Learning and Verbal Behavior},
  volume = {5},
  number = {5},
  pages = {459--468},
  issn = {00225371},
  doi = {10.1016/S0022-5371(66)80061-0},
  urldate = {2024-10-07},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english}
}

@book{stassen-1997-intransitive,
  title = {Intransitive {{Predication}}},
  author = {Stassen, Leon},
  year = 1997,
  month = sep,
  publisher = {Oxford University Press},
  address = {Oxford, UK},
  doi = {10.1093/oso/9780198236931.001.0001},
  abstract = {Intransitive Predication constitutes a major contribution to the study of typological linguistics and theoretical linguistics in general. Basing his analysis on a sample of 410 languages, Leon Stassen investigates cross-linguistic variation in one of the core domains of all natural languages. The author views this domain as a `cognitive space', the topography of which is the same for all languages. It is assumed to consist of four subdomains, which correspond to a four-way distinction between the semantic classes of event predicates, property predicates, class predicates, and locational predicates. Leon Stassen offers a typology of the structural manifestations of this domain, in terms of the nature and number of the formal strategies used in its encoding. He discusses a number of abstract principles which can be employed in explaining the cross-linguistic variation embodied by the typology. In the final chapter, he brings together the research results in a universally applicable model, which can be read as a `flow chart' for the encoding of intransitive predications in different language types.},
  isbn = {978-0-19-823693-1}
}

@incollection{stassen-2021-black,
  title = {Black and White {{Languages}}},
  booktitle = {Meaning and {{Grammar}} of {{Nouns}} and {{Verbs}}},
  author = {Stassen, Leon},
  editor = {Gerland, Doris and Horn, Christian and Latrouite, Anja and Ortmann, Albert},
  year = 2021,
  month = dec,
  pages = {315--338},
  publisher = {De Gruyter},
  doi = {10.1515/9783110720075-012},
  urldate = {2025-09-05},
  copyright = {http://creativecommons.org/licenses/by-nc-nd/4.0},
  isbn = {978-3-11-072007-5},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/IZJJJ8WV/Stassen - 2021 - Black and white Languages.pdf}
}

@article{staub-Forthcoming-predictability,
  title = {Predictability in Language Comprehension: Prospects and Problems for Surprisal},
  shorttitle = {Predictability in {{Language Comprehension}}},
  author = {Staub, Adrian},
  year = {Forthcoming},
  journal = {Annual Review of Linguistics},
  publisher = {Annual Reviews},
  doi = {10.1146/annurev-linguistics-011724-121517},
  urldate = {2024-10-14},
  abstract = {Surprisal theory proposes that a word\&apos;s predictability influences processing difficulty because each word requires the comprehender to update a probability distribution over possible sentences. This article first considers the theory\&apos;s detailed predictions regarding the effects of predictability on reading time and N400 amplitude. Two rather unintuitive predictions appear to be correct based on the current evidence: There is no specific cost when an unpredictable word is encountered in a context where another word is predictable, and the function relating predictability to processing difficulty is logarithmic, not linear. Next, the article addresses the viability of the claim, also associated with Surprisal, that conditional probability is the ``causal bottleneck'' mediating all effects on incremental processing difficulty. This claim fares less well as conditional probability does not account for the difficulty associated with encountering a low-frequency word or the difficulty associated with garden path disambiguation. Surprisal provides a compelling account of predictability effects but does not provide a complete account of incremental processing difficulty.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/5MYJ9MNU/annurev-linguistics-011724-121517.html}
}

@article{steinert-threlkeld-2021-quantifiers,
  title = {Quantifiers in {{Natural Language}}: {{Efficient Communication}} and {{Degrees}} of {{Semantic Universals}}},
  shorttitle = {Quantifiers in {{Natural Language}}},
  author = {{Steinert-Threlkeld}, Shane},
  year = 2021,
  month = oct,
  journal = {Entropy},
  volume = {23},
  number = {10},
  pages = {1335},
  issn = {1099-4300},
  doi = {10.3390/e23101335},
  urldate = {2025-09-25},
  abstract = {While the languages of the world vary greatly, they exhibit systematic patterns, as well. Semantic universals are restrictions on the variation in meaning exhibit cross-linguistically (e.g., that, in all languages, expressions of a certain type can only denote meanings with a certain special property). This paper pursues an efficient communication analysis to explain the presence of semantic universals in a domain of function words: quantifiers. Two experiments measure how well languages do in optimally trading off between competing pressures of simplicity and informativeness. First, we show that artificial languages which more closely resemble natural languages are more optimal. Then, we introduce information-theoretic measures of degrees of semantic universals and show that these are not correlated with optimality in a random sample of artificial languages. These results suggest both that efficient communication shapes semantic typology in both content and function word domains, as well as that semantic universals may not stand in need of independent explanation.},
  pmcid = {PMC8570335},
  pmid = {34682059}
}

@article{steinert-threlkeld-2021-quantifiersa,
  title = {Quantifiers in {{Natural Language}}: {{Efficient Communication}} and {{Degrees}} of {{Semantic Universals}}},
  shorttitle = {Quantifiers in {{Natural Language}}},
  author = {{Steinert-Threlkeld}, Shane},
  year = 2021,
  month = oct,
  journal = {Entropy},
  volume = {23},
  number = {10},
  pages = {1335},
  issn = {1099-4300},
  doi = {10.3390/e23101335},
  urldate = {2025-11-09},
  abstract = {While the languages of the world vary greatly, they exhibit systematic patterns, as well. Semantic universals are restrictions on the variation in meaning exhibit cross-linguistically (e.g., that, in all languages, expressions of a certain type can only denote meanings with a certain special property). This paper pursues an efficient communication analysis to explain the presence of semantic universals in a domain of function words: quantifiers. Two experiments measure how well languages do in optimally trading off between competing pressures of simplicity and informativeness. First, we show that artificial languages which more closely resemble natural languages are more optimal. Then, we introduce information-theoretic measures of degrees of semantic universals and show that these are not correlated with optimality in a random sample of artificial languages. These results suggest both that efficient communication shapes semantic typology in both content and function word domains, as well as that semantic universals may not stand in need of independent explanation.},
  pmcid = {PMC8570335},
  pmid = {34682059},
  file = {/Users/coleman/Zotero/storage/EVKU4J6U/Steinert-Threlkeld - 2021 - Quantifiers in Natural Language Efficient Communication and Degrees of Semantic Universals.pdf}
}

@incollection{stekauer-2015-delimitation,
  title = {The Delimitation of Derivation and Inflection},
  booktitle = {Volume 1 Word-Formation},
  author = {{\v S}tekauer, Pavol},
  editor = {M{\"u}ller, Peter O. and Ohnheiser, Ingeborg and Olsen, Susan and Rainer, Franz},
  year = 2015,
  pages = {218--235},
  publisher = {De Gruyter Mouton},
  address = {Berlin, Germany}
}

@incollection{stevens-1972-quantal,
  title = {The Quantal Nature of Speech: {{Evidence}} from Articulatory-Acoustic Data},
  booktitle = {Human {{Communication}}: {{A Unified View}}},
  author = {Stevens, Kenneth N.},
  editor = {David, Edward E. and Denes, Peter B.},
  year = 1972,
  pages = {51--56},
  publisher = {McGraw-Hill},
  address = {New York, New York, USA},
  url = {https://archive.org/details/humancommunicati0000davi/page/n5/mode/2up}
}

@article{stevens-1989-quantal,
  title = {On the Quantal Nature of Speech},
  author = {Stevens, Kenneth N.},
  year = 1989,
  month = jan,
  journal = {Journal of Phonetics},
  volume = {17},
  number = {1-2},
  pages = {3--45},
  issn = {00954470},
  doi = {10.1016/S0095-4470(19)31520-7},
  urldate = {2025-09-26},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/8DA7BFPG/Stevens - 1989 - On the quantal nature of speech.pdf}
}

@article{striklievers-et-al-2021-linguistic,
  title = {The Linguistic Dimensions of Concrete and Abstract Concepts: Lexical Category, Morphological Structure, Countability, and Etymology},
  author = {Strik Lievers, Francesca and Bolognesi, Marianna and Winter, Bodo},
  year = 2021,
  journal = {Cognitive Linguistics},
  volume = {32},
  number = {4},
  pages = {641--670},
  doi = {10.1515/cog-2021-0007},
  urldate = {2024-05-15}
}

@mastersthesis{strunk-2020-finitestate,
  title = {A Finite-State Morphological Analyzer for {{Central Alaskan Yup}}'ik},
  author = {Strunk, Lonny Alaskuk},
  year = 2020,
  address = {Seattle, Washington, USA},
  school = {University of Washington}
}

@article{swingley-2005-statistical,
  title = {Statistical Clustering and the Contents of the Infant Vocabulary},
  author = {Swingley, Daniel},
  year = 2005,
  journal = {Cognitive psychology},
  volume = {50},
  number = {1},
  pages = {86--132},
  publisher = {Elsevier}
}

@misc{sylak-glassman-2016-composition,
  title = {The Composition and Use of the Universal Morphological Feature Schema ({{UniMorph}} Schema)},
  author = {{Sylak-Glassman}, John},
  year = 2016,
  url = {https://unimorph.github.io/doc/unimorph-schema.pdf}
}

@inproceedings{takaoka-et-al-2018-sudachi,
  title = {Sudachi: A {{Japanese Tokenizer}} for {{Business}}},
  shorttitle = {Sudachi},
  booktitle = {Proceedings of the {{Eleventh International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}} 2018)},
  author = {Takaoka, Kazuma and Hisamoto, Sorami and Kawahara, Noriko and Sakamoto, Miho and Uchida, Yoshitaka and Matsumoto, Yuji},
  editor = {Calzolari, Nicoletta and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Hasida, Koiti and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, H{\'e}l{\`e}ne and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios and Tokunaga, Takenobu},
  year = 2018,
  month = may,
  publisher = {European Language Resources Association (ELRA)},
  address = {Miyazaki, Japan},
  url = {https://aclanthology.org/L18-1355/},
  urldate = {2025-09-12},
  file = {/Users/coleman/Zotero/storage/FJUN6AYU/Takaoka et al. - 2018 - Sudachi a Japanese Tokenizer for Business.pdf}
}

@phdthesis{talmy-1972-semantic,
  title = {Semantic {{Structures}} in {{English}} and {{Atsugewi}}},
  author = {Talmy, Leonard},
  year = 1972,
  address = {Berkley, California, USA},
  url = {https://escholarship.org/uc/item/5g15p348},
  urldate = {2025-11-08},
  abstract = {Author(s): Talmy, Leonard},
  langid = {english},
  school = {University of California, Berkeley,},
  file = {/Users/coleman/Zotero/storage/XIRYMIX4/Talmy - 1972 - Semantic Structures in English and Atsugewi.pdf}
}

@book{talmy-2000-cognitive,
  title = {Toward a {{Cognitive Semantics}}, {{Volume}} 1: {{Concept Structuring Systems}}},
  shorttitle = {Toward a {{Cognitive Semantics}}, {{Volume}} 1},
  author = {Talmy, Leonard},
  year = 2000,
  month = sep,
  publisher = {The MIT Press},
  address = {Cambridge, MA, USA},
  doi = {10.7551/mitpress/6847.001.0001},
  urldate = {2025-11-08},
  abstract = {In this two-volume set, Talmy approaches the question of how language organizes conceptual material both at a general level and by analyzing a crucial set},
  isbn = {978-0-262-28466-0},
  langid = {english}
}

@inproceedings{tanzer-et-al-2024-benchmark,
  title = {A {{Benchmark}} for {{Learning}} to {{Translate}} a {{New Language}} from {{One Grammar Book}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}, {{ICLR}} 2024, {{Vienna}}, {{Austria}}, {{May}} 7-11, 2024},
  author = {Tanzer, Garrett and Suzgun, Mirac and Visser, Eline and Jurafsky, Dan and {Melas-Kyriazi}, Luke},
  year = 2024,
  publisher = {OpenReview.net},
  url = {https://openreview.net/forum?id=tbVWug9f2h},
  urldate = {2025-11-08}
}

@mastersthesis{tariq-2018-fuzzy,
  title = {The `{{Fuzzy}}' {{Boundary Between Two Types}} of {{Japanese Adjectives}}},
  author = {Tariq, Ana Kanza},
  year = 2018,
  month = nov,
  url = {https://ualberta.scholaris.ca/items/33849965-3b2c-4b8e-a09f-5c167a6bcccb},
  urldate = {2025-11-09},
  abstract = {The two kinds of Japanese adjectives, i-adjectives and na-adjectives, along with nouns, employ different forms (-i, -na, and no) to modify a noun. Based on such patterns, along with other grammatical characteristics identified in constructed examples, boundaries between lexical categories have traditionally been understood to be clear-cut. However, Uehara (1998, 2003) has found a number of lexical items which inflect both as na-adjectives and nouns. In fact, Uehara finds that more than 70 percent of na-taking lexical items exhibit noun-like behaviours. Using prior research to support his claim (Rosch1978; Lakoff 1987; Taylor 1989), he has suggested that the boundaries between lexical categories might not be as clear-cut as previously conceived. This thesis supports Uehara's proposal by highlighting a new set of data from internet discourse which suggests that the boundary between i-adjectives and na-adjectives might also be `fuzzy', using examples in which what are traditionally considered i-adjectives are inflected as na-adjectives. It presents the results of two studies. The first looks at the factors of word length and frequency of use of lexical items and how they affect this `improper' i-adjective conjugation in internet discourse. The second is a survey conducted to see how ``natural sounding'' native Japanese speakers consider this usage.},
  langid = {english},
  school = {University of Alberta},
  file = {/Users/coleman/Zotero/storage/V43X7IAF/Tariq - 2018 - The Fuzzy Boundary Between Two Types of Japanese Adjectives.pdf}
}

@article{tariq-fuzzy,
  title = {The `{{Fuzzy}}' {{Boundary Between Two Types}} of {{Japanese Adjectives}}},
  author = {Tariq, Ana Kanza},
  abstract = {The two kinds of Japanese adjectives, i-adjectives and na-adjectives, along with nouns, employ different forms (-i, -na, and no) to modify a noun. Based on such patterns, along with other grammatical characteristics identified in constructed examples, boundaries between lexical categories have traditionally been understood to be clear-cut.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/SC8SMW6S/Tariq - The Fuzzy Boundary Between Two Types of Japanese Adjectives.pdf}
}

@article{tarng-et-al-2025-visual,
  title = {Visual Features across Modalities: {{SVG}} and {{ASCII}} Art Reveal Cross-Modal Understanding},
  author = {Tarng, Julius and Goel, Purvi and Kauvar, Isaac},
  editor = {Batson, Joshua and Jermyn, Adam},
  year = 2025,
  month = oct,
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2025/october-update/index.html#visual-features-across-modalities}
}

@incollection{taylor-2003-syntactic,
  title = {Syntactic {{Constructions}} as {{Prototype Categories}}},
  booktitle = {Linguistic {{Categorization}}},
  author = {Taylor, John R},
  editor = {Taylor, John R},
  year = 2003,
  month = nov,
  pages = {0},
  publisher = {Oxford University Press},
  doi = {10.1093/oso/9780199266647.003.0012},
  urldate = {2025-09-20},
  abstract = {In reviewing the evidence for the prototype structure of grammatical cate-gories, we saw in Chapter 11 that members of a grammatical category do not necessarily exhibit a common set of syntactic properties. Not every noun can be inserted with equal facility into the possessor slot of the possessive construction, not every transitive verb has a passive counterpart, and so on. Possibility of occurrence in a construction is a matter of gradience, some items being readily available, others being totally excluded, with, in between, a range of items whose use is dubious or sporadic. As a consequence, constructions, no less than other kinds of linguistic objects, need to be regarded as prototype categories, with some instantiations counting as better examples of the construction than others. The main body of this chapter examines the prototype structure of some selected English constructions. First, however, it is necessary to say a few words about the notion of construction within cognitive linguistics, and why we need to recognize constructions in the first place.},
  isbn = {978-0-19-926664-7},
  file = {/Users/coleman/Zotero/storage/LXWXYPUU/9780199266647.003.html}
}

@book{tenhacken-1994-defining,
  title = {Defining Morphology: A Principled Approach to Determining the Boundaries of Compounding, Derivation, and Inflection},
  author = {{\noopsort{hacken}}{ten Hacken}, Pius},
  year = 1994,
  series = {Altertumswissenschaftliche Texte Und Studien},
  publisher = {Georg Olms Verlag},
  address = {Hildesheim, Germany},
  url = {https://books.google.co.uk/books?id=E8mWh_6mRAcC},
  isbn = {978-3-487-09891-3},
  lccn = {lc95155890}
}

@inproceedings{tenney-et-al-2019-what,
  title = {What Do You Learn from Context? {{Probing}} for Sentence Structure in Contextualized Word Representations},
  shorttitle = {What Do You Learn from Context?},
  booktitle = {7th {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2019, {{New Orleans}}, {{LA}}, {{USA}}, {{May}} 6-9, 2019},
  author = {Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R. Thomas and Kim, Najoung and Durme, Benjamin Van and Bowman, Samuel R. and Das, Dipanjan and Pavlick, Ellie},
  year = 2019,
  publisher = {OpenReview.net},
  url = {https://openreview.net/forum?id=SJzSgnRcKX},
  urldate = {2025-11-04}
}

@inproceedings{thapliyal-et-al-2022-crossmodal3600,
  title = {Crossmodal-3600: {{A}} Massively Multilingual Multimodal Evaluation Dataset},
  shorttitle = {Crossmodal-3600},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Thapliyal, Ashish V. and Pont Tuset, Jordi and Chen, Xi and Soricut, Radu},
  editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
  year = 2022,
  month = dec,
  pages = {715--729},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.45},
  urldate = {2024-10-07},
  abstract = {Research in massively multilingual image captioning has been severely hampered by a lack of high-quality evaluation datasets. In this paper we present the Crossmodal-3600 dataset (XM3600 in short), a geographically diverse set of 3600 images annotated with human-generated reference captions in 36 languages. The images were selected from across the world, covering regions where the 36 languages are spoken, and annotated with captions that achieve consistency in terms of style across all languages, while avoiding annotation artifacts due to direct translation. We apply this benchmark to model selection for massively multilingual image captioning models, and show superior correlation results with human evaluations when using XM3600 as golden references for automatic metrics.},
  file = {/Users/coleman/Zotero/storage/5CX4XFX2/Thapliyal et al_2022_Crossmodal-3600.pdf}
}

@article{theil-1970-estimation,
  title = {On the {{Estimation}} of {{Relationships Involving Qualitative Variables}}},
  author = {Theil, Henri},
  year = 1970,
  journal = {American Journal of Sociology},
  volume = {76},
  number = {1},
  pages = {103--154},
  publisher = {The University of Chicago Press},
  issn = {0002-9602},
  urldate = {2024-10-15},
  abstract = {This article is concerned with the specification and estimation of relationships whose dependent variable is qualitative in nature (such as "yes" or "no"). It discusses logit equations with and without interaction, and the estimation procedure is generalized least squares. Part I deals with dependent variables that take only two values, Par II with variables taking more than two values, and part III describes informational measures for the explanatory power of the determining factors. The discussion of more advanced technical matters is contained in various appendixes.},
  jstor = {2775440},
  file = {/Users/coleman/Zotero/storage/WWPTV9JQ/Theil_1970_On the Estimation of Relationships Involving Qualitative Variables.pdf}
}

@article{thiessen-et-al-2003-when,
  title = {When Cues Collide: Use of Stress and Statistical Cues to Word Boundaries by 7-to 9-Month-Old Infants.},
  author = {Thiessen, Erik D. and Saffran, Jenny R.},
  year = 2003,
  journal = {Developmental psychology},
  volume = {39},
  number = {4},
  pages = {706},
  publisher = {American Psychological Association}
}

@article{thiessen-et-al-2013-extraction,
  title = {The Extraction and Integration Framework: A Two-Process Account of Statistical Learning.},
  author = {Thiessen, Erik D. and Kronstein, Alexandra T. and Hufnagle, Daniel G.},
  year = 2013,
  journal = {Psychological bulletin},
  volume = {139},
  number = {4},
  pages = {792},
  publisher = {American Psychological Association}
}

@incollection{thompson-1989-discourse,
  title = {A Discourse Approach to the Cross-Linguistic Category `{{Adjective}}'},
  booktitle = {Linguistic {{Categorization}}: {{Proceedings}} of an {{International Symposium}} in {{Milwaukee}}, {{Wisconsin}}, {{April}} 10--11, 1987},
  author = {Thompson, Sandra},
  editor = {Corrigan, Roberta and Eckman, Fred and Noonan, Michael},
  year = 1989,
  month = jan,
  series = {Current {{Issues}} in {{Linguistic Theory}}},
  pages = {245--265},
  publisher = {John Benjamins Publishing Company},
  address = {Amsterdam, Netherlands},
  doi = {10.1075/cilt.61.16tho},
  urldate = {2025-09-20},
  isbn = {978-90-272-3558-9 978-90-272-7852-4},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/8UCU9DNV/cilt.61.html}
}

@article{thompson-et-al-2007-statistical,
  title = {Statistical Learning of Syntax: {{The}} Role of Transitional Probability},
  author = {Thompson, Susan P. and Newport, Elissa L.},
  year = 2007,
  journal = {Language learning and development},
  volume = {3},
  number = {1},
  pages = {1--42},
  publisher = {Taylor \& Francis}
}

@article{thompson-et-al-2012-verb,
  title = {Verb and Noun Deficits in Stroke-Induced and Primary Progressive Aphasia: {{The Northwestern Naming Battery}}},
  shorttitle = {Verb and Noun Deficits in Stroke-Induced and Primary Progressive Aphasia},
  author = {Thompson, Cynthia K. and Lukic, Sladjana and King, Monique C. and Mesulam, M. Marsel and Weintraub, Sandra},
  year = 2012,
  month = may,
  journal = {Aphasiology},
  volume = {26},
  number = {5},
  pages = {632--655},
  issn = {0268-7038},
  doi = {10.1080/02687038.2012.676852},
  urldate = {2025-10-24},
  abstract = {Background Word class naming deficits are commonly seen in aphasia resulting from stroke (StrAph) and primary progressive aphasia (PPA), with differential production of nouns (objects) and verbs (actions) found based on StrAph type or PPA variant for some individuals. Studies to date, however, have not compared word class naming (or comprehension) ability in the two aphasic disorders. In addition, there are no available measures for testing word class deficits, which control for important psycholinguistic variables across language domains. This study examined noun and verb production and comprehension in individuals with StrAph and PPA using a new test, the Northwestern Naming Battery (NNB; Thompson \& Weintraub, experimental version), developed explicitly for this purpose. In addition, we tested verb type effects, based on verb argument structure characteristics, which also is addressed by the NNB. Method Fifty-two participants with StrAph (33 agrammatic, Broca's (StrAg); 19 anomic (StrAn)) and 28 PPA (10 agrammatic (PPA-G); 14 logopenic (PPA-L); 4 semantic (PPA-S)) were included in the study. Nouns and verbs were tested in the Confrontation Naming and Auditory Comprehension subtests of the NNB, with scores used to compute noun to verb ratios as well as performance by verb type. Performance patterns within and across StrAph and PPA groups were then examined. The external validity of the NNB also was tested by comparing (a) NNB Noun Naming scores to the Boston Naming Test (BNT; ) and Western Aphasia Battery (WAB-R, ) Noun Naming subtest scores, (b) NNB Verb Naming scores to the Boston Diagnostic Aphasia Examination (BDAE; ) Action Naming score (for StrAph participants only), and (c) NNB Comprehension subtest scores to WAB-R Auditory Comprehension subtest scores. Outcomes and Results Both agrammatic (StrAg and PPA-G) groups showed significantly greater difficulty producing verbs compared to nouns, but no comprehension impairment for either word class. Whereas, three of the four PPA-S participants showed poorer noun compared to verb production, as well as comprehension. However, neither the StrAn or PPA-L participants showed significant differences between the two word classes in production or comprehension. In addition, similar to the agrammatic participants, the StrAn participants showed a significant transitivity effect, producing intransitive (one-argument) verbs with greater accuracy than transitive (two- and three-argument) verbs. However, no transitivity effects were found for the PPA-L or PPA-S participants. There were significant correlations between NNB scores and all external validation measures. Conclusions These data indicate that the NNB is sensitive to word class deficits in stroke and neurodegenerative aphasia. This is important both clinically for treatment planning and theoretically to inform both psycholinguistic and neural models of language processing.},
  pmcid = {PMC3505449},
  pmid = {23188949},
  file = {/Users/coleman/Zotero/storage/CMY7JJGS/Thompson et al. - 2012 - Verb and noun deficits in stroke-induced and primary progressive aphasia The Northwestern Naming Ba.pdf}
}

@book{trubetzkoy-1939-grundzuge,
  title = {Grundz\"uge Der {{Phonologie}}},
  author = {Trubetzkoy, Nicholas},
  year = 1939,
  series = {Travaux Du {{Cercle Linguistique}} de {{Prague}}},
  number = {7},
  address = {G\"ottingen, Germany}
}

@article{tseng-representation,
  title = {The {{Representation}} and {{Selection}} of {{Prepositions}}},
  author = {Tseng, Jesse},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/6DJRBFS3/Tseng - The Representation and Selection of Prepositions.pdf}
}

@inproceedings{tsimpoukelli-et-al-2021-multimodal,
  title = {Multimodal Few-Shot Learning with Frozen Language Models},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Tsimpoukelli, Maria and Menick, Jacob and Cabi, Serkan and Eslami, S. M. Ali and Vinyals, Oriol and Hill, Felix},
  year = 2021,
  month = dec,
  series = {{{NIPS}} '21},
  pages = {200--212},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2025-11-01},
  abstract = {When trained at sufficient scale, auto-regressive language models exhibit the notable ability to learn a new language task after being prompted with just a few examples. Here, we present a simple, yet effective, approach for transferring this few-shot learning ability to a multimodal setting (vision and language). Using aligned image and caption data, we train a vision encoder to represent each image as a sequence of continuous embeddings, such that a pre-trained, frozen language model prompted with this prefix generates the appropriate caption. The resulting system is a multimodal few-shot learner, with the surprising ability to learn a variety of new tasks when conditioned on examples, represented as a sequence of multiple interleaved image and text embeddings. We demonstrate that it can rapidly learn words for new objects and novel visual categories, do visual question-answering with only a handful of examples, and make use of outside knowledge, by measuring a single model on a variety of established and new benchmarks.},
  isbn = {978-1-7138-4539-3}
}

@phdthesis{uehara-1995-syntactic,
  title = {Syntactic Categories in {{Japanese}}: {{A}} Typological and Cognitive Introduction},
  shorttitle = {Syntactic Categories in {{Japanese}}},
  author = {Uehara, Satoshi},
  year = 1995,
  address = {Ann Arbor, Michigan, USA},
  url = {https://www.proquest.com/docview/304224225/abstract/8E9037B9963C4950PQ/1},
  urldate = {2025-08-09},
  abstract = {This dissertation examines the organization of syntactic categories in Japanese and provides a typologically valid basis for cognitive linguistic analyses of the language. Three fundamental problems in syntactic category analyses of Japanese in the past are identified in Chapter 1. The chapter then briefly outlines recent discoveries on the nature of "category" in cognitive linguistics; specifically "prototype" semantic theory (Lakoff 1987, Taylor 1990), discusses central notions of Cognitive Grammar developed by Langacker (1987, 1991), and summarizes Croft's (1991) theory for the universal definition of major syntactic categories. Chap. 2 examines structural criteria used in previous analyses for identification of the five so-called "major" categories in Japanese, clarifies points of disagreement, re-evaluates each criterion in light of Croft's model, and argues that the crucial notion characterizing the structural aspects of the categories in Japanese is a language-specific property of morphological boundness. This boundness criterion not only serves as a basis for the definition of inflection in Japanese, but divides syntactic categories in Japanese into two major classes, each of which is unmarked, or "designed" for one of the two primary pragmatic functions of predication and reference. Two "unique" categories of Japanese, Nominal Adjectives and Verbal Nouns are examined in Chap. 3 and 4 respectively, and cognitive semantic analyses of the two problematic categories are presented. Specifically, it is argued that the grammatical behavior that the two categories exhibit is well motivated by their meaning--meaning as "conceptualization." Evidence is provided for the fundamental assumptions necessarily underlying syntactic category analyses of the language: prototype organization, the syntax-lexicon continuum, and the conceptualization process. Chap. 5 examines a functional motivation of the cardinal structural feature of boundness in Japanese, presents the results of a lexical semantic survey, and argues that the boundness distinction is well motivated by the semantic distinction of relationality, although this correlation has been obscured to some extent in contemporary Japanese. A diachronic account for the form/meaning mismatch is offered in which a semantic shift that occurred to Nouns, which led to the birth of Nominal Adjectives, was not fully accompanied by the shift in morphological boundness.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9798209363859},
  langid = {english},
  school = {University of Michigan},
  keywords = {Education,Language,Language arts,Language literature and linguistics,Linguistics,Modern language},
  file = {/Users/coleman/Zotero/storage/SR85WN3F/Uehara - 1995 - Syntactic categories in Japanese A typological and cognitive introduction.pdf}
}

@incollection{uehara-2003-diachronic,
  title = {A Diachronic Perspective on Prototypicality: {{The}} Case of Nominal Adjectives in {{Japanese}}},
  shorttitle = {A Diachronic Perspective on Prototypicality},
  booktitle = {Cognitive {{Approaches}} to {{Lexical Semantics}}},
  author = {Uehara, Satoshi},
  editor = {Cuyckens, Hubert and Dirven, Ren{\'e} and Taylor, John R.},
  year = 2003,
  pages = {363--392},
  publisher = {De Gruyter Mouton},
  address = {Berlin, Germany},
  url = {https://www.degruyterbrill.com/document/doi/10.1515/9783110219074.363/html?srsltid=AfmBOooPHTfSqJNvd9V3dJusey2KXqMnw4-gfuh6FT2FbhWxlCDuBLFR},
  urldate = {2025-08-09},
  isbn = {978-3-11-021907-4},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/HXFDRKCT/Uehara - 2009 - A diachronic perspective on prototypicality The case of nominal adjectives in Japanese.pdf}
}

@misc{ustun-et-al-2024-aya,
  title = {Aya {{Model}}: {{An Instruction Finetuned Open-Access Multilingual Language Model}}},
  shorttitle = {Aya {{Model}}},
  author = {{\"U}st{\"u}n, Ahmet and Aryabumi, Viraat and Yong, Zheng-Xin and Ko, Wei-Yin and D'souza, Daniel and Onilude, Gbemileke and Bhandari, Neel and Singh, Shivalika and Ooi, Hui-Lee and Kayid, Amr and Vargus, Freddie and Blunsom, Phil and Longpre, Shayne and Muennighoff, Niklas and Fadaee, Marzieh and Kreutzer, Julia and Hooker, Sara},
  year = 2024,
  month = feb,
  number = {arXiv:2402.07827},
  eprint = {2402.07827},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.07827},
  urldate = {2025-11-09},
  abstract = {Recent breakthroughs in large language models (LLMs) have centered around a handful of data-rich languages. What does it take to broaden access to breakthroughs beyond first-class citizen languages? Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50\% are considered as lower-resourced. Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages. We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages -- including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance. Furthermore, we conduct detailed investigations on the optimal finetuning mixture composition, data pruning, as well as the toxicity, bias, and safety of our models. We open-source our instruction datasets and our model at https://hf.co/CohereForAI/aya-101},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/coleman/Zotero/storage/2XVI3RQN/stn et al. - 2024 - Aya Model An Instruction Finetuned Open-Access Multilingual Language Model.pdf;/Users/coleman/Zotero/storage/MFX9QSCX/2402.html}
}

@article{vanarsdall-et-al-2022-analyzing,
  title = {Analyzing the Structure of Animacy: {{Exploring}} Relationships among Six New Animacy and 15 Existing Normative Dimensions for 1,200 Concrete Nouns},
  shorttitle = {Analyzing the Structure of Animacy},
  author = {VanArsdall, Joshua E. and Blunt, Janell R.},
  year = 2022,
  month = jul,
  journal = {Memory \& Cognition},
  volume = {50},
  number = {5},
  pages = {997--1012},
  issn = {1532-5946},
  doi = {10.3758/s13421-021-01266-y},
  urldate = {2025-11-09},
  abstract = {Animacy is an important word variable (especially for episodic memory), yet no norms exist in the literature. We present a complete, usable normative data set of 1,200 relatively concrete nouns normed on 15 existing dimensions (concreteness, familiarity, imagery, availability, valence, arousal, dominance, age of acquisition, length, orthographic neighborhood, phonographic neighborhood, number of syllables, and subtitle frequency/contextual diversity) and six new animacy dimensions (a general living/non-living scale, ability to think, ability to reproduce, similarity to a person, goal-directedness, and movement likelihood). Principal component analysis of these 21 dimensions revealed that animacy scales were conceptually different from extant word variables. Further, factor analysis of the six new scales revealed these animacy norms may be separable into two dimensions: a ``Mental'' component related to animates' ability to think and have goals, and a ``Physical'' component related to animates' general resemblance to living things. These data provide useful theoretical insight into the structure of the animacy dimension, an important factor in many cognitive processes. The norms are accessible at https://osf.io/4t3cu.},
  langid = {english},
  keywords = {Adaptive memory,Animacy,Normative data,Rating task},
  file = {/Users/coleman/Zotero/storage/PDWWJ5RQ/VanArsdall and Blunt - 2022 - Analyzing the structure of animacy Exploring relationships among six new animacy and 15 existing no.pdf}
}

@inproceedings{vanderklis-et-al-2017-mapping,
  title = {Mapping the Perfect via Translation Mining},
  booktitle = {Proceedings of the 15th Conference of the {{European}} Chapter of the Association for Computational Linguistics: {{Volume}} 2, Short Papers},
  author = {{\noopsort{klis}}{van der Klis}, Martijn and Le Bruyn, Bert and {\noopsort{swart}}{de Swart}, Henri{\"e}tte},
  editor = {Lapata, Mirella and Blunsom, Phil and Koller, Alexander},
  year = 2017,
  month = apr,
  pages = {497--502},
  publisher = {Association for Computational Linguistics},
  address = {Valencia, Spain},
  url = {https://aclanthology.org/E17-2080/},
  abstract = {Semantic analyses of the Perfect often defeat their own purpose: by restricting their attention to `real' perfects (like the English one), they implicitly assume the Perfect has predefined meanings and usages. We turn the tables and focus on form, using data extracted from multilingual parallel corpora to automatically generate semantic maps (Haspelmath, 1997) of the sequence `Have/Be + past participle' in five European languages (German, English, Spanish, French, Dutch). This technique, which we dub Translation Mining, has been applied before in the lexical domain (W\"alchli and Cysouw, 2012) but we showcase its application at the level of the grammar.}
}

@article{vanderklis-et-al-2022-generating,
  title = {Generating Semantic Maps through Multidimensional Scaling: Linguistic Applications and Theory},
  shorttitle = {Generating Semantic Maps through Multidimensional Scaling},
  author = {{\noopsort{klis}}{van der Klis}, Martijn and Tellings, Jos},
  year = 2022,
  month = oct,
  journal = {Corpus Linguistics and Linguistic Theory},
  volume = {18},
  number = {3},
  pages = {627--665},
  publisher = {De Gruyter Mouton},
  issn = {1613-7035},
  doi = {10.1515/cllt-2021-0018},
  urldate = {2025-09-20},
  abstract = {This paper reports on the state-of-the-art in application of multidimensional scaling (MDS) techniques to create semantic maps in linguistic research. MDS refers to a statistical technique that represents objects (lexical items, linguistic contexts, languages, etc.) as points in a space so that close similarity between the objects corresponds to close distances between the corresponding points in the representation. We focus on the use of MDS in combination with parallel corpus data as used in research on cross-linguistic variation. We first introduce the mathematical foundations of MDS and then give an exhaustive overview of past research that employs MDS techniques in combination with parallel corpus data. We propose a set of terminology to succinctly describe the key parameters of a particular MDS application. We then show that this computational methodology is theory-neutral, i.e. it can be employed to answer research questions in a variety of linguistic theoretical frameworks. Finally, we show how this leads to two lines of future developments for MDS research in linguistics.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  keywords = {cross-linguistic variation,multidimensional scaling,parallel corpora,semantic maps},
  file = {/Users/coleman/Zotero/storage/WZ757T9G/Klis and Tellings - 2022 - Generating semantic maps through multidimensional scaling linguistic applications and theory.pdf}
}

@article{vangijn-et-al-2014-word,
  title = {Word and the {{Americanist}} Perspective},
  author = {Van Gijn, Rik and Z{\'u}{\~n}iga, Fernando},
  year = 2014,
  month = sep,
  journal = {Morphology},
  volume = {24},
  number = {3},
  pages = {135--160},
  issn = {1871-5621, 1871-5656},
  doi = {10.1007/s11525-014-9242-z},
  urldate = {2025-09-20},
  abstract = {Even though recently appeared reference grammars of lesser-known languages usually do pay attention to issues to do with wordhood, studies of the theoretical and typological import of wordhood-related questions in indigenous languages of the Americas are not numerous. This publication aims to address the challenges posed by individual phenomena found in the Americas to the received views of wordhood.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/62IVXK25/Van Gijn and Ziga - 2014 - Word and the Americanist perspective.pdf;/Users/coleman/Zotero/storage/AY3RRUK5/Van Gijn and Ziga - 2014 - Word and the Americanist perspective.pdf}
}

@inproceedings{vasselli-et-al-2024-applying,
  title = {Applying {{Linguistic Expertise}} to {{LLMs}} for {{Educational Material Development}} in {{Indigenous Languages}}},
  booktitle = {Proceedings of the 4th {{Workshop}} on {{Natural Language Processing}} for {{Indigenous Languages}} of the {{Americas}} ({{AmericasNLP}} 2024)},
  author = {Vasselli, Justin and Mart{\'i}nez Peguero, Arturo and Sung, Junehwan and Watanabe, Taro},
  editor = {Mager, Manuel and Ebrahimi, Abteen and Rijhwani, Shruti and Oncevay, Arturo and Chiruzzo, Luis and Pugh, Robert and {\noopsort{wense}}{von der Wense}, Katharina},
  year = 2024,
  month = jun,
  pages = {201--208},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.americasnlp-1.24},
  urldate = {2025-11-08},
  abstract = {This paper presents our approach to the AmericasNLP 2024 Shared Task 2 as the JAJ (/d\ae z/) team. The task aimed at creating educational materials for indigenous languages, and we focused on Maya and Bribri. Given the unique linguistic features and challenges of these languages, and the limited size of the training datasets, we developed a hybrid methodology combining rule-based NLP methods with prompt-based techniques. This approach leverages the meta-linguistic capabilities of large language models, enabling us to blend broad, language-agnostic processing with customized solutions. Our approach lays a foundational framework that can be expanded to other indigenous languages languages in future work.},
  file = {/Users/coleman/Zotero/storage/6RJ9GNJN/Vasselli et al. - 2024 - Applying Linguistic Expertise to LLMs for Educational Material Development in Indigenous Languages.pdf}
}

@inproceedings{vaswani-et-al-2017-attention,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = 2017,
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  urldate = {2025-10-31},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  file = {/Users/coleman/Zotero/storage/PSYI3QCQ/Vaswani et al. - 2017 - Attention is All you Need.pdf}
}

@article{vaux-et-al-2015-explaining,
  title = {Explaining Vowel Systems: Dispersion Theory vs Natural Selection},
  shorttitle = {Explaining Vowel Systems},
  author = {Vaux, Bert and Samuels, Bridget},
  year = 2015,
  month = sep,
  journal = {The Linguistic Review},
  volume = {32},
  number = {3},
  pages = {573--599},
  publisher = {De Gruyter Mouton},
  issn = {1613-3676},
  doi = {10.1515/tlr-2014-0028},
  urldate = {2025-11-09},
  abstract = {We argue that the cross-linguistic distribution of vowel systems is best accounted for by grammar-external forces of learnability operating in tandem with cognitive constraints on phonological computation, as argued for other phonological phenomena by Blevins (2004). On this view, the range of possible vowel systems is constrained only by what is computable and learnable; the range of attested vowel systems is a subset of this, constrained by relative learnability (Hale and Reiss 2000a, Hale and Reiss 2000b; Newmeyer 2005). A system that is easier to learn (e.g., one whose members are more dispersed in perceptual space) is predicted by our model to become more common cross-linguistically over evolutionary time than its less learnable competitors. This analysis efficiently accounts for both the typological patterns found in vowel systems and the existence of a non-trivial number of ``unnatural'' systems in the world's languages. We compare this model with the leading forms of Dispersion Theory (notably Flemming's (1995) implementation in Optimality Theory), which seek to explain sound patterns in terms of interaction between conflicting functional constraints on maximization of perceptual contrast and minimization of articulatory effort. Dispersion Theory is shown to be unable to generate the attested range of vowel systems or predict their interesting properties, such as the centralization typically found in two-vowel systems and the quality of epenthetic segments.},
  langid = {english},
  keywords = {dispersion theory,evolutionary phonology,Vowel systems},
  file = {/Users/coleman/Zotero/storage/7M3BPP32/Vaux and Samuels - 2015 - Explaining vowel systems dispersion theory vs natural selection.pdf}
}

@article{verkerk-et-al-2008-encoding,
  title = {The Encoding of Adjectives},
  author = {Verkerk, Annemarie and Lestrade, Sander},
  year = 2008,
  month = jan,
  journal = {Linguistics in the Netherlands},
  volume = {25},
  number = {1},
  pages = {157--168},
  publisher = {John Benjamins},
  issn = {0929-7332, 1569-9919},
  doi = {10.1075/avt.25.17ver},
  urldate = {2025-09-05},
  abstract = {Welcome to e-content platform of John Benjamins Publishing Company. Here you can find all of our electronic books and journals, for purchase and download or subscriber access.},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/6495XJY6/Verkerk and Lestrade - 2008 - The encoding of adjectives.pdf;/Users/coleman/Zotero/storage/M9V5NCRH/avt.25.html}
}

@inproceedings{viechnicki-etal-2024-large,
  title = {Large-Scale Bitext Corpora Provide New Evidence for Cognitive Representations of Spatial Terms},
  booktitle = {Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Viechnicki, Peter and Duh, Kevin and Kostacos, Anthony and Landau, Barbara},
  editor = {Graham, Yvette and Purver, Matthew},
  year = 2024,
  month = mar,
  pages = {1089--1099},
  publisher = {Association for Computational Linguistics},
  address = {St. Julian's, Malta},
  doi = {10.18653/v1/2024.eacl-long.66},
  abstract = {Recent evidence from cognitive science suggests that there exist two classes of cognitive representations within the spatial terms of a language, one represented geometrically (e.g., above, below) and the other functionally (e.g., on, in). It has been hypothesized that geometric terms are more constrained and are mastered relatively early in language learning, whereas functional terms are less constrained and are mastered over longer time periods (Landau, 2016). One consequence of this hypothesis is that these two classes should exhibit different cross-linguistic variability, which is supported by human elicitation studies. In this work we present to our knowledge the first corpus-based empirical test of this hypothesis. We develop a pipeline for extracting, isolating, and aligning spatial terms in basic locative constructions from parallel text. Using Shannon entropy to measure the variability of spatial term use across eight languages, we find supporting evidence that variability in functional terms differs significantly from that of geometric terms. We also perform latent variable modeling and find support for the division of spatial terms into geometric and functional classes.}
}

@article{vilain-et-al-brief,
  title = {A Brief History of Articulatory-Acoustic Vowel Representation},
  author = {Vilain, Coriandre and Berthommier, Fr{\'e}d{\'e}ric and Bo{\"e}, Louis-Jean},
  abstract = {This paper aims at following the concept of vowel space across history. It shows that even with very poor experimental means, researchers from the 17th century started to organize the vowel systems along perceptual dimensions, either articulatory, by means of proprioceptive introspection, or auditory. With the development of experimental devices, and the increasing knowledge in acoustic and articulatory theories in the 19th century, it is shown how the relationship between the two dimensions tended to tighten. At the mid 20th century, the link between articulatory parameters such as jaw opening, position of the constriction of the tongue, or lip rounding, and the acoustical values of formants was clear. At this period, with the increasing amount of phonological descriptions of the languages of the world, and the power of the computer database analysis allowing extracting universal tendencies, the question of how the vowel systems are organized arose. The paper discusses this important question, focusing on two points: (1) how the auditory constraints shape the positioning of a specific set of vowel within the acoustic space, and (2) how the articulatory constraints shape the maximal extension of the vowel systems, the so-called maximal vowel space (MVS).},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/PT5M4455/Vilain et al. - A brief history of articulatory-acoustic vowel representation.pdf}
}

@book{vogel-et-al-2011-approaches,
  title = {Approaches to the Typology of Word Classes},
  author = {Vogel, Petra M and Comrie, Bernard},
  year = 2011,
  volume = {23},
  publisher = {Walter de Gruyter}
}

@inproceedings{vulic-et-al-2020-are,
  title = {Are All Good Word Vector Spaces Isomorphic?},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Vuli{\'c}, Ivan and Ruder, Sebastian and S{\o}gaard, Anders},
  editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
  year = 2020,
  month = nov,
  pages = {3178--3192},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.emnlp-main.257},
  urldate = {2025-10-07},
  abstract = {Existing algorithms for aligning cross-lingual word vector spaces assume that vector spaces are approximately isomorphic. As a result, they perform poorly or fail completely on non-isomorphic spaces. Such non-isomorphism has been hypothesised to result from typological differences between languages. In this work, we ask whether non-isomorphism is also crucially a sign of degenerate word vector spaces. We present a series of experiments across diverse languages which show that variance in performance across language pairs is not only due to typological differences, but can mostly be attributed to the size of the monolingual resources available, and to the properties and duration of monolingual training (e.g. ``under-training'').},
  file = {/Users/coleman/Zotero/storage/SNQYY4IB/Vuli et al. - 2020 - Are All Good Word Vector Spaces Isomorphic.pdf}
}

@article{vulic-et-al-2020-multisimlex,
  title = {Multi-{{SimLex}}: A Large-Scale Evaluation of Multilingual and Crosslingual Lexical Semantic Similarity},
  author = {Vuli{\'c}, Ivan and Baker, Simon and Ponti, Edoardo M. and Petti, Ulla and Leviant, Ira and Wing, Kelly and Majewska, Olga and Bar, Eden and Malone, Matt and Poibeau, Thierry and Reichart, Roi and Korhonen, Anna},
  year = 2020,
  month = dec,
  journal = {Computational Linguistics},
  volume = {46},
  number = {4},
  pages = {847--897},
  doi = {10.1162/coli_a_00391},
  abstract = {We introduce Multi-SimLex, a large-scale lexical resource and evaluation benchmark covering data sets for 12 typologically diverse languages, including major languages (e.g., Mandarin Chinese, Spanish, Russian) as well as less-resourced ones (e.g., Welsh, Kiswahili). Each language data set is annotated for the lexical relation of semantic similarity and contains 1,888 semantically aligned concept pairs, providing a representative coverage of word classes (nouns, verbs, adjectives, adverbs), frequency ranks, similarity intervals, lexical fields, and concreteness levels. Additionally, owing to the alignment of concepts across languages, we provide a suite of 66 crosslingual semantic similarity data sets. Because of its extensive size and language coverage, Multi-SimLex provides entirely novel opportunities for experimental evaluation and analysis. On its monolingual and crosslingual benchmarks, we evaluate and analyze a wide array of recent state-of-the-art monolingual and crosslingual representation models, including static and contextualized word embeddings (such as fastText, monolingual and multilingual BERT, XLM), externally informed lexical representations, as well as fully unsupervised and (weakly) supervised crosslingual word embeddings. We also present a step-by-step data set creation protocol for creating consistent, Multi-Simlex--style resources for additional languages. We make these contributions---the public release of Multi-SimLex data sets, their creation protocol, strong baseline results, and in-depth analyses which can be helpful in guiding future developments in multilingual lexical semantics and representation learning---available via a Web site that will encourage community effort in further expansion of Multi-Simlex to many more languages. Such a large-scale semantic resource could inspire significant further advances in NLP across languages.}
}

@article{wagemans-et-al-2012-century,
  title = {A {{Century}} of {{Gestalt Psychology}} in {{Visual Perception I}}. {{Perceptual Grouping}} and {{Figure-Ground Organization}}},
  author = {Wagemans, Johan and Elder, James H. and Kubovy, Michael and Palmer, Stephen E. and Peterson, Mary A. and Singh, Manish and {\noopsort{heydt}}{von der Heydt}, R{\"u}diger},
  year = 2012,
  month = nov,
  journal = {Psychological bulletin},
  volume = {138},
  number = {6},
  pages = {1172--1217},
  issn = {0033-2909},
  doi = {10.1037/a0029333},
  urldate = {2025-11-08},
  abstract = {In 1912, Max Wertheimer published his paper on phi motion, widely recognized as the start of Gestalt psychology. Because of its continued relevance in modern psychology, this centennial anniversary is an excellent opportunity to take stock of what Gestalt psychology has offered and how it has changed since its inception. We first introduce the key findings and ideas in the Berlin school of Gestalt psychology, and then briefly sketch its development, rise, and fall. Next, we discuss its empirical and conceptual problems, and indicate how they are addressed in contemporary research on perceptual grouping and figure-ground organization. In particular, we review the principles of grouping, both classical (e.g., proximity, similarity, common fate, good continuation, closure, symmetry, parallelism) and new (e.g., synchrony, common region, element and uniform connectedness), and their role in contour integration and completion. We then review classic and new image-based principles of figure-ground organization, how it is influenced by past experience and attention, and how it relates to shape and depth perception. After an integrated review of the neural mechanisms involved in contour grouping, border-ownership, and figure-ground perception, we conclude by evaluating what modern vision science has offered compared to traditional Gestalt psychology, whether we can speak of a Gestalt revival, and where the remaining limitations and challenges lie. A better integration of this research tradition with the rest of vision science requires further progress regarding the conceptual and theoretical foundations of the Gestalt approach, which will be the focus of a second review paper.},
  pmcid = {PMC3482144},
  pmid = {22845751},
  file = {/Users/coleman/Zotero/storage/CAZDDLMZ/Wagemans et al. - 2012 - A Century of Gestalt Psychology in Visual Perception I. Perceptual Grouping and Figure-Ground Organi.pdf}
}

@article{walchli-2016-nonspecific,
  title = {Non-Specific, Specific and Obscured Perception Verbs in {{Baltic}} Languages},
  author = {W{\"a}lchli, Bernard},
  year = 2016,
  journal = {Baltic Linguistics},
  volume = {7},
  pages = {53--135}
}

@article{walchli-2018-long,
  title = {`{{As}} Long as', `until' and `before' Clauses: {{Zooming}} in on Linguistic Diversity},
  author = {W{\"a}lchli, Bernard},
  year = 2018,
  journal = {Baltic Linguistics},
  volume = {9},
  pages = {141--236}
}

@article{walchli-et-al-2012-lexical,
  title = {Lexical Typology through Similarity Semantics: {{Toward}} a Semantic Map of Motion Verbs},
  shorttitle = {Lexical Typology through Similarity Semantics},
  author = {W{\"a}lchli, Bernhard and Cysouw, Michael},
  year = 2012,
  month = may,
  journal = {Linguistics},
  volume = {50},
  number = {3},
  pages = {671--710},
  publisher = {De Gruyter Mouton},
  issn = {1613-396X},
  doi = {10.1515/ling-2012-0021},
  urldate = {2025-11-10},
  abstract = {This paper discusses a multidimensional probabilistic semantic map of lexical motion verb stems based on data collected from parallel texts (viz. translations of the Gospel according to Mark) for 100 languages from all continents. The crosslinguistic diversity of lexical semantics in motion verbs is illustrated in detail for the domain of `go', `come', and `arrive' type contexts. It is argued that the theoretical bases underlying probabilistic semantic maps from exemplar data are the isomorphism hypothesis (given any two meanings and their corresponding forms in any particular language, more similar meanings are more likely to be expressed by the same form in any language), similarity semantics (similarity is more basic than identity), and exemplar semantics (exemplar meaning is more fundamental than abstract concepts).},
  langid = {english},
  file = {/Users/coleman/Zotero/storage/BHIMJ2JN/Bernhard Wlchli and Michael Cysouw - 2012 - Lexical typology through similarity semantics Toward a semantic map of motion verbs.pdf}
}

@book{wals,
  type = {Data Set},
  title = {{{WALS}} Online (V2020.4)},
  editor = {Dryer, Matthew S. and Haspelmath, Martin},
  year = 2013,
  publisher = {Zenodo},
  doi = {10.5281/zenodo.13950591}
}

@inproceedings{warstadt-et-al-2023-findings,
  title = {Findings of the {{BabyLM Challenge}}: {{Sample-Efficient Pretraining}} on {{Developmentally Plausible Corpora}}},
  shorttitle = {Findings of the {{BabyLM Challenge}}},
  booktitle = {Proceedings of the {{BabyLM Challenge}} at the 27th {{Conference}} on {{Computational Natural Language Learning}}},
  author = {Warstadt, Alex and Mueller, Aaron and Choshen, Leshem and Wilcox, Ethan and Zhuang, Chengxu and Ciro, Juan and Mosquera, Rafael and Paranjabe, Bhargavi and Williams, Adina and Linzen, Tal and Cotterell, Ryan},
  editor = {Warstadt, Alex and Mueller, Aaron and Choshen, Leshem and Wilcox, Ethan and Zhuang, Chengxu and Ciro, Juan and Mosquera, Rafael and Paranjabe, Bhargavi and Williams, Adina and Linzen, Tal and Cotterell, Ryan},
  year = 2023,
  month = dec,
  pages = {1--34},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.conll-babylm.1},
  urldate = {2025-11-08},
  file = {/Users/coleman/Zotero/storage/FZD3DK2W/Warstadt et al. - 2023 - Findings of the BabyLM Challenge Sample-Efficient Pretraining on Developmentally Plausible Corpora.pdf}
}

@inproceedings{wartena-2013-distributional,
  title = {Distributional Similarity of Words with Different Frequencies},
  booktitle = {Proceedings of the 13th Edition of the {{Dutch-Belgian}} Information Retrieval {{Workshop}} ({{DIR}} 2013)},
  author = {Wartena, Christian},
  year = 2013,
  pages = {8--11},
  publisher = {Hochschule Hannover}
}

@phdthesis{weber-1983-grammar,
  title = {A {{Grammar}} of {{Huallaga}} (Huanuco) {{Quechua}}.},
  author = {Weber, David John},
  year = 1983,
  address = {Los Angeles, California, USA},
  url = {https://www.proquest.com/docview/303135208/abstract/B0302B76EE6B4E87PQ/1},
  urldate = {2024-10-07},
  abstract = {This is a reference grammar of Huallaga (Huanuco) Quechua, an American Indian language spoken in central Peru. After (1) a general introduction and (2) an introduction to HgQ syntax, it contains chapters of the following topics: on word and suffix classes for (3) verbs, (4) substantives, (5) adverbs, and (6) other classes; on morphology: (7) word formation generally, (8) the "transitions," i.e., the complex which indicates the person of the subject and object, and (9) the suffixes which occur between the root and the transition; on grammatical relations: (10) case markers (11) and passives; (12) on substantive phrases; (13) on relative clauses and complements; (14) on adverbial clauses; (15) on reduplication; (16) on question formation; (17) on negation; (18) on conjunction; on the post-transition suffixes: (19) the "shading" suffixes (-lla, -pis, -na, and -raq), (20) the (so-called) "topic" marker -qa, and (21) the evidential suffixes (-mi, -shi and -chi); (22) on idiomatic and formulaic expressions; and (23) on phonology and loan processes.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9798403416092},
  langid = {english},
  school = {University of California},
  keywords = {Language literature and linguistics,Linguistics},
  file = {/Users/coleman/Zotero/storage/EQUMMBDA/WEBER_A Grammar of Huallaga (huanuco) Quechua.pdf}
}

@incollection{wetzer-1992-nouny,
  title = {``{{Nouny}}'' and ``Verby'' Adjectivale: A Typology of Predicative Adjectival Constructions},
  shorttitle = {``{{Nouny}}'' and ``Verby'' Adjectivale},
  booktitle = {Meaning and {{Grammar}}: {{Cross-Linguistic Perspectives}}},
  author = {Wetzer, Harrie},
  editor = {Kefer, Michel and {\noopsort{auwera}}van der Auwera, Johan},
  year = 1992,
  pages = {223--262},
  publisher = {De Gruyter Mouton},
  url = {https://www.degruyterbrill.com/document/doi/10.1515/9783110851656.223/html},
  urldate = {2025-08-09},
  isbn = {978-3-11-085165-6},
  langid = {english},
  keywords = {adjectives,cognitive,continuum,thesis},
  file = {/Users/coleman/Zotero/storage/45LKAGKV/Wetzer - 2013 - Nouny and verby adjectivale a typology of predicative adjectival constructions.pdf}
}

@book{wetzer-1996-nouniness,
  title = {Nouniness and Verbiness: A Typological Study of Adjectival Predication},
  author = {Wetzer, Harrie},
  year = 1996,
  publisher = {De Gruyter Mouton},
  address = {Berlin, Germany}
}

@book{wetzer-2013-typology,
  title = {The {{Typology}} of {{Adjectival Predication}}},
  author = {Wetzer, Harrie},
  year = 2013,
  month = mar,
  publisher = {De Gruyter Mouton},
  address = {Berlin, Germany},
  doi = {10.1515/9783110813586},
  urldate = {2025-08-09},
  abstract = {The Typology of Adjectival Predication by Harrie Wetzer was published on March 1, 2013 by De Gruyter Mouton.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  isbn = {978-3-11-081358-6},
  langid = {english},
  keywords = {adjectives,Adjektiv,cognitive,Kontrastive Grammatik},
  file = {/Users/coleman/Zotero/storage/FAZXE6SF/Wetzer - 2013 - The Typology of Adjectival Predication.pdf}
}

@inproceedings{wiedemann-et-al-2019-does,
  title = {Does {{BERT}} Make Any Sense? Interpretable Word Sense Disambiguation with Contextualized Embeddings},
  shorttitle = {Does {{BERT Make Any Sense}}?},
  booktitle = {Proceedings of the 15th {{Conference}} on {{Natural Language Processing}}, {{KONVENS}}},
  author = {Wiedemann, Gregor and Remus, Steffen and Chawla, Avi and Biemann, Chris},
  year = 2019,
  address = {Erlangen, Germany},
  url = {https://corpora.linguistik.uni-erlangen.de/data/konvens/proceedings/papers/KONVENS2019\_paper\_43.pdf},
  urldate = {2025-11-04}
}

@article{wilcox-et-al-2023-testing,
  title = {Testing the Predictions of Surprisal Theory in 11 Languages},
  author = {Wilcox, Ethan G. and Pimentel, Tiago and Meister, Clara and Cotterell, Ryan and Levy, Roger P.},
  year = 2023,
  month = dec,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {1451--1470},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00612},
  urldate = {2025-11-01},
  abstract = {Abstract             Surprisal theory posits that less-predictable words should take more time to process, with word predictability quantified as surprisal, i.e., negative log probability in context. While evidence supporting the predictions of surprisal theory has been replicated widely, much of it has focused on a very narrow slice of data: native English speakers reading English texts. Indeed, no comprehensive multilingual analysis exists. We address this gap in the current literature by investigating the relationship between surprisal and reading times in eleven different languages, distributed across five language families. Deriving estimates from language models trained on monolingual and multilingual corpora, we test three predictions associated with surprisal theory: (i) whether surprisal is predictive of reading times, (ii) whether expected surprisal, i.e., contextual entropy, is predictive of reading times, and (iii) whether the linking function between surprisal and reading times is linear. We find that all three predictions are borne out crosslinguistically. By focusing on a more diverse set of languages, we argue that these results offer the most robust link to date between information theory and incremental language processing across languages.},
  langid = {english}
}

@book{wiltschko-2014-universal,
  title = {The {{Universal Structure}} of {{Categories}}},
  author = {Wiltschko, Martina},
  year = 2014,
  series = {Cambridge {{Studies}} in {{Linguistics}}},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9781139833899},
  abstract = {Using data from a variety of languages such as Blackfoot, Halkomelem, and Upper Austrian German, this book explores a range of grammatical categories and constructions, including tense, aspect, subjunctive, case and demonstratives. It presents a new theory of grammatical categories - the Universal Spine Hypothesis - and reinforces generative notions of Universal Grammar while accommodating insights from linguistic typology. In essence, this new theory shows that language-specific categories are built from a small set of universal categories and language-specific units of language. Throughout the book the Universal Spine Hypothesis is compared to two alternative theories - the Universal Base Hypothesis and the No Base Hypothesis. This valuable addition to the field will be welcomed by graduate students and researchers in linguistics.},
  isbn = {978-1-107-03851-6}
}

@incollection{woodard-2023-greek,
  title = {Greek {{Linguistic Thought}} and Its {{Roman Reception}}},
  booktitle = {The {{Cambridge History}} of {{Linguistics}}},
  author = {Woodard, Roger D.},
  editor = {Joseph, John E. and Waugh, Linda R. and {Monville-Burston}, Monique},
  year = 2023,
  pages = {102--143},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/9780511842788.008},
  urldate = {2025-11-08},
  abstract = {This chapter takes an evolutionary perspective on Greek linguistic thought and ends with consideration of the Roman grammatical tradition. It looks at important steps in Greek conceptualization and the terminology linked to it. Under phonic conceptualizing the author studies the development of scripts, the categorization of sounds according to +/- features and the absence of a functional distinction between letter and sound. Regarding morphosyntactic conceptualization, Plato's (and Aristotle's) two elements of discourse structure, on\'omata and rh\textacutemacron emata (roughly nouns and verbs) are discussed, and also Aristotle's parts of speech, his division of categories `providing signification' and those that don't, and the antithesis between nature and convention. The author highlights the Stoics' fundamental recognition of six parts of speech and presents Greek grammarians' views on processual aspects of on\'omata and rh\textacutemacron emata, i.e., pt\^osis/kl\'isis (similar to modern `inflection/case'). Greek influence on Latin grammarians and Varro's lexical choices for translating Greek terminology are reviewed. It is emphasized that Dionysus Thrax's Techne grammatike was the most influential handbook of language analysis in Late Antiquity and comes close to what we would call a grammar. Finally, Donatus' Ars grammatica and its long-lasting impact on language study in the West are discussed.},
  isbn = {978-0-521-84990-6},
  keywords = {categorization of Greek sounds,language constituent units (Aristotle Stoics),morphosyntactic conceptualization in Plato and Aristotle,phonic conceptualization,Techne grammatike by Dionysus Thrax,Varro: Roman language conceptualization},
  file = {/Users/coleman/Zotero/storage/LQHNBPHP/FEC7F945B05E2C4B45FC1BEABD17E32F.html}
}

@article{wu-et-al-2022-leveraging,
  title = {Leveraging {{Multi-Modal Information}} for {{Cross-Lingual Entity Matching}} across {{Knowledge Graphs}}},
  author = {Wu, Tianxing and Gao, Chaoyu and Li, Lin and Wang, Yuxiang},
  year = 2022,
  month = jan,
  journal = {Applied Sciences},
  volume = {12},
  number = {19},
  pages = {10107},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-3417},
  doi = {10.3390/app121910107},
  urldate = {2025-11-06},
  abstract = {In recent years, the scale of knowledge graphs and the number of entities have grown rapidly. Entity matching across different knowledge graphs has become an urgent problem to be solved for knowledge fusion. With the importance of entity matching being increasingly evident, the use of representation learning technologies to find matched entities has attracted extensive attention due to the computability of vector representations. However, existing studies on representation learning technologies cannot make full use of knowledge graph relevant multi-modal information. In this paper, we propose a new cross-lingual entity matching method (called CLEM) with knowledge graph representation learning on rich multi-modal information. The core is the multi-view intact space learning method to integrate embeddings of multi-modal information for matching entities. Experimental results on cross-lingual datasets show the superiority and competitiveness of our proposed method.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {cross-lingual entity matching,knowledge graph,knowledge graph embedding,representation learning},
  file = {/Users/coleman/Zotero/storage/XA6B5NZU/Wu et al. - 2022 - Leveraging Multi-Modal Information for Cross-Lingual Entity Matching across Knowledge Graphs.pdf}
}

@inproceedings{wu-et-al-2023-composition,
  title = {Composition and {{Deformance}}: {{Measuring Imageability}} with a {{Text-to-Image Model}}},
  shorttitle = {Composition and {{Deformance}}},
  booktitle = {Proceedings of the 5th {{Workshop}} on {{Narrative Understanding}}},
  author = {Wu, Si and Smith, David},
  editor = {Akoury, Nader and Clark, Elizabeth and Iyyer, Mohit and Chaturvedi, Snigdha and Brahman, Faeze and Chandu, Khyathi},
  year = 2023,
  month = jul,
  pages = {106--117},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.wnu-1.16},
  urldate = {2024-07-23},
  abstract = {Although psycholinguists and psychologists have long studied the tendency of linguistic strings to evoke mental images in hearers or readers, most computational studies have applied this concept of imageability only to isolated words. Using recent developments in text-to-image generation models, such as DALLE mini, we propose computational methods that use generated images to measure the imageability of both single English words and connected text. We sample text prompts for image generation from three corpora: human-generated image captions, news article sentences, and poem lines. We subject these prompts to different deformances to examine the model's ability to detect changes in imageability caused by compositional change. We find high correlation between the proposed computational measures of imageability and human judgments of individual words. We also find the proposed measures more consistently respond to changes in compositionality than baseline approaches. We discuss possible effects of model training and implications for the study of compositionality in text-to-image models.},
  file = {/Users/coleman/Zotero/storage/ZKNF3ZQU/Wu_Smith_2023_Composition and Deformance.pdf}
}

@inproceedings{wu-et-al-2025-semantic,
  title = {The Semantic Hub Hypothesis: {{Language}} Models Share Semantic Representations across Languages and Modalities},
  shorttitle = {The {{Semantic Hub Hypothesis}}},
  booktitle = {The {{Thirteenth International Conference}} on {{Learning Representations}}, {{ICLR}} 2025, {{Singapore}}, {{April}} 24-28, 2025},
  author = {Wu, Zhaofeng and Yu, Xinyan Velocity and Yogatama, Dani and Lu, Jiasen and Kim, Yoon},
  year = 2025,
  publisher = {OpenReview.net},
  url = {https://openreview.net/forum?id=FrFQpAgnGE},
  urldate = {2025-11-01}
}

@article{xu-et-al-2020-numeral,
  title = {Numeral {{Systems Across Languages Support Efficient Communication}}: {{From Approximate Numerosity}} to {{Recursion}}},
  shorttitle = {Numeral {{Systems Across Languages Support Efficient Communication}}},
  author = {Xu, Yang and Liu, Emmy and Regier, Terry},
  year = 2020,
  month = aug,
  journal = {Open Mind},
  volume = {4},
  pages = {57--70},
  issn = {2470-2986},
  doi = {10.1162/opmi_a_00034},
  urldate = {2025-11-09},
  abstract = {Languages differ qualitatively in their numeral systems. At one extreme, some languages have a small set of number terms, which denote approximate or inexact numerosities; at the other extreme, many languages have forms for exact numerosities over a very large range, through a recursively defined counting system. Why do numeral systems vary as they do? Here, we use computational analyses to explore the numeral systems of 30 languages that span this spectrum. We find that these numeral systems all reflect a functional need for efficient communication, mirroring existing arguments in other semantic domains such as color, kinship, and space. Our findings suggest that cross-language variation in numeral systems may be understood in terms of a shared functional need to communicate precisely while using minimal cognitive resources.},
  file = {/Users/coleman/Zotero/storage/57YPGSPE/Xu et al. - 2020 - Numeral Systems Across Languages Support Efficient Communication From Approximate Numerosity to Rec.pdf;/Users/coleman/Zotero/storage/Q7Z6NT2W/opmi_a_00034.html}
}

@misc{xu-et-al-2025-can,
  title = {Can {{Language Models Learn Typologically Implausible Languages}}?},
  author = {Xu, Tianyang and Kuribayashi, Tatsuki and Oseki, Yohei and Cotterell, Ryan and Warstadt, Alex},
  year = 2025,
  month = feb,
  number = {arXiv:2502.12317},
  eprint = {2502.12317},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.12317},
  urldate = {2025-11-09},
  abstract = {Grammatical features across human languages show intriguing correlations often attributed to learning biases in humans. However, empirical evidence has been limited to experiments with highly simplified artificial languages, and whether these correlations arise from domain-general or language-specific biases remains a matter of debate. Language models (LMs) provide an opportunity to study artificial language learning at a large scale and with a high degree of naturalism. In this paper, we begin with an in-depth discussion of how LMs allow us to better determine the role of domain-general learning biases in language universals. We then assess learnability differences for LMs resulting from typologically plausible and implausible languages closely following the word-order universals identified by linguistic typologists. We conduct a symmetrical cross-lingual study training and testing LMs on an array of highly naturalistic but counterfactual versions of the English (head-initial) and Japanese (head-final) languages. Compared to similar work, our datasets are more naturalistic and fall closer to the boundary of plausibility. Our experiments show that these LMs are often slower to learn these subtly implausible languages, while ultimately achieving similar performance on some metrics regardless of typological plausibility. These findings lend credence to the conclusion that LMs do show some typologically-aligned learning preferences, and that the typological patterns may result from, at least to some degree, domain-general learning biases.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/coleman/Zotero/storage/GCK5T4BV/Xu et al. - 2025 - Can Language Models Learn Typologically Implausible Languages.pdf;/Users/coleman/Zotero/storage/T9KAMZJ5/2502.html}
}

@inproceedings{yao-2010-stage,
  title = {Stage/{{Individual-level Predicates}}, {{Topics}} and {{Indefinite Subjects}}},
  booktitle = {Proceedings of the 24th {{Pacific Asia Conference}} on {{Language}}, {{Information}} and {{Computation}}},
  author = {Yao, Shuiying},
  editor = {Otoguro, Ryo and Ishikawa, Kiyoshi and Umemoto, Hiroshi and Yoshimoto, Kei and Harada, Yasunari},
  year = 2010,
  month = nov,
  pages = {573--582},
  publisher = {Institute of Digital Enhancement of Cognitive Processing, Waseda University},
  address = {Tohoku University, Sendai, Japan},
  url = {https://aclanthology.org/Y10-1066/},
  urldate = {2025-09-20},
  file = {/Users/coleman/Zotero/storage/TLQKHYAZ/Yao - 2010 - StageIndividual-level Predicates, Topics and Indefinite Subjects.pdf}
}

@misc{ye-et-al-2024-computer,
  title = {Computer Vision Datasets and Models Exhibit Cultural and Linguistic Diversity in Perception},
  author = {Ye, Andre and Santy, Sebastin and Hwang, Jena D. and Zhang, Amy X. and Krishna, Ranjay},
  year = 2024,
  eprint = {2310.14356},
  primaryclass = {cs.CV},
  url = {https://arxiv.org/abs/2310.14356},
  archiveprefix = {arXiv}
}

@article{yong-gyun-et-al-2013,
  title = {},
  author = {{Yong-gyun}, Kim and {Kyung-won}, Seo},
  year = 2013,
  journal = {Journal of Japanese Culture},
  volume = {59},
  pages = {5--22},
  url = {https://www.kci.go.kr/kciportal/landing/article.kci?arti_id=ART001823482}
}

@article{youn-et-al-2016-universal,
  title = {On the Universal Structure of Human Lexical Semantics},
  author = {Youn, Hyejin and Sutton, Logan and Smith, Eric and Moore, Cristopher and Wilkins, Jon F. and Maddieson, Ian and Croft, William and Bhattacharya, Tanmoy},
  year = 2016,
  month = feb,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {113},
  number = {7},
  pages = {1766--1771},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1520752113},
  urldate = {2025-10-07},
  abstract = {How universal is human conceptual structure? The way concepts are organized in the human brain may reflect distinct features of cultural, historical, and environmental background in addition to properties universal to human cognition. Semantics, or meaning expressed through language, provides indirect access to the underlying conceptual structure, but meaning is notoriously difficult to measure, let alone parameterize. Here, we provide an empirical measure of semantic proximity between concepts using cross-linguistic dictionaries to translate words to and from languages carefully selected to be representative of worldwide diversity. These translations reveal cases where a particular language uses a single ``polysemous'' word to express multiple concepts that another language represents using distinct words. We use the frequency of such polysemies linking two concepts as a measure of their semantic proximity and represent the pattern of these linkages by a weighted network. This network is highly structured: Certain concepts are far more prone to polysemy than others, and naturally interpretable clusters of closely related concepts emerge. Statistical analysis of the polysemies observed in a subset of the basic vocabulary shows that these structural properties are consistent across different language groups, and largely independent of geography, environment, and the presence or absence of a literary tradition. The methods developed here can be applied to any semantic domain to reveal the extent to which its conceptual structure is, similarly, a universal attribute of human cognition and language use.},
  file = {/Users/coleman/Zotero/storage/WXSY4UCL/Youn et al. - 2016 - On the universal structure of human lexical semantics.pdf}
}

@article{zaslavsky-et-al-2018-efficient,
  title = {Efficient Compression in Color Naming and Its Evolution},
  author = {Zaslavsky, Noga and Kemp, Charles and Regier, Terry and Tishby, Naftali},
  year = 2018,
  month = jul,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {31},
  pages = {7937--7942},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1800521115},
  urldate = {2025-10-07},
  abstract = {We derive a principled information-theoretic account of cross-language semantic variation. Specifically, we argue that languages efficiently compress ideas into words by optimizing the information bottleneck (IB) trade-off between the complexity and accuracy of the lexicon. We test this proposal in the domain of color naming and show that (i) color-naming systems across languages achieve near-optimal compression; (ii) small changes in a single trade-off parameter account to a large extent for observed cross-language variation; (iii) efficient IB color-naming systems exhibit soft rather than hard category boundaries and often leave large regions of color space inconsistently named, both of which phenomena are found empirically; and (iv) these IB systems evolve through a sequence of structural phase transitions, in a single process that captures key ideas associated with different accounts of color category evolution. These results suggest that a drive for information-theoretic efficiency may shape color-naming systems across languages. This principle is not specific to color, and so it may also apply to cross-language variation in other semantic domains.},
  file = {/Users/coleman/Zotero/storage/FV2RBGAA/Zaslavsky et al. - 2018 - Efficient compression in color naming and its evolution.pdf}
}

@article{zaslavsky-et-al-2019-color,
  title = {Color Naming Reflects Both Perceptual Structure and Communicative Need},
  author = {Zaslavsky, Noga and Kemp, Charles and Tishby, Naftali and Regier, Terry},
  year = 2019,
  journal = {Topics in Cognitive Science},
  volume = {11},
  number = {1},
  pages = {207--219},
  issn = {1756-8765},
  doi = {10.1111/tops.12395},
  urldate = {2025-11-06},
  abstract = {Gibson et al. () argued that color naming is shaped by patterns of communicative need. In support of this claim, they showed that color naming systems across languages support more precise communication about warm colors than cool colors, and that the objects we talk about tend to be warm-colored rather than cool-colored. Here, we present new analyses that alter this picture. We show that greater communicative precision for warm than for cool colors, and greater communicative need, may both be explained by perceptual structure. However, using an information-theoretic analysis, we also show that color naming across languages bears signs of communicative need beyond what would be predicted by perceptual structure alone. We conclude that color naming is shaped both by perceptual structure, as has traditionally been argued, and by patterns of communicative need, as argued by Gibson et al. ---although for reasons other than those they advanced.},
  copyright = {\copyright{} 2018 Cognitive Science Society, Inc.},
  langid = {english},
  keywords = {Categorization,Color naming,Information theory},
  file = {/Users/coleman/Zotero/storage/VWNJWNZK/Zaslavsky et al. - 2019 - Color Naming Reflects Both Perceptual Structure and Communicative Need.pdf;/Users/coleman/Zotero/storage/S2QPJGRU/tops.html}
}

@inproceedings{zhai-et-al-2023-sigmoid,
  title = {Sigmoid Loss for Language Image Pre-Training},
  booktitle = {2023 {{IEEE}}/{{CVF}} International Conference on Computer Vision ({{ICCV}})},
  author = {Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
  year = 2023,
  month = oct,
  pages = {11941--11952},
  publisher = {IEEE Computer Society},
  address = {Los Alamitos, California, USA},
  doi = {10.1109/ICCV51070.2023.01100},
  abstract = {We propose a simple pairwise sigmoid loss for imagetext pre-training. Unlike standard contrastive learning with softmax normalization, the sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. With only four TPUv4 chips, we can train a Base CLIP model at 4k batch size and a Large LiT model at 20k batch size, the latter achieves 84.5},
  keywords = {computer vision,memory management,robustness,self-supervised learning,standards}
}

@book{zipf-1936-psychobiology,
  title = {The {{Psychobiology}} of {{Language}}},
  author = {Zipf, George K.},
  year = 1936,
  publisher = {Routledge},
  address = {London, UK}
}

@book{zipf-1949-human,
  title = {Human {{Behavior}} and the {{Priciple}} of {{Least Effort}}},
  author = {Zipf, George K.},
  year = 1949,
  publisher = {Addison-Wesley},
  address = {New York, New York, USA}
}

@misc{zotero-item-2472,
  type = {Misc}
}

@incollection{Zwicky1994,
  title = {What Is a Clitic?},
  booktitle = {Clitics: {{A}} Comprehensive Bibliography 1892--1991},
  author = {Zwicky, Arnold M.},
  editor = {Nevis, Joel A. and Joseph, Brian D. and Wanner, Dieter and Zwicky, Arnold M.},
  year = 1994,
  pages = {xii--xx},
  publisher = {John Benjamins},
  address = {Amsterdam, Netherlands}
}

@preamble{ "\providecommand{\noopsort}[1]{} " }

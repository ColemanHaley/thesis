\babel@toc {british}{}\relax 
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces An example semantic map for the dative domain, adapted from \citet {haspelmath-2003-geometry}. Nodes represent different functions which ``dative-like'' elements can express. The boundaries for English {\em to} and French {\em \`{a}} are shown in pink and blue, respectively. Both terms cover contiguous regions of the map, satisfying the Semantic Map Connectivity Hypothesis.}}{19}{figure.caption.17}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces A typical vision-and-language model architecture, in the process of captioning an image. A vision transformer produces a representation of the input image, which is linearly projected into embedding vectors for an autoregressive transformer language model. The model generates text one subword token at a time based on the image and the preceding tokens. Here, it has generated the token \texttt {ying} as the next token after \texttt {\_a \_cute \_cat \_is \_pla}. The token \texttt {ying} will be added to the input at the next time step to continue generating a caption.}}{31}{figure.caption.22}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces \added [id=r2]{The empirical distributions of our four measures (quantifying the magnitude $M$ and variability $V$ of changes in Form and in Embedding space) for inflections and derivations in UniMorph}}}{76}{figure.caption.31}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces The mean cosine similarity between FastText embeddings of words of the same and different parts of speech in UniMorph.}}{80}{figure.caption.32}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces \added [id=r2]{Cross-validation accuracy and standard error} in reconstructing UniMorph's inflection–derivation distinction by various supervised classifiers. \added [id=r2]{Linguistically-motivated hypotheses referred to in the text are denoted with letters}}}{89}{figure.caption.33}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces Probability and Odds ratio with 95\% confidence intervals of being classified as derivation for various kinds of inflectional meaning. Inflections to the right of the dotted line were disproportionately likely to be classified as derivation by our model}}{92}{figure.caption.34}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Probability and Odds ratio with 95\% confidence intervals of being classified as derivation for inherent inflections and transpositions}}{94}{figure.caption.35}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces Probability and Odds ratio with 95\% confidence intervals of being classified as derivation for inherent vs. contextual noun inflections}}{95}{figure.caption.36}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces Our two most predictive measures for inflection and derivation. \added {Saturation represents overlapping constructions.} With respect to these two variables, the inflection–derivation distinction appears gradient rather than categorical}}{105}{figure.caption.37}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Heatmap of mutual information estimates across parts of speech in thirty languages. Cells show the statistical significance of a word class's groundedness (MI > 0). Unattested classes are white. Some functional classes display non-significant levels of groundedness in several languages, while lexical classes dominantly show highly significant grounding.}}{133}{figure.caption.46}%
\contentsline {figure}{\numberline {5.2}{\ignorespaces Word token level distributions of the groundedness measure (PMI) across all languages and datasets, grouped by part of speech (word class). We also report the estimated marginal mean and ranking of each word class. Colors are based on the ranking of classes, rather than their average PMIs. Overall, the distribution and estimated ranking of word classes strongly suggest our groundedness measure quantitatively captures the distinction between lexical and functional classes.}}{134}{figure.caption.47}%
\contentsline {figure}{\numberline {5.3}{\ignorespaces Mean and standard deviation of per-language mutual information estimates between word class and image. Across 30 languages, we see clear and consistent tendencies about which parts of speech are more ``grounded'', corresponding to a gradeddistinction between lexical and functional classes.}}{136}{figure.caption.49}%
\contentsline {figure}{\numberline {5.4}{\ignorespaces Correlation between human concreteness ratings and type-level groundedness (PMI; left, $\rho \,{=}\,0.368$) or uncertainty coefficient (right, $\rho \,{=}\,0.609$): i.e., the average ratio between LM surprisal and captioning model surprisal.}}{137}{figure.caption.51}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces Groundedness scores for {\em na}-adjective\xspace \xspace {\em makka} (completely red; right) and {\em i}-adjective\xspace \xspace {\em akai} (red; left) in the STAIR-full-dev dataset.}}{156}{figure.caption.57}%
\contentsline {figure}{\numberline {6.2}{\ignorespaces Goundedness of the verbal categories across the 30 languages in this study. Error bars represent standard error in the mean groundedness across the datasets considered (COCO-35L, XM3600, and Multi30K where available). Contra theoretical predictions, verby languages do not exhibit higher mean groundedness of verbs, but are somewhat below average. However, this effect is confounded by model quality issues, as suggested by the lower groundedness of verbs in non-Latin script languages.}}{167}{figure.caption.62}%
\contentsline {figure}{\numberline {6.3}{\ignorespaces Z-scored groundedness of the verbal categories. Error bars represent standard error in the Z-scores across the datasets considered (COCO-35L, XM3600, and Multi30K where available). The results suggest verbs are not {\em relatively} more grounded than other words in verby languages. However, we observe a clear effect of script, with languages written in Latin script exhibiting relatively more grounded verbs.}}{170}{figure.caption.65}%
\contentsline {figure}{\numberline {6.4}{\ignorespaces Z-scored groundedness of the verbal categories, with adjectives included for verby languages. Error bars represent standard error in the Z-scores across the datasets considered (COCO-35L, XM3600, and Multi30K where available). Despite the higher groundedness of adjectives than verbs in general, and concerns that legitimate members of the verbal category could be disproporionally ``lost'' to the adjective tag in verby languages, we still observe lower groundedness for the verby languages. This suggests an disproportionate effect of captioning and language model quality on verbs.}}{171}{figure.caption.66}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {A.1}{\ignorespaces Accuracy in reconstructing UniMorph's inflection–derivation distinction by MLP classifiers using Word2Vec- vs. FastText-based distributional features. Hypotheses referred to in the main text are denoted with letters.}}{181}{figure.caption.73}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {C.1}{\ignorespaces Correlation between English psycholinguistic norms and type-level groundedness (left) or uncertainty coefficent (right): i.e., the average ratio between LM surprisal and captioning model surprisal. Type-level measures were computed by averaging scores across the COCO-dev dataset for types which occur at least 30 times.}}{188}{figure.caption.74}%
\addvspace {10\p@ }
